<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/flower.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/flower.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/flower-16x16.png">
  <link rel="mask-icon" href="/images/flower.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://hellojialee.github.io').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"right","display":"hide","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="经常使用的Pytorch代码片段技巧，以及环境搭建和使用易错点">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch实用指南">
<meta property="og:url" content="https://hellojialee.github.io/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="佳の博客 Jia&#39;s Blog">
<meta property="og:description" content="经常使用的Pytorch代码片段技巧，以及环境搭建和使用易错点">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-05-28T08:07:51.000Z">
<meta property="article:modified_time" content="2023-09-11T03:26:41.546Z">
<meta property="article:author" content="Jia Li">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hellojialee.github.io/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Pytorch实用指南 | 佳の博客 Jia's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">佳の博客 Jia's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-about-me">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About Me</a>

  </li>
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="https://hellojialee.github.io/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang1.jpg">
      <meta itemprop="name" content="Jia Li">
      <meta itemprop="description" content="Be happy, be healthy!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="佳の博客 Jia's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch实用指南
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-28 16:07:51" itemprop="dateCreated datePublished" datetime="2020-05-28T16:07:51+08:00">2020-05-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-09-11 11:26:41" itemprop="dateModified" datetime="2023-09-11T11:26:41+08:00">2023-09-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index">
                    <span itemprop="name">科研</span>
                  </a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>30k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>27 mins.</span>
            </span>
            <div class="post-description">经常使用的Pytorch代码片段技巧，以及环境搭建和使用易错点</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/84784982">Source</a></p>
<h1 id="网络模型构建">网络模型构建</h1>
<h2 id="nn.sequential和nn.modulelist的区别">1.
nn.Sequential和nn.ModuleList的区别</h2>
<p>简而言之就是，nn.Sequential类似于Keras中的贯序模型，它是Module的子类，在构建数个网络层之后会自动调用forward()方法，从而有网络模型生成。而nn.ModuleList仅仅类似于pytho中的list类型，只是将一系列层装入列表，并没有实现forward()方法，因此也不会有网络模型产生的副作用。两者使用的一个很好的例子如链接：<a
href="https://www.cnblogs.com/hellcat/p/8477195.html"
class="uri">https://www.cnblogs.com/hellcat/p/8477195.html</a></p>
<p>另外需要注意的是<strong>，网络中需要训练的参数一定要被正确地注册，比如如果使用了普通list,
dict等，之后一定要用nn.Sequential或者nn.ModuleList包装一下；甚至在定义网络时，网络的一个attribute是一个list,
list里面是一个或者多个子网络Module类别，也依然需要用nn.ModuleList替换掉这个普通的list，这样才能将模型参数和子网络模型参数顺利被优化器识别</strong>。否则，运行时不会报错，但是没有被注册的参数将不会被训练！并且，只有被正确注册之后，我们用model.cuda()，这些参数才会被自动迁移到GPU上，否则只会停留在CPU上。</p>
<h2 id="nn.modulelist可以由多维下标索引但用嵌套的list初始化时需注意">2.
nn.ModuleList可以由多维下标索引，但用嵌套的list初始化时需注意</h2>
<p>注意：
比如下面self.outs定义了具有二维索引的modulelist，需要注意的是，内层list也要加nn.ModuleList包装，这样内层list内部就是可迭代的Module
subclass对象，
<strong>否则内层就是普通的list，不满足输入参数的类型要求</strong>，pytorch不能正确识别它们是可训练的模型参数，会报错。</p>
<pre><code>class PoseNet(nn.Module):
    def __init__(self, nstack, inp_dim, oup_dim, bn=False, increase=128, **kwargs):
        &quot;&quot;&quot; Pack or initialize the trainable parameters of the network&quot;&quot;&quot;
        super(PoseNet, self).__init__()
        self.pre = nn.Sequential(
            Conv(3, 64, 7, 2, bn=bn),
            Conv(64, 128, bn=bn),
            nn.MaxPool2d(2, 2))

        self.outs = nn.ModuleList(
            [nn.ModuleList([Conv(inp_dim, oup_dim, 1, relu=False, bn=False) for j in range(4)]) for i in range(nstack)])</code></pre>
<h1 id="网络结构可视化">网络结构可视化</h1>
<h2 id="网络结构可视化-1">1. 网络结构可视化</h2>
<pre><code>def make_dot(var, params=None):
    &quot;&quot;&quot; Produces Graphviz representation of PyTorch autograd graph
    Blue nodes are the Variables that require grad, orange are Tensors
    saved for backward in torch.autograd.Function
    Args:
        var: output Variable
        params: dict of (name, Variable) to add names to node that
            require grad (TODO: make optional)
    &quot;&quot;&quot;
    if params is not None:
        assert isinstance(params.values()[0], Variable)
        param_map = {id(v): k for k, v in params.items()}

    node_attr = dict(style=&#39;filled&#39;,
                     shape=&#39;box&#39;,
                     align=&#39;left&#39;,
                     fontsize=&#39;12&#39;,
                     ranksep=&#39;0.1&#39;,
                     height=&#39;0.2&#39;)
    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=&quot;12,12&quot;))
    seen = set()

    def size_to_str(size):
        return &#39;(&#39; + (&#39;, &#39;).join([&#39;%d&#39; % v for v in size]) + &#39;)&#39;

    def add_nodes(var):
        if var not in seen:
            if torch.is_tensor(var):
                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=&#39;orange&#39;)
            elif hasattr(var, &#39;variable&#39;):
                u = var.variable
                name = param_map[id(u)] if params is not None else &#39;&#39;
                node_name = &#39;%s\n %s&#39; % (name, size_to_str(u.size()))
                dot.node(str(id(var)), node_name, fillcolor=&#39;lightblue&#39;)
            else:
                dot.node(str(id(var)), str(type(var).__name__))
            seen.add(var)
            if hasattr(var, &#39;next_functions&#39;):
                for u in var.next_functions:
                    if u[0] is not None:
                        dot.edge(str(id(u[0])), str(id(var)))
                        add_nodes(u[0])
            if hasattr(var, &#39;saved_tensors&#39;):
                for t in var.saved_tensors:
                    dot.edge(str(id(t)), str(id(var)))
                    add_nodes(t)

    add_nodes(var.grad_fn)
    return dot</code></pre>
<p>使用以上代码的例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># plot the model</span><br><span class="line"># net &#x3D; PoseNet(nstack&#x3D;4, inp_dim&#x3D;256, oup_dim&#x3D;68)</span><br><span class="line"># x &#x3D; Variable(torch.randn(1, 3, 512, 512))  # x的shape为(batch，channels，height，width)</span><br><span class="line"># y &#x3D; net(x)</span><br><span class="line"># g &#x3D; make_dot(y)</span><br><span class="line"># g.view()</span><br></pre></td></tr></table></figure>
<h2 id="类似于keras-打印网络每层输出的形状shape">2. 类似于keras,
打印网络每层输出的形状shape</h2>
<p>更新：推荐使用增强版工具 <a
href="https://github.com/nmhkahn/torchsummaryX"><strong>torchsummaryX</strong></a>，它可以同时给出输出shape，参数数目，以及乘加运算数目等</p>
<p>Improved visualization tool of <a
href="https://github.com/sksq96/pytorch-summary">torchsummary</a>. Here,
it visualizes kernel size, output shape, # params, and Mult-Adds. Also
the torchsummaryX can handle RNN, Recursive NN, or model with multiple
inputs.</p>
<hr />
<p>使用模仿keras中的summary()函数，<strong>torchsummary</strong> <a
href="https://www.jianshu.com/p/97c626d33924">转载自</a></p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">device &#x3D; torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # PyTorch v0.4.0</span><br><span class="line">model &#x3D; Net().to(device)</span><br><span class="line"></span><br><span class="line">summary(model, (1, 28, 28))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">            Conv2d-1           [-1, 10, 24, 24]             260</span><br><span class="line">            Conv2d-2             [-1, 20, 8, 8]           5,020</span><br><span class="line">         Dropout2d-3             [-1, 20, 8, 8]               0</span><br><span class="line">            Linear-4                   [-1, 50]          16,050</span><br><span class="line">            Linear-5                   [-1, 10]             510</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 21,840</span><br><span class="line">Trainable params: 21,840</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.00</span><br><span class="line">Forward&#x2F;backward pass size (MB): 0.06</span><br><span class="line">Params size (MB): 0.08</span><br><span class="line">Estimated Total Size (MB): 0.15</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="pytorch中layer的输出shape的尺寸取整">3.
pytorch中layer的输出shape的尺寸取整</h2>
<p>默认使用的是向下取整(floor)，如：</p>
<pre><code>self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)  # (batch_size, 512, 38, 38)

# (H + 2*p - d(ks - 1) - 1) / 2 + 1
# (38 + 12 - 6*(3 - 1) -1 ) / 2 + 1 = 19.5 向下取整 19
self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # (batch_size, 1024, 19, 19)</code></pre>
<p>Maxpooling层也是默认使用向下取整。如果想使用向上取整(ceil)，需要设置取整模式
ceil_mode=True， 默认是False</p>
<pre><code>nn.MaxPool2d(kernel_size=2, stride=2),  # (batch_size, 256, 37, 37), 想变成38*38可以使用　ceil_mode=True</code></pre>
<h2 id="超级给力的网络结构可视化工具netron-和-hiddenlayer">4.
超级给力的网络结构可视化工具：Netron 和 hiddenlayer</h2>
<p>前者是一款在浏览器中使用的可视化工具，可以使用pip安装，然后在命令行中输入netron或者netron
-b [model file]。需要把模型转换onnx模型。</p>
<pre><code>import torch.onnx

net = Hourglass2(2, 32, 1, Residual)
dummy_input = Variable(torch.randn(1, 32, 128, 128))
torch.onnx.export(net, dummy_input, &quot;model.onnx&quot;)</code></pre>
<p>后者是在jupyter notebook内使用的，例子如下：</p>
<p>Netron: <a href="https://github.com/lutzroeder/netron"
class="uri">https://github.com/lutzroeder/netron</a></p>
<p>hiddenlayer: <a
href="https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb">https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb</a></p>
<h2 id="计算网络模型的参数量和浮点运算数">5.
计算网络模型的参数量和浮点运算数</h2>
<p>使用第三方库thop</p>
<pre><code>from thop import profile
from thop import clever_format

dummy_input = torch.randn(1, 256, 128, 128)
flops, params = profile(MyNetwork, inputs=(dummy_input,))
flops, params = clever_format([flops, params], &quot;%.3f&quot;)
print(flops, params)</code></pre>
<h1 id="tensor的操作">Tensor的操作</h1>
<h2 id="tensor.view和tensor.permute-permute变换">１.
Tensor.view和Tensor.permute (permute:变换)</h2>
<p>torch中的view类似与numpy中的reshape，但不同的是前者会与变换后的tensor共享内存，而后者不共享不会影响原始数组。PyTorch在0.4版本以后提供了<strong><code>reshape</code></strong>方法，实现了类似于
<code>tensor.contigous().view(*args)</code>的功能，如果不关心底层数据是否使用了新的内存，则使用<strong><code>reshape</code></strong>方法更方便。</p>
<p>torch中的permute类似与numpy中的transpose.
<strong>注意：</strong>view只能用在contiguous的variable上。如果在view之前用了transpose,
permute等，需要用contiguous()来返回一个contiguous copy。</p>
<p>一个在SSD中的例子：</p>
<pre><code> y_loc = self.loc_layers[i](x)
            batch_size = y_loc.size(0)  # int
            # 此处y_loc的shape是(batch_size, anchor*4, Hi, Wi), pytorch的数据结构为(N, C, H, W)
            y_loc = y_loc.permute(0, 2, 3, 1).contiguous()
            # 此处y_loc的shape是(batch_size, Hi, Wi, anchor*4)
            # 要先把4放到最后，然后再改变shape 变成 ##### (batch_size, anchor_all_number, 4) ######,  anchor_all_number代表anchor的总数
            # permute可以对任意高维矩阵进行转置. 但没有 torch.permute() 这个调用方式， 只能 Tensor.permute()。
            # view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。
            y_loc = y_loc.view(batch_size, -1, 4)</code></pre>
<h2 id="若前面有一个tensor输入需要梯度则后面的输出也需要梯度">２.
若前面有一个tensor输入需要梯度，则后面的输出也需要梯度</h2>
<pre><code>x = torch.zeros((1), requires_grad=True)
# 若前面有一个输入需要梯度，则后面的输出也需要梯度。有的版本这里是默认值false
# 注：　Tensor变量的requires_grad的属性默认为False,若一个节点requires_grad被设置为True，那么所有依赖它的节点的requires_grad都为True。</code></pre>
<h2
id="tensor之间要是同一个数据类型dtype才能运算因此有时需要进行类型转换">３.
Tensor之间要是同一个数据类型<strong>dtype</strong>才能运算，因此有时需要进行类型转换</h2>
<p>比如即便都是int类型，但是一个是int16，一个是int32也需要先转换然后才能进行运算。使用Tensor.<code>to(torch.float32)进行转换。</code></p>
<pre><code># 因为loc_loss是float32，而num_matched_box是int64，没办法直接除所以转换一下
# 这里是不会损失数据的，因为假如batch_size=32,每个图片8732个，就只有8732*32=279424
# num_matched_boxes最大的值不会超过float32的表示范围的
num_matched_boxes = num_matched_boxes.to(torch.float32)  # Tensor dtype and/or device 转换
loc_loss /= num_matched_boxes   # 除以的是正样本的数目</code></pre>
<h2 id="tensor的clone和copy_的区别">４.
Tensor的clone和copy_的区别：</h2>
<p>copy_()不会追踪梯度，而clone会追踪并进行梯度的反向传播</p>
<p>Unlike copy_(), clone is recorded in the computation graph. Gradients
propagating to the cloned tensor will propagate to the original
tensor.</p>
<h2 id="tensor初始化">５. Tensor初始化</h2>
<h3 id="a.-torch.tensor和torch.from_numpy效果不同">a.
torch.tensor和torch.from_numpy()效果不同</h3>
<p>torch.tensor会重新拷贝原始数据，返回新的数据。如果不想拷贝，即内存相关联，对numpy
array来说可以使用torch.from_numpy()。</p>
<p>可以直接用list数据进行初始化，并且对list中某一个元素是tuple还是list都无所谓，如：</p>
<p>x= [(1,2,3,4), [5,6,7,8]] # x[0]是tuple而x[1]是list torch.tensor(x)
Out[20]: tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8]])</p>
<h2 id="data和detach的区别">６. data和detach()的区别</h2>
<p>推荐使用detach()，这样万一需要在反向传播时需要记录变量，可以报错指出，避免Tensor.data没有报错，但是计算错误的情况。</p>
<p><a href="https://zhuanlan.zhihu.com/p/38475183"
class="uri">https://zhuanlan.zhihu.com/p/38475183</a></p>
<blockquote>
<p><em>"However, .data can be unsafe in some cases. Any changes on
x.data wouldn’t be tracked by autograd, and the computed gradients would
be incorrect if x is needed in a backward pass. A safer alternative is
to use x.detach(), which also returns a Tensor that shares data with
requires_grad=False, but will have its in-place changes reported by
autograd if x is needed in backward."</em></p>
</blockquote>
<p><strong>Any in-place change on x.detach() will cause errors when x is
needed in backward, so .detach() is a safer way for the exclusion of
subgraphs from gradient computation. <a
href="https://github.com/pytorch/pytorch/issues/6990"
class="uri">https://github.com/pytorch/pytorch/issues/6990</a></strong></p>
<h2 id="pytorch中损失函数对tensor操作的reducesize_average参数说明">7.
pytorch中损失函数对tensor操作的reduce,size_average参数说明</h2>
<p>参考：<a
href="https://blog.csdn.net/u013548568/article/details/81532605"
class="uri">https://blog.csdn.net/u013548568/article/details/81532605</a></p>
<p>以及 <a href="https://zhuanlan.zhihu.com/p/91485607"
class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>size_average是说是不是对一个batch里面的所有的数据求均值</p>
<hr />
<p><strong>Reduce </strong> <strong>size_average </strong> * 意义* True
True 对batch里面的数据取均值loss.mean() True False
对batch里面的数据求和loss.sum() False – returns a loss per batch element
instead, 这个时候忽略size_average参数</p>
<hr />
<p>reduction : 可选的参数有：‘none’ | ‘elementwise_mean’ | ‘sum’,
正如参数的字面意思</p>
<hr />
<p>假设输入和target的大小分别是NxCxWxH，那么一旦reduce设置为False，loss的大小为NxCxWxH，返回每一个元素的loss</p>
<p><strong>reduction代表了上面的reduce和size_average双重含义，这也是文档里为什么说reduce和size_average要被Deprecated
的原因</strong></p>
<p>例子：</p>
<pre><code>import torch
import torch.nn as nn

# ----------------------------------- MSE loss

# 生成网络输出 以及 目标输出
output = torch.ones(2, 2, requires_grad=True) * 0.5
target = torch.ones(2, 2)

# 设置三种不同参数的L1Loss
reduce_False = nn.MSELoss(size_average=True, reduce=False) # 等效于reduction=&#39;none&#39;
size_average_True = nn.MSELoss(size_average=True, reduce=True) # 等效于reduction=&#39;mean&#39;
size_average_False = nn.MSELoss(size_average=False, reduce=True) # 等效于reduction=&#39;sum&#39;

o_0 = reduce_False(output, target)
o_1 = size_average_True(output, target)
o_2 = size_average_False(output, target)

print(&#39;\nreduce=False, 输出同维度的loss:\n{}\n&#39;.format(o_0))
print(&#39;size_average=True，\t求平均:\t{}&#39;.format(o_1))
print(&#39;size_average=False，\t求和:\t{}&#39;.format(o_2))</code></pre>
<p>输出：</p>
<pre><code>reduce=False, 输出同维度的loss:
tensor([[0.2500, 0.2500],
        [0.2500, 0.2500]], grad_fn=&lt;MseLossBackward&gt;)

size_average=True，  求平均:    0.25

size_average=False， 求和: 1.0</code></pre>
<h2 id="将tensor以及model迁移至cuda上">8.
将tensor以及model迁移至cuda上</h2>
<p><strong>将数据迁移到cuda上必须reassign，tensor.cuda()不是in-place操作，而是返回一个新的在cuda上的tensor。而网络模型不需要reassign.</strong></p>
<h3 id="a.-迁移tensor">a. 迁移tensor</h3>
<p><strong>问题：</strong>Hi, this works,
<code>a = torch.LongTensor(1).random_(0, 10).to("cuda")</code>. but this
won’t work:</p>
<p><strong>回答：</strong></p>
<p>If you are pushing tensors to a device or host, <strong>you have to
reassign them:</strong></p>
<pre><code>a = a.to(device=&#39;cuda&#39;)</code></pre>
<h3 id="b.-迁移模型">b. 迁移模型</h3>
<p><code>nn.Module</code>s push all parameters, buffers and submodules
recursively and don’t need the assignment.</p>
<blockquote>
<p>model.cuda()</p>
</blockquote>
<h2 id="对feature-map-即也是tensor做尺寸上的缩放">9. 对feature map
(即也是tensor)做尺寸上的缩放</h2>
<blockquote>
<p><code>torch.nn.functional.``interpolate</code>(<em>input</em>,
<em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>,
<em>align_corners=None</em>)</p>
</blockquote>
<p>默认的<em>align_corners=None就是和Opencv中的缩放规则保持一致，默认使用几何中心对齐，以此消除量化误差（或者说</em>计算出的灰度值也相对于源图像偏左偏上）<em>。</em></p>
<p>若做缩放，需要在缩放后图像 的位置上找到对应的 原始图像位置上
的像素值，有以下</p>
<p>SrcX=(dstX+0.5)* (srcWidth/dstWidth) -0.5 SrcY=(dstY+0.5) *
(srcHeight/dstHeight)-0.5</p>
<p>具体参考我的另一篇博客：</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/100150726"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/100150726</a></p>
<h2
id="注册参数--模型的普通类成员变量和pytorch中自动注册的parameter或者buffer区别">10.
注册参数--模型的普通类成员变量和Pytorch中自动注册的Parameter或者buffer区别</h2>
<p><a href="https://zhuanlan.zhihu.com/p/89442276"
class="uri">https://zhuanlan.zhihu.com/p/89442276</a></p>
<p>同时参考 第一节#网络模型构建中nn.ModuleList</p>
<p>模型中需要保存下来的参数包括两种:</p>
<p>一种是反向传播需要被optimizer更新的，称之为 parameter
一种是反向传播不需要被optimizer更新，称之为
buffer，它只能在forward中被更新。</p>
<p>第一种参数我们可以通过 model.parameters()
返回；第二种参数我们可以通过 model.buffers()
返回。因为我们的模型保存的是 state_dict 返回的
OrderDict，所以这两种参数不仅要满足是否需要被更新的要求，还会被保存到OrderDict。而<strong>普通的类成员变量属性是无法自动保存到模型的
OrderDict中去的。</strong></p>
<p>模型进行设备移动时，模型中注册的参数(Parameter和buffer)会同时进行移动，比如使用model.cuda()之后注册的参数parameter和buffer会自动迁移到cuda上去，<strong>而普通成员变量不会自动设备移动</strong>。</p>
<h2 id="tensor的缩放">11. Tensor的缩放</h2>
<p><a
href="https://discuss.pytorch.org/t/how-do-i-interpolate-directly-on-tensor/23081/3">一个讨论</a></p>
<p>使用functional.interpolate函数对Tensor进行缩放，注意，bicubic插值算法只能对4-D
Tensor正常操作，如果是3-D操作，需要先扩展纬度之后再进行。下面例子中，hmps是一个shape=(N,
C, H,
W)的张量，bicubic默认会对<font color="#dddd00">最后两个维度进行缩放插值</font>，而batch
size and channels (dim0,
dim1)不变。即把张量的空间分辨率（长和宽）放大。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sizeHW &#x3D; (args.square_length, args.square_length)  # 设square_length是hmps的4倍</span><br><span class="line">hmps1 &#x3D; torch.nn.functional.interpolate(hmps, size&#x3D;sizeHW, mode&#x3D;&quot;bicubic&quot;)</span><br><span class="line">hmps2 &#x3D; torch.nn.functional.interpolate(hmps, scale_factor&#x3D;4, mode&#x3D;&quot;bicubic&quot;)</span><br><span class="line">t &#x3D; (hmps1&#x3D;&#x3D;hmps2).all() &gt;&gt;&gt; 将输出一个为True的Tensor</span><br></pre></td></tr></table></figure>
<h2 id="tensor的contiguousstoragestride">12.
Tensor的contiguous、storage、stride</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/64551412">PyTorch中的contiguous</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/101434655" target="_blank" rel="noopener">Pytorch中的Size,
storage offset, stride概念</a></p>
<p><strong><code>contiguous</code></strong>直观的解释是<strong>Tensor底层一维数组元素的存储顺序与Tensor按<font color="#dddd00">行优先</font>一维展开的元素顺序是否一致</strong>。pytorch中的storage指的是连续的内存块，而tensor则是映射到storage的视图，他把单条的内存区域映射成了n维的空间视图。size是tensor的维度，storage
offset是数据在storage中的开头索引，stride是storage中对应于tensor的相邻维度间第一个索引的跨度。示例详情见链接。</p>
<h2
id="如何判断索引或者切片是对原始tensor的view共享内存还是copy">如何判断索引或者切片是对原始Tensor的view（共享内存）还是copy?</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/81352299">umpy
array 以及pytorch tensor 的索引（切片索引，整型索引）</a></p>
<h3 id="简而言之">简而言之</h3>
<p>基本的切片索引（slice）是对原始数组的一个view，会影响原始数组的。而后者情况比较复杂，不过是basic
indexing，则依然是原始数组的一个view，如果是advanced
indexig，会用原始数组创建一个新的数组，不会影响原始数据。</p>
<p>x[1, 3:8], x[2:5,
6:9]是基本slice索引，是view共享内存的；而x[1,2]是基本integer索引,
x[[1,2], [1,4]]是高级integer索引。</p>
<h3
id="针对pytorch-tensor可以通过data_ptr查看第一个元素地址是否相同来判断">针对pytorch
tensor可以通过data_ptr()查看第一个元素地址是否相同来判断</h3>
<p>full list of view ops in PyTorch</p>
<blockquote>
<ul>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.as_strided"><code>as_strided()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach"><code>detach()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.diagonal"><code>diagonal()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand"><code>expand()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as"><code>expand_as()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.movedim"><code>movedim()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.narrow"><code>narrow()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"><code>permute()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.select"><code>select()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze"><code>squeeze()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.transpose"><code>transpose()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t"><code>t()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T"><code>T</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.real"><code>real</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.imag"><code>imag</code></a></li>
<li><code>view_as_real()</code></li>
<li><code>view_as_imag()</code></li>
<li><a
href="https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.unflatten"><code>unflatten()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unfold"><code>unfold()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze"><code>unsqueeze()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"><code>view()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as"><code>view_as()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unbind"><code>unbind()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.split"><code>split()</code></a></li>
<li><code>split_with_sizes()</code></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.chunk"><code>chunk()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.indices"><code>indices()</code></a>
(sparse tensor only)</li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.values"><code>values()</code></a>
(sparse tensor only)</li>
</ul>
<p>It’s also worth mentioning a few ops with special behaviors:</p>
<ul>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape"><code>reshape()</code></a>,
<a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape_as"><code>reshape_as()</code></a>
and <a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.flatten"><code>flatten()</code></a>
can return either a view or new tensor, user code shouldn’t rely on
whether it’s view or not.</li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous"><code>contiguous()</code></a>
returns <strong>itself</strong> if input tensor is already contiguous,
otherwise it returns a new contiguous tensor by copying data.</li>
</ul>
</blockquote>
<h3 id="numpy中的判断准则">Numpy中的判断准则</h3>
<p>Pytorch的行为是模仿Numpy的，numpy提供了详细的说明，什么时候是view，什么时候是copy:</p>
<p>https://numpy.org/doc/stable/reference/arrays.indexing.html</p>
<blockquote>
<p>Advanced indexing is triggered when the selection object,
<em>obj</em>, is a non-tuple sequence object, an <a
href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray"><code>ndarray</code></a>
(of data type integer or bool), or a tuple with at least one sequence
object or ndarray (of data type integer or bool). There are two types of
advanced indexing: integer and Boolean.</p>
<p>Advanced indexing always returns a <em>copy</em> of the data
(contrast with basic slicing that returns a <a
href="https://numpy.org/doc/stable/glossary.html#term-view">view</a>).</p>
<p>具体说明参加上面链接</p>
</blockquote>
<h1 id="pytorch训练数据准备">pytorch训练数据准备</h1>
<h2 id="dataloader-类">1. DataLoader 类</h2>
<h3 id="参数说明-摘录自">参数说明 <a
href="https://blog.csdn.net/weixin_42236288/article/details/80893882%C2%A0">摘录自</a></h3>
<p>1. dataset：加载的数据集(Dataset对象) 2. batch_size：batch size 3.
shuffle:：是否将数据打乱 4. sampler： 样本抽样，后续会详细介绍 5.
num_workers：使用多进程加载的进程数，0代表不使用多进程 6. collate_fn：
<strong>如何将多个样本数据拼接成一个batch</strong>，一般使用默认的拼接方式即可，即默认调用default_collate，但是如果数据异常往往无法自动处理而报错
7. pin_memory：是否将数据保存在pin memory区，pin
memory中的数据转到GPU会快一些 8.
drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</p>
<h3 id="对于-pin_memory-的解释摘录自">对于 pin_memory 的解释：<a
href="https://oldpan.me/archives/pytorch-to-use-multiple-gpus">摘录自</a></h3>
<p><strong>pin_memory就是锁页内存</strong></p>
<blockquote>
<p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。
主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。显卡中的显存全部是锁页内存,当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>
</blockquote>
<h3
id="collate_fn的作用和默认的default_collate">collate_fn的作用，和默认的default_collate</h3>
<p>这个函数的决定<strong>如何将多个样本数据拼接成一个batch</strong>，一般使用默认的拼接方式即可，即默认调用default_collate，它会自动地把__getitem__生成的单个张量，数字，字符串，列表，字典等进行串联拼接成batch的数据。但是如果数据异常往往无法自动处理而报错。比如如果我们读取图片失败，default_collate自动处理时就会报错：</p>
<blockquote>
<p>TypeError: batch must contain tensors, numbers, dicts or lists; found
&lt;class 'NoneType'&gt;</p>
</blockquote>
<p>这个时候需要靠我们自定义collate_fn，返回的batch数据会自定清理掉不合法的数据，并且我们还可以通过自己的collate_fn自由地对dataloader生产的batch数据做各种选择处理。</p>
<h2 id="多进程读取hdf5文件支持的不好以及解决办法">2.
多进程读取HDF5文件支持的不好以及解决办法</h2>
<p>DataLoader中多进程高效处理hdf5文件：</p>
<p><a
href="https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643">摘录自</a></p>
<p><strong>My recommendations:</strong></p>
<blockquote>
<ul>
<li>Use HDF5 in version 1.10 (better multiprocessing handling),</li>
<li>Because an opened HDF5 file isn’t pickleable and to send Dataset to
workers’ processes it needs to be serialised with pickle, you can’t open
the HDF5 file in <code>__init__</code>. Open it in
<code>__getitem__</code>and <strong>store as the singleton!</strong>. Do
not open it each time as it introduces huge overhead.</li>
<li>Use <code>DataLoader</code> with <code>num_workers</code> &gt; 0
(reading from hdf5 (i.e. hard drive) is slow) and
<code>batch_sampler</code> (random access to hdf5 (i.e. hard drive) is
slow).</li>
</ul>
</blockquote>
<p><strong>Sample code:</strong></p>
<pre><code>class H5Dataset(torch.utils.data.Dataset):
    def __init__(self, path):
        self.file_path = path
        self.dataset = None
        with h5py.File(self.file_path, &#39;r&#39;) as file:
            self.dataset_len = len(file[&quot;dataset&quot;])

    def __getitem__(self, index):
        if self.dataset is None:
            self.dataset = h5py.File(self.file_path, &#39;r&#39;)[&quot;dataset&quot;]
        return self.dataset[index]

    def __len__(self):
        return self.dataset_len</code></pre>
<p><strong>如何安装HDF5 1.10以及对应的python hdf5的包呢？ 查看<a
href="https://blog.csdn.net/xiaojiajia007/article/details/87873443">我的另一个博客</a></strong></p>
<p><strong>使用命令行环境变量HDF5_DIR=/usr/local/hdf5 pip install
h5py。具体如下：</strong></p>
<p>Then you should be fine. Install HDF5 1.10 from source into somewhere
you want to. The .tar is here:
https://www.hdfgroup.org/HDF5/release/obtainsrc5110.html Follow the
install readme but basically you just need to give it a directory with:
&gt; ./configure --prefix=/usr/local/h5py before you make.</p>
<p>Now install with you anaconda version of python. You may want to make
a separate environment using conda but that's your call.</p>
<p>Remove the h5py you have with anaconda using &gt; conda uninstall
h5py or &gt; pip uninstall h5py</p>
<p>Then use pip to reinstall h5py but pointing to the HDF5 library you
made from source. From here: http://docs.h5py.org/en/latest/build.html
<strong>&gt; HDF5_DIR=/usr/local/hdf5 pip install h5py</strong></p>
<p>Then you should be good. Open up a python terminal and test if you
can use SWMR mode: &gt; import h5py &gt; f = h5py.File("./swmr.h5", 'a',
libver='latest', swmr=True)</p>
<h2 id="多进程准备数据随机种子seed的问题">3.
多进程准备数据<strong>随机种子seed</strong>的问题</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/87881231">参见我另一个博客</a></p>
<h2 id="如何加速训练数据准备并载入gpu训练">4.
如何加速训练数据准备并载入GPU训练</h2>
<p>参考一个知乎博客，data_prefetcher： <a
href="https://zhuanlan.zhihu.com/p/80695364"
class="uri">https://zhuanlan.zhihu.com/p/80695364</a></p>
<p>以及Pytorch论坛上的一个讨论： <a
href="https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee">https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee</a></p>
<h1 id="pytorch训练阶段">Pytorch训练阶段</h1>
<h2 id="stochastic-weight-averaging-in-pytorch">1. Stochastic Weight
Averaging in PyTorch</h2>
<p>这是一种model weight
average策略，类似于模型集成，常常用来刷指标，提高模型的泛化精度。详细说明请见我的单独博客：</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/90748115"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90748115</a></p>
<h2 id="通过梯度积累变相增大batch-size">2. 通过梯度积累变相增大batch
size</h2>
<p><a
href="https://www.zhihu.com/question/303070254/answer/573037166">详情请见
PyTorch中在反向传播前为什么要手动将梯度清零？ - Pascal的回答 - 知乎</a>
但是需要注意的是，因为BN层的参数是在
forward()阶段更新的，这样积累梯度并没有增大BN layers的实际batch
size。可以通过减少BN层的 momentum
值，让BN层动态更新统计参数时能够记住更长。</p>
<h1 id="pytorch-测试阶段">Pytorch 测试阶段</h1>
<h2 id="正确的测试预测时间计时代码">1.
正确的测试（预测）时间计时代码</h2>
<pre><code>torch.cuda.synchronize() # 等待当前设备上所有流中的所有核心完成
start = time.time() 
result = model(input) 
torch.cuda.synchronize() 
end = time.time()</code></pre>
<p>在pytorch里面，程序的执行都是异步的。如果没有torch.cuda.synchronize()
，测试的时间会很短，因为执行完end=time.time()程序就退出了，后台的cu也因为python的退出退出了，如果采用torch.cuda.synchronize()
，代码会同步cu的操作，等待gpu上的操作都完成了再继续成形end =
time.time()</p>
<p>如果将代码改为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start &#x3D; time.time()</span><br><span class="line">result &#x3D; model(input)</span><br><span class="line">print(result)</span><br><span class="line">end &#x3D; time.time()</span><br><span class="line">1234</span><br></pre></td></tr></table></figure>
<p>这时候会发祥第三段代码和第二段代码的时间是类似的，因为第三段代码会等待gpu上的结果执行完传给print函数，所以真个的时间就和第二段同步的操作的时间基本上是一致的了，将print(result)换成result.cpu()结果是一致的惹。</p>
<p>原文：https://blog.csdn.net/u013548568/article/details/81368019</p>
<h2 id="训练测试两个阶段需要注意设置不同状态-参考">2.
训练，测试两个阶段需要注意设置不同状态 <a
href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/10">参考</a></h2>
<h3 id="a.-model.train和model.val">a. model.train()和model.val()</h3>
<p>比如BN和Dropout</p>
<p>During eval <code>Dropout</code> is deactivated and just passes its
input. During the training the probability <code>p</code> is used to
drop activations. Also, the activations are scaled with
<code>1./p</code> as otherwise the expected values would differ between
training and eval.</p>
<pre><code>drop = nn.Dropout()
x = torch.ones(1, 10)

# Train mode (default after construction)
drop.train()
print(drop(x))

# Eval mode
drop.eval()
print(drop(x))</code></pre>
<h3
id="b.-测试val时不光要设置model.eval-为了防止内存爆炸应该追加torch.no_grad">b.
测试（val)时不光要设置<code>model.eval()</code>
，为了防止内存爆炸，应该追加<code>torch.no_grad()</code></h3>
<ul>
<li><code>model.eval()</code> will notify all your layers that you are
in eval mode, that way, batchnorm or dropout layers will work in eval
model instead of training mode.</li>
<li><code>torch.no_grad():</code> impacts the autograd engine and
deactivate it. It will reduce memory usage and speed up computations but
you won’t be able to backprop (which you don’t want in an eval script).
注意，<code>torch.no_grad()是</code>context manager。</li>
</ul>
<h2 id="dropout里需要设置训练标志位否则会踩坑">3.
Dropout里需要设置训练标志位，否则会踩坑</h2>
<h3
id="使用f.dropout-nn.functional.dropout-的时候需要设置它的可选参数training-state">使用F.dropout
( nn.functional.dropout )的时候需要设置它的可选参数training state</h3>
<p>这个状态参数与模型整体的一致，否则就是out=out，没有效果，具体说明见链接
<a
href="https://www.zhihu.com/question/67209417/answer/302434279">查看</a></p>
<pre><code>Class DropoutFC(nn.Module):
   def __init__(self):
       super(DropoutFC, self).__init__()
       self.fc = nn.Linear(100,20)

   def forward(self, input):
       out = self.fc(input)
       out = F.dropout(out, p=0.5, training=self.training) # set dropout&#39;s training sate
       return out

Net = DropoutFC()
Net.train()

# train the Net
#作者：雷杰
#链接：https://www.zhihu.com/question/67209417/answer/302434279</code></pre>
<h3
id="或者直接使用nn.dropout即利用包装后的layer">或者直接使用nn.Dropout()，即利用包装后的layer</h3>
<p>nn.Dropout()实际上是对F.dropout的一个包装,
也将self.training传入了)</p>
<pre><code>Class DropoutFC(nn.Module):
  def __init__(self):
      super(DropoutFC, self).__init__()
      self.fc = nn.Linear(100,20)
      self.dropout = nn.Dropout(p=0.5)

  def forward(self, input):
      out = self.fc(input)
      out = self.dropout(out)
      return out
Net = DropoutFC()
Net.train()

# train the Net</code></pre>
<h2 id="多gpu模型权重的保存与加载">4. 多GPU模型权重的保存与加载</h2>
<p>Instead of deleting the “module.” string from all the state_dict
keys, you can save your model with:
<code>torch.save(model.module.state_dict(), path_to_file)</code> instead
of <code>torch.save(model.state_dict(), path_to_file)</code>
<strong><em>that way you don’t get the “module.” string to begin
with…</em></strong></p>
<pre><code># original saved file with DataParallel
state_dict = torch.load(&#39;myfile.pth.tar&#39;)
# 把所有的张量加载到CPU中
# torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage)

# create new OrderedDict that does not contain `module.`
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
model.load_state_dict(new_state_dict)

############## 还有一个可用的封装更好的函数
# 加载模型，解决命名和维度不匹配问题,解决多个gpu并行
def load_state_keywise(model, model_path):
    model_dict = model.state_dict()
    pretrained_dict = torch.load(model_path, map_location=&#39;cpu&#39;)
    key = list(pretrained_dict.keys())[0]
    # 1. filter out unnecessary keys
    # 1.1 multi-GPU -&gt;CPU
    if (str(key).startswith(&#39;module.&#39;)):
        pretrained_dict = {k[7:]: v for k, v in pretrained_dict.items() if
                           k[7:] in model_dict and v.size() == model_dict[k[7:]].size()}
    else:
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           k in model_dict and v.size() == model_dict[k].size()}
    # 2. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)
    # 3. load the new state dict
    model.load_state_dict(model_dict)

 ################## 更简单直接的方式 ##################
# Instead of deleting the “module.” string from all the state_dict keys, you can save your model with:

torch.save(model.module.state_dict(), path_to_file)
# instead of

torch.save(model.state_dict(), path_to_file)

# that way you don’t get the “module.” string to begin with…</code></pre>
<h2 id="恢复保存的优化器状态optimizer-checkpoint-resume继续优化">5.
恢复保存的优化器状态(optimizer checkpoint resume)，继续优化</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/88417329"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/88417329</a></p>
<h2 id="载入模型权重gpu内存被额外占用的bug解决">6.
载入模型权重GPU内存被额外占用的bug解决</h2>
<h3
id="分布式多进程中的这种情况的现象是对于同一个python进程pid号相同会两次占用固定大小的gpu内存">分布式/多进程中的这种情况的现象是，对于同一个python进程（pid号相同）会两次占用固定大小的gpu内存</h3>
<p>观察到的现象是python进程多于预期应有的进程数。比如我们单机多卡分布式训练，已经完成了网络模型的in-place参数设备转换，即network.cuda()，现在我们有4块GPU，我们在程序中的每一个进程分配一块GPU时本来应该只有4个进程，每个进程占用一定的GPU显存，但实际情况如所示：</p>
<pre><code>Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 1291 G /usr/lib/xorg/Xorg 153MiB |
| 0 2549 G fcitx-qimpanel 14MiB |
| 0 21740 G compiz 138MiB |
| 0 22840 C /home/jia/.virtualenvs/phoenix/bin/python 6097MiB | 
| 0 22841 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22842 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22843 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 23207 G /opt/teamviewer/tv_bin/TeamViewer 24MiB |
| 0 23985 G .../Software/pycharm-2019.2.4/jbr/bin/java 12MiB |
| 1 22841 C /home/jia/.virtualenvs/phoenix/bin/python 6129MiB |
| 2 22842 C /home/jia/.virtualenvs/phoenix/bin/python 6227MiB |
| 3 22843 C /home/jia/.virtualenvs/phoenix/bin/python 6229MiB</code></pre>
<p>原因：在同一个cuda上之后不使用的内存将会被自动销毁并回收，但是对于不同GPU之间目前没有自动的内存管理机制??，如果某一个进程在cuda0上实例化的tensor
x，在另一个使用cuda2的进程中使用了，但cuda2上的进程并没有对tensor
x进行内存销毁回收，造成GPU内存的占用。</p>
<p>解决办法：在当前进程中销毁不在同一个cuda上的内存垃圾，或者载入权重时使用torch.load(model_path,
<strong>map_location='cpu'</strong>)</p>
<h3
id="gpu预训练保存的权值可以直接载入到cpu下的网络模型network中并且载入之后network的参数会移到预训练权值所在的device上">GPU预训练保存的权值可以直接载入到CPU下的网络模型network中，并且载入之后network的参数会移到预训练权值所在的device上</h3>
<p>如下图所示：</p>
<p><img
src="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20201215222559.png" /></p>
<p>这个行为挺诡异，按照正常的设计逻辑，本来CPU的模型直接载入GPU预训练权值应该会因为device不同而报错（cpu,
cuda0)但结果并没有，可以成功载入，并且载入之后CPU下的模型network的device也变成cuda0了。甚至我们可以仅仅载入某一layer的权值，那么这一layer的weight.data将变到cuda0上，而其没有载入更改的layer的weight.data仍然在cpu上！</p>
<p>解决办法同上一种情况，把GPU预训练权值map到cpu上之后再network.load_state_dict()。</p>
<h1 id="pytorch的内存优化和加速">Pytorch的内存优化和加速</h1>
<p><strong>有一个 pytorch提速指南： <a
href="https://zhuanlan.zhihu.com/p/39752167"
class="uri">https://zhuanlan.zhihu.com/p/39752167</a></strong></p>
<p><strong>可以参考 <a
href="https://blog.csdn.net/jacke121/article/details/81329679%C2%A0">原文</a></strong></p>
<h2 id="使用inplace减少内存开辟从而压缩内存需求">1.
使用inplace减少内存开辟，从而压缩内存需求</h2>
<p>对于in-place operation的解读，见：<a
href="https://blog.csdn.net/u012436149/article/details/80819523"
class="uri">https://blog.csdn.net/u012436149/article/details/80819523</a></p>
<p>以及：<a
href="https://blog.csdn.net/york1996/article/details/81835873"
class="uri">https://blog.csdn.net/york1996/article/details/81835873</a></p>
<p>如，ReLu(inplace=True)</p>
<p>在官方问文档中由这一段话：</p>
<blockquote>
<p>如果你使用了in-place
operation而没有报错的话，那么你可以确定你的梯度计算是正确的。<strong>因为Pytorch在内存占用和执行速度上做了很多算法优化，哪些需要保留梯度不能使用in-place覆盖就显得不那么显而易见了，不能单纯地用原始梯度反向传播过程来决定。</strong></p>
</blockquote>
<p>inplace只是可以节省存储tensor的内存，但是PYTORCH中的自动微分机制仍然能够追踪，对于内存来说inplace可能是同一个对象，但是对于autograd来说，依然是两个不同的对象。
一个例子：<a
href="https://discuss.pytorch.org/t/why-relu-inplace-true-does-not-give-error-in-official-resnet-py-but-it-gives-error-in-my-code/21004/3">resnet</a></p>
<blockquote>
<p><strong><code>inplace</code> means that it will not allocate new
memory and change tensors inplace</strong>. <strong>But from the
autograd point of view, you have two different tensors (even though they
actually share the same memory)</strong>. One is the output of conv (or
batchnorm for resnet) and one is the output of the relu.</p>
</blockquote>
<h2 id="torch.backends.cudnn.benchmark-true">2.
torch.backends.cudnn.benchmark = True</h2>
<p>在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</p>
<h2 id="torch.cuda.empty_cache">3. torch.cuda.empty_cache()</h2>
<p>因为每次迭代都会引入点临时变量，会导致训练速度越来越慢，基本呈线性增长。开发人员还不清楚原因，但如果周期性的使用torch.cuda.empty_cache()的话就可以解决这个问题。</p>
<h2 id="使用checkpoint分阶段计算这样可以在显卡上放下更大的网络">4.
使用checkpoint分阶段计算，这样可以在显卡上放下更大的网络</h2>
<p>知乎回答的一个例子：https://www.zhihu.com/question/274635237/answer/574193034</p>
<h2 id="尝试nvidia-apex-16位浮点数扩展">5. 尝试Nvidia Apex
16位浮点数扩展</h2>
<p>温馨提示：我的另一篇博客<a
href="https://blog.csdn.net/xiaojiajia007/article/details/84784982">pip
install, python setup.py, egg-info的说明--以Nvidia Apex安装为例</a></p>
<h3 id="clean-the-old-install-before-rebuilding">Clean the old install
before rebuilding:</h3>
<blockquote>
<p>pip uninstall apex cd apex_repo_dir rm -rf build (if present) rm -rf
apex.egg-info (if present)</p>
</blockquote>
<h3 id="install-package">Install package：</h3>
<blockquote>
<p>pip install -v --no-cache-dir --global-option="--cpp_ext"
--global-option="--cuda_ext" ./</p>
<p># --no-cache-dir 清除安装缓存文件</p>
</blockquote>
<p>或者</p>
<blockquote>
<p>python setup.py install --cuda_ext --cpp_ext</p>
</blockquote>
<h3
id="ps-如果遇到cuda版本不兼容的问题解决办法见若pytorch升级到1.3cuda10.1则没有这个error了">ps:
如果遇到Cuda版本不兼容的问题，解决办法见：（若pytorch升级到1.3，cuda10.1则没有这个error了）</h3>
<p><a
href="https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952">https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952</a></p>
<p>如果没有出现其他error，可以直接使用上面链接的建议，删除版本检查抛出的报错。</p>
<p>讨论：</p>
<p><a href="https://github.com/NVIDIA/apex/issues/350"
class="uri">https://github.com/NVIDIA/apex/issues/350</a></p>
<p><a href="https://github.com/NVIDIA/apex/pull/323"
class="uri">https://github.com/NVIDIA/apex/pull/323</a></p>
<h2 id="pytorch内存泄露僵尸进程解决办法-原文链接">6.
Pytorch内存泄露（僵尸进程）解决办法 <a
href="https://blog.csdn.net/liuyifang0810680/article/details/79628394%C2%A0">原文链接</a></h2>
<p>nvidia-smi 发现内存泄露问题，即没有进程时，内存被占用</p>
<blockquote>
<p>fuser -v /dev/nvidia* 发现僵尸进程</p>
<p>ps x |grep python|awk '{print $1}'|xargs kill 杀死所有僵尸进程</p>
</blockquote>
<p>命令解读：</p>
<p>ps x: show all process of current user</p>
<p>grep python: to get process that has python in command line</p>
<p>awk '{print $1}': to get the related process pidxargs kill`: to kill
the process</p>
<p>note: make sure you don’t kill other processes! do ps x |grep python
first.</p>
<h2 id="相关的进程和内存管理bash-cmd-命令行命令">7.
相关的进程和内存管理bash cmd (命令行命令）</h2>
<p>nvidia-smi -l xxx
监控GPU，动态刷新信息（默认5s刷新一次），按Ctrl+C停止，可指定刷新频率，以秒为单位；</p>
<p>watch -n 1 nvidia-smi <strong>实时监控GPU</strong>； watch -n 1
lscpu实时监控CPU，watch是周期性的执行下个程序 ps -elf进程查看，
<strong>ps -elf | grep python
查看Python子进程</strong>，这个也是命令比较实用，能够用在监视其他基于python解释器运行的进程，
kill -9 [PID]杀死进程PID。</p>
<blockquote>
<blockquote>
<p><strong>watch -n 5 -t -d=cumulative 'command'</strong></p>
</blockquote>
<p>watch是周期性的执行下个程序，并全屏显示执行结果</p>
<p>-n 每隔5秒周期执行一次</p>
<p>-t 开头的间隔时间和信息等不显示</p>
<p><strong>-d=cumulative 发生变动的地方高亮</strong></p>
</blockquote>
<h2 id="如何才能使用-tensor-core">8. 如何才能使用 Tensor Core</h2>
<p><strong>Convolutions</strong>: For cudnn versions 7.2 and ealier,
<span class="citation" data-cites="vaibhav0195">@vaibhav0195</span> is
correct: input channels, output channels, and batch size should be
multiples of 8 to use tensor cores. However, this requirement is lifted
for cudnn versions 7.3 and later. <strong>For cudnn 7.3 and later, you
don't need to worry about making your channels/batch size multiples of 8
to enable Tensor Core use</strong>.</p>
<p><strong>GEMMs (fully connected layers)</strong>: For matrix A x
matrix B, where A has size [I, J] and B has size [J, K], I, J, and K
must be multiples of 8 to use Tensor Cores. This requirement exists for
all cublas and cudnn versions. This means that for <strong>bare fully
connected layers, the batch size, input features, and output features
must be multiples of 8</strong>, and** for RNNs, you usually (but not
always, it can be architecture-dependent depending on what you use for
encoder/decoder) need to have batch size, hidden size, embedding size,
and dictionary size as multiples of 8.**</p>
<h2
id="apex的fused-adam的特点是模型参数更新迭代得比pytorch中原生的adam快"><strong>9.
Apex的Fused
Adam的特点是模型参数更新迭代得比Pytorch中原生的Adam快</strong></h2>
<p>What is the difference between FusedAdam optimizer in Nvidia AMP
package with the Adam optimizer in Pytorch?</p>
<p><a
href="https://discuss.pytorch.org/t/fusedadam-optimizer-in-nvidia-amp-package/47544">摘录自</a></p>
<blockquote>
<p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries
out optimizer.step() by looping over parameters, and launching a series
of kernels for each parameter. This can require hundreds of small
launches that are mostly bound by CPU-side Python looping and kernel
launch overhead, resulting in poor device utilization. Currently, the
FusedAdam implementation in Apex flattens the parameters for the
optimization step, then carries out the optimization step itself via a
fused kernel that combines all the Adam operations. In this way, the
loop over parameters as well as the internal series of Adam operations
for each parameter are fused such that optimizer.step() requires only a
few kernel launches.</p>
<p>The current implementation (in Apex master) is brittle and only works
with Amp opt_level O2. I’ve got a WIP branch to make it work for any
opt_level (<a href="https://github.com/NVIDIA/apex/pull/351"
class="uri">https://github.com/NVIDIA/apex/pull/351</a>). I recommend
waiting until this is merged then trying it.</p>
</blockquote>
<h1 id="pytorch-使用陷阱易错点"><strong>Pytorch
使用陷阱，易错点</strong></h1>
<h2
id="tensor.expand-expand_as是共享内存的只是原始数据的一个视图-view并没有在扩展的axis上有新的数据复制牵一发动全身"><strong>1.
Tensor.expand, expand_as是共享内存的，只是原始数据的一个视图
view，并没有在扩展的axis上有新的数据复制，牵一发动全身！</strong></h2>
<p><strong>为了避免对 expand()
后对某个channel操作会影响原始tensor的全部元素，需要使用clone()</strong></p>
<p>如果没有clone()，对mask_miss的某个通道赋值后，所有通道上的tensor都会变成1！</p>
<blockquote>
<p># Notice! expand does not allocate more memory but just make the
tensor look as if you expanded it. # You should call .clone() on the
resulting tensor if you plan on modifying it #
https://discuss.pytorch.org/t/very-strange-behavior-change-one-element-of-a-tensor-will-influence-all-elements/41190</p>
</blockquote>
<pre><code>mask = mask_miss.expand_as(sxing).clone()            # type: torch.Tensor
mask[:, :, -2, :, :] = 1   # except for person mask channel</code></pre>
<h2 id="损失计算图因为pytorch的动态机制越来越大直到耗尽内存">2.
损失计算图因为Pytorch的动态机制越来越大，直到耗尽内存</h2>
<p>摘录自</p>
<p>常见的原因有</p>
<h3
id="在循环中使用全局变量当做累加器且累加梯度信息">在循环中使用全局变量当做累加器，且累加梯度信息</h3>
<p>举个例子，下面的代码中</p>
<pre><code>total_loss=0
for i in range(10000):
  optimizer.zero_grad()
  output=model(input)
  loss=criterion(output)
  loss.backward() # 计算的梯度自动叠加到各个权重的grad上，并且计算完成后销毁计算图！！！
  optimizer.step()
  total_loss+=loss
  #这里total_loss是跨越循环的变量，起着累加的作用，
  #loss变量是带有梯度的tensor，会保持历史梯度信息，在循环过程中会不断积累梯度信息到tota_loss，占用内存</code></pre>
<p>以上例子的修正方法是在循环中的最后一句修改为：</p>
<p>total_loss+=float(loss)</p>
<p>或者 total_loss += loss.item() #
tensor.item()是取张量的python数值</p>
<p>利用类型变换解除梯度信息，这样，多次累加不会累加梯度信息。</p>
<h3 id="局部变量逗留导致内存泄露">局部变量逗留导致内存泄露</h3>
<p>局部变量通常在变量作用域之外会被Python自动销毁，在作用域之内，不需要的临时变量可以使用del
x来销毁。</p>
<h3
id="list数据类型不断append增长了计算图大小">list数据类型，不断append增长了计算图大小</h3>
<h2 id="pytorch中的batch-normalization-layer踩坑">3. Pytorch中的Batch
Normalization layer踩坑</h2>
<p>详情查看我的另一篇博客：<a
href="https://blog.csdn.net/xiaojiajia007/article/details/90115174"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90115174</a></p>
<h2
id="优化器的权值衰减weight_decay项导致的隐蔽bug即是的网络的权值不断减少至0">4.
优化器的权值衰减weight_decay项导致的隐蔽bug，即是的网络的权值不断减少至0</h2>
<p>摘录自：<a href="https://zhuanlan.zhihu.com/p/91485607"
class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>我们都知道weight_decay指的是权值衰减，（<strong>注意：<font color="#dddd00">权值衰减不等价于在原损失的基础上加上一个L2惩罚项！</font>
具体说明见下面那条笔记</strong>），使得模型趋向于选择更小的权重参数，起到正则化的效果。但是我经常会忽略掉这一项的存在，从而引发了意想不到的问题。</p>
<p>这次的坑是这样的，在训练一个ResNet50的时候，网络的高层部分layer4暂时没有用到，因此也并不会有梯度回传，于是我就放心地将ResNet50的所有参数都传递给Optimizer进行更新了，想着layer4应该能保持原来的权重不变才对。但是实际上，尽管layer4没有梯度回传，但是weight_decay的作用仍然存在，它使得layer4权值越来越小，趋向于0。后面需要用到layer4的时候，发现输出异常（接近于0），才注意到这个问题的存在。</p>
<p>虽然这样的情况可能不容易遇到，但是还是要谨慎：暂时不需要更新的权值，一定不要传递给Optimizer，避免不必要的麻烦。</p>
<h2 id="l2正则不等于权值衰减">5. L2正则不等于权值衰减</h2>
<p>权值衰减（Weight
Decay）：在网络权值通过损失函数更新后，直接再减去权值本身的一个倍数，可以写成
W(t+1)’ = W(t+1)-W(t)；</p>
<p>而 L2正则（L2
Regulation）：在原有的算是函数基础上，添加了网络权值平方和*一个倍数，L'
=
L+1/2∑w^2，注意在参数更新，对L'求关于某个分量的导数时其他参数视作常数，导数为0。</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/104045066"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/104045066</a></p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111126481.png" /></p>
<p>在Pytorch中，对于SGD优化器，两者是等效的，但是对于Adam优化器，两者作用有差别，对于Adam会有耦合的错误。</p>
<p>我看到有的开源项目中(<a
href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/utils/utils.py#L60">链接</a>)，SGD使用weight
decay，而Adam中没有使用weight decay。</p>
<p>具体分析见下面两个文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40814046"
class="uri">https://zhuanlan.zhihu.com/p/40814046</a>，</p>
<p><a href="https://zhuanlan.zhihu.com/p/63982470"
class="uri">https://zhuanlan.zhihu.com/p/63982470</a></p>
<h2 id="pytorch中的优化器weight-decay默认对bias偏置也起作用不合理">6.
Pytorch中的优化器weight decay默认对bias(偏置)也起作用，不合理</h2>
<p>添加偏置是有必要的：</p>
<p>https://zhuanlan.zhihu.com/p/158739701</p>
<blockquote>
<p>一般来说，我们只会对神经网络的<strong>权值</strong>进行正则操作，使得权值具有一定的稀疏性[21]或者控制其尺寸，使得其不至于幅度太大，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是<strong>调节每一层超平面的方向</strong>（因为<img
src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D"
alt="[公式]" />就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是<strong>调节每一层超平面的平移长度的</strong>，如果你对偏置进行了正则，那么我们的<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。</p>
</blockquote>
<p>解决方法不止一种</p>
<p>例如进行weight和bias参数过滤：https://www.cnblogs.com/lart/p/10672935.html</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">self.opti &#x3D; optim.SGD(</span><br><span class="line">    [</span><br><span class="line">        # 不对bias参数执行weight decay操作，weight decay主要的作用就是通过对网络</span><br><span class="line">        # 层的参数（包括weight和bias）做约束（L2正则化会使得网络层的参数更加平滑）达</span><br><span class="line">        # 到减少模型过拟合的效果。</span><br><span class="line">        &#123;&#39;params&#39;: [param for name, param in self.net.named_parameters()</span><br><span class="line">                    if name[-4:] &#x3D;&#x3D; &#39;bias&#39;],</span><br><span class="line">         &#39;lr&#39;: 2 * self.args[&#39;lr&#39;]&#125;,</span><br><span class="line">        &#123;&#39;params&#39;: [param for name, param in self.net.named_parameters()</span><br><span class="line">                    if name[-4:] !&#x3D; &#39;bias&#39;],</span><br><span class="line">         &#39;lr&#39;: self.args[&#39;lr&#39;],</span><br><span class="line">         &#39;weight_decay&#39;: self.args[&#39;weight_decay&#39;]&#125;</span><br><span class="line">    ],</span><br></pre></td></tr></table></figure>
<h2 id="torch.sqrt在0处的左导数没有定义会返回nan换用-torch.norm">7.
torch.sqrt()在0处的左导数没有定义，会返回nan，换用 torch.norm()</h2>
<p>例如： # https://github.com/pytorch/pytorch/issues/2421 # norm =
torch.sqrt((x1 - t1)**2 + (x2 - t2)**2)</p>
<p><code>norm = (torch.stack((x1, x2)) - torch.stack((t1, t2))).norm(dim=0)</code></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>Donate me via WeChatPay or AliPay.</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      <div style="display: inline-block;">
        <img src="/images/weixin_pay.jpg" alt="Jia Li WechatPay">
        <p>WechatPay</p>
      </div>
      <div style="display: inline-block;">
        <img src="/images/ali_pay.jpg" alt="Jia Li AliPay">
        <p>AliPay</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Jia Li
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://hellojialee.github.io/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/" title="Pytorch实用指南">https://hellojialee.github.io/2020/05/28/Pytorch实用指南/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/pytorch/" rel="tag"># pytorch</a>
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/15/%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E6%98%93%E9%94%99%E7%82%B9/" rel="prev" title="英语写作">
      <i class="fa fa-chevron-left"></i> 英语写作
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/25/%E7%BC%96%E7%A8%8B%E5%88%B7%E9%A2%98/" rel="next" title="算法刷题">
      算法刷题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <!-- Insert clustrmaps.com, the following single line is inserted by jialee-->
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=QL-1Sagpgczc7G2fmX1QXKQOnj-EMUBDxB3pA6RxWIY'></script>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#网络模型构建"><span class="nav-number">1.</span> <span class="nav-text">网络模型构建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#nn.sequential和nn.modulelist的区别"><span class="nav-number">1.1.</span> <span class="nav-text">1.
nn.Sequential和nn.ModuleList的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn.modulelist可以由多维下标索引但用嵌套的list初始化时需注意"><span class="nav-number">1.2.</span> <span class="nav-text">2.
nn.ModuleList可以由多维下标索引，但用嵌套的list初始化时需注意</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#网络结构可视化"><span class="nav-number">2.</span> <span class="nav-text">网络结构可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#网络结构可视化-1"><span class="nav-number">2.1.</span> <span class="nav-text">1. 网络结构可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#类似于keras-打印网络每层输出的形状shape"><span class="nav-number">2.2.</span> <span class="nav-text">2. 类似于keras,
打印网络每层输出的形状shape</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch中layer的输出shape的尺寸取整"><span class="nav-number">2.3.</span> <span class="nav-text">3.
pytorch中layer的输出shape的尺寸取整</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超级给力的网络结构可视化工具netron-和-hiddenlayer"><span class="nav-number">2.4.</span> <span class="nav-text">4.
超级给力的网络结构可视化工具：Netron 和 hiddenlayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算网络模型的参数量和浮点运算数"><span class="nav-number">2.5.</span> <span class="nav-text">5.
计算网络模型的参数量和浮点运算数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor的操作"><span class="nav-number">3.</span> <span class="nav-text">Tensor的操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor.view和tensor.permute-permute变换"><span class="nav-number">3.1.</span> <span class="nav-text">１.
Tensor.view和Tensor.permute (permute:变换)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#若前面有一个tensor输入需要梯度则后面的输出也需要梯度"><span class="nav-number">3.2.</span> <span class="nav-text">２.
若前面有一个tensor输入需要梯度，则后面的输出也需要梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor之间要是同一个数据类型dtype才能运算因此有时需要进行类型转换"><span class="nav-number">3.3.</span> <span class="nav-text">３.
Tensor之间要是同一个数据类型dtype才能运算，因此有时需要进行类型转换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor的clone和copy_的区别"><span class="nav-number">3.4.</span> <span class="nav-text">４.
Tensor的clone和copy_的区别：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor初始化"><span class="nav-number">3.5.</span> <span class="nav-text">５. Tensor初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-torch.tensor和torch.from_numpy效果不同"><span class="nav-number">3.5.1.</span> <span class="nav-text">a.
torch.tensor和torch.from_numpy()效果不同</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data和detach的区别"><span class="nav-number">3.6.</span> <span class="nav-text">６. data和detach()的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch中损失函数对tensor操作的reducesize_average参数说明"><span class="nav-number">3.7.</span> <span class="nav-text">7.
pytorch中损失函数对tensor操作的reduce,size_average参数说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将tensor以及model迁移至cuda上"><span class="nav-number">3.8.</span> <span class="nav-text">8.
将tensor以及model迁移至cuda上</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-迁移tensor"><span class="nav-number">3.8.1.</span> <span class="nav-text">a. 迁移tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#b.-迁移模型"><span class="nav-number">3.8.2.</span> <span class="nav-text">b. 迁移模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对feature-map-即也是tensor做尺寸上的缩放"><span class="nav-number">3.9.</span> <span class="nav-text">9. 对feature map
(即也是tensor)做尺寸上的缩放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#注册参数--模型的普通类成员变量和pytorch中自动注册的parameter或者buffer区别"><span class="nav-number">3.10.</span> <span class="nav-text">10.
注册参数--模型的普通类成员变量和Pytorch中自动注册的Parameter或者buffer区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor的缩放"><span class="nav-number">3.11.</span> <span class="nav-text">11. Tensor的缩放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor的contiguousstoragestride"><span class="nav-number">3.12.</span> <span class="nav-text">12.
Tensor的contiguous、storage、stride</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何判断索引或者切片是对原始tensor的view共享内存还是copy"><span class="nav-number">3.13.</span> <span class="nav-text">如何判断索引或者切片是对原始Tensor的view（共享内存）还是copy?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简而言之"><span class="nav-number">3.13.1.</span> <span class="nav-text">简而言之</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#针对pytorch-tensor可以通过data_ptr查看第一个元素地址是否相同来判断"><span class="nav-number">3.13.2.</span> <span class="nav-text">针对pytorch
tensor可以通过data_ptr()查看第一个元素地址是否相同来判断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy中的判断准则"><span class="nav-number">3.13.3.</span> <span class="nav-text">Numpy中的判断准则</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch训练数据准备"><span class="nav-number">4.</span> <span class="nav-text">pytorch训练数据准备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#dataloader-类"><span class="nav-number">4.1.</span> <span class="nav-text">1. DataLoader 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#参数说明-摘录自"><span class="nav-number">4.1.1.</span> <span class="nav-text">参数说明 摘录自</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对于-pin_memory-的解释摘录自"><span class="nav-number">4.1.2.</span> <span class="nav-text">对于 pin_memory 的解释：摘录自</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#collate_fn的作用和默认的default_collate"><span class="nav-number">4.1.3.</span> <span class="nav-text">collate_fn的作用，和默认的default_collate</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多进程读取hdf5文件支持的不好以及解决办法"><span class="nav-number">4.2.</span> <span class="nav-text">2.
多进程读取HDF5文件支持的不好以及解决办法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多进程准备数据随机种子seed的问题"><span class="nav-number">4.3.</span> <span class="nav-text">3.
多进程准备数据随机种子seed的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何加速训练数据准备并载入gpu训练"><span class="nav-number">4.4.</span> <span class="nav-text">4.
如何加速训练数据准备并载入GPU训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch训练阶段"><span class="nav-number">5.</span> <span class="nav-text">Pytorch训练阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-weight-averaging-in-pytorch"><span class="nav-number">5.1.</span> <span class="nav-text">1. Stochastic Weight
Averaging in PyTorch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#通过梯度积累变相增大batch-size"><span class="nav-number">5.2.</span> <span class="nav-text">2. 通过梯度积累变相增大batch
size</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch-测试阶段"><span class="nav-number">6.</span> <span class="nav-text">Pytorch 测试阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正确的测试预测时间计时代码"><span class="nav-number">6.1.</span> <span class="nav-text">1.
正确的测试（预测）时间计时代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练测试两个阶段需要注意设置不同状态-参考"><span class="nav-number">6.2.</span> <span class="nav-text">2.
训练，测试两个阶段需要注意设置不同状态 参考</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-model.train和model.val"><span class="nav-number">6.2.1.</span> <span class="nav-text">a. model.train()和model.val()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#b.-测试val时不光要设置model.eval-为了防止内存爆炸应该追加torch.no_grad"><span class="nav-number">6.2.2.</span> <span class="nav-text">b.
测试（val)时不光要设置model.eval()
，为了防止内存爆炸，应该追加torch.no_grad()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout里需要设置训练标志位否则会踩坑"><span class="nav-number">6.3.</span> <span class="nav-text">3.
Dropout里需要设置训练标志位，否则会踩坑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用f.dropout-nn.functional.dropout-的时候需要设置它的可选参数training-state"><span class="nav-number">6.3.1.</span> <span class="nav-text">使用F.dropout
( nn.functional.dropout )的时候需要设置它的可选参数training state</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#或者直接使用nn.dropout即利用包装后的layer"><span class="nav-number">6.3.2.</span> <span class="nav-text">或者直接使用nn.Dropout()，即利用包装后的layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多gpu模型权重的保存与加载"><span class="nav-number">6.4.</span> <span class="nav-text">4. 多GPU模型权重的保存与加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#恢复保存的优化器状态optimizer-checkpoint-resume继续优化"><span class="nav-number">6.5.</span> <span class="nav-text">5.
恢复保存的优化器状态(optimizer checkpoint resume)，继续优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#载入模型权重gpu内存被额外占用的bug解决"><span class="nav-number">6.6.</span> <span class="nav-text">6.
载入模型权重GPU内存被额外占用的bug解决</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分布式多进程中的这种情况的现象是对于同一个python进程pid号相同会两次占用固定大小的gpu内存"><span class="nav-number">6.6.1.</span> <span class="nav-text">分布式&#x2F;多进程中的这种情况的现象是，对于同一个python进程（pid号相同）会两次占用固定大小的gpu内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu预训练保存的权值可以直接载入到cpu下的网络模型network中并且载入之后network的参数会移到预训练权值所在的device上"><span class="nav-number">6.6.2.</span> <span class="nav-text">GPU预训练保存的权值可以直接载入到CPU下的网络模型network中，并且载入之后network的参数会移到预训练权值所在的device上</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch的内存优化和加速"><span class="nav-number">7.</span> <span class="nav-text">Pytorch的内存优化和加速</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用inplace减少内存开辟从而压缩内存需求"><span class="nav-number">7.1.</span> <span class="nav-text">1.
使用inplace减少内存开辟，从而压缩内存需求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch.backends.cudnn.benchmark-true"><span class="nav-number">7.2.</span> <span class="nav-text">2.
torch.backends.cudnn.benchmark &#x3D; True</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch.cuda.empty_cache"><span class="nav-number">7.3.</span> <span class="nav-text">3. torch.cuda.empty_cache()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用checkpoint分阶段计算这样可以在显卡上放下更大的网络"><span class="nav-number">7.4.</span> <span class="nav-text">4.
使用checkpoint分阶段计算，这样可以在显卡上放下更大的网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#尝试nvidia-apex-16位浮点数扩展"><span class="nav-number">7.5.</span> <span class="nav-text">5. 尝试Nvidia Apex
16位浮点数扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#clean-the-old-install-before-rebuilding"><span class="nav-number">7.5.1.</span> <span class="nav-text">Clean the old install
before rebuilding:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#install-package"><span class="nav-number">7.5.2.</span> <span class="nav-text">Install package：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ps-如果遇到cuda版本不兼容的问题解决办法见若pytorch升级到1.3cuda10.1则没有这个error了"><span class="nav-number">7.5.3.</span> <span class="nav-text">ps:
如果遇到Cuda版本不兼容的问题，解决办法见：（若pytorch升级到1.3，cuda10.1则没有这个error了）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch内存泄露僵尸进程解决办法-原文链接"><span class="nav-number">7.6.</span> <span class="nav-text">6.
Pytorch内存泄露（僵尸进程）解决办法 原文链接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相关的进程和内存管理bash-cmd-命令行命令"><span class="nav-number">7.7.</span> <span class="nav-text">7.
相关的进程和内存管理bash cmd (命令行命令）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何才能使用-tensor-core"><span class="nav-number">7.8.</span> <span class="nav-text">8. 如何才能使用 Tensor Core</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#apex的fused-adam的特点是模型参数更新迭代得比pytorch中原生的adam快"><span class="nav-number">7.9.</span> <span class="nav-text">9.
Apex的Fused
Adam的特点是模型参数更新迭代得比Pytorch中原生的Adam快</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch-使用陷阱易错点"><span class="nav-number">8.</span> <span class="nav-text">Pytorch
使用陷阱，易错点</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tensor.expand-expand_as是共享内存的只是原始数据的一个视图-view并没有在扩展的axis上有新的数据复制牵一发动全身"><span class="nav-number">8.1.</span> <span class="nav-text">1.
Tensor.expand, expand_as是共享内存的，只是原始数据的一个视图
view，并没有在扩展的axis上有新的数据复制，牵一发动全身！</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失计算图因为pytorch的动态机制越来越大直到耗尽内存"><span class="nav-number">8.2.</span> <span class="nav-text">2.
损失计算图因为Pytorch的动态机制越来越大，直到耗尽内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#在循环中使用全局变量当做累加器且累加梯度信息"><span class="nav-number">8.2.1.</span> <span class="nav-text">在循环中使用全局变量当做累加器，且累加梯度信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#局部变量逗留导致内存泄露"><span class="nav-number">8.2.2.</span> <span class="nav-text">局部变量逗留导致内存泄露</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#list数据类型不断append增长了计算图大小"><span class="nav-number">8.2.3.</span> <span class="nav-text">list数据类型，不断append增长了计算图大小</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch中的batch-normalization-layer踩坑"><span class="nav-number">8.3.</span> <span class="nav-text">3. Pytorch中的Batch
Normalization layer踩坑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化器的权值衰减weight_decay项导致的隐蔽bug即是的网络的权值不断减少至0"><span class="nav-number">8.4.</span> <span class="nav-text">4.
优化器的权值衰减weight_decay项导致的隐蔽bug，即是的网络的权值不断减少至0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#l2正则不等于权值衰减"><span class="nav-number">8.5.</span> <span class="nav-text">5. L2正则不等于权值衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch中的优化器weight-decay默认对bias偏置也起作用不合理"><span class="nav-number">8.6.</span> <span class="nav-text">6.
Pytorch中的优化器weight decay默认对bias(偏置)也起作用，不合理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch.sqrt在0处的左导数没有定义会返回nan换用-torch.norm"><span class="nav-number">8.7.</span> <span class="nav-text">7.
torch.sqrt()在0处的左导数没有定义，会返回nan，换用 torch.norm()</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jia Li"
      src="/images/touxiang1.jpg">
  <p class="site-author-name" itemprop="name">Jia Li</p>
  <div class="site-description" itemprop="description">Be happy, be healthy!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hellojialee" title="GitHub → https://github.com/hellojialee" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=LVAnDxwAAAAJ" title="Scholar → https://scholar.google.com/citations?user=LVAnDxwAAAAJ" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hellojialee@gmail.com" title="E-Mail → mailto:hellojialee@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://faculty.hfut.edu.cn/lijia/zh_CN/index.htm" title="SchoolPage → http://faculty.hfut.edu.cn/lijia/zh_CN/index.htm" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>SchoolPage</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/xiaojiajia007" title="CSDN → https://blog.csdn.net/xiaojiajia007" rel="noopener" target="_blank"><i class="fa fa-fw fa-codiepie"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/lijia.ustc" title="ZhiHu → https://www.zhihu.com/people/lijia.ustc" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>ZhiHu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jia Li</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="Symbols count total">144k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">2:10</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.7.0
  </div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
