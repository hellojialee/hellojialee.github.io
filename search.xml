<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hexo实用技巧随记</title>
    <url>/2023/03/14/hello-world/</url>
    <content><![CDATA[<h1 id="hexo简单使用"><a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>简单使用</h1>
<p>This is your very first post. Check <a
href="https://hexo.io/docs/">documentation</a> for more info. If you get
any problems when using Hexo, you can find the answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<p>注意⚠️：需要在博客项目文件夹路径下执行hexo命令！命令行export代理后，部署到github更快</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line"><span class="comment"># or hexo s</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line"><span class="comment"># or hexo g</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line"><span class="comment"># or hexo d</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a>，
https://zhuanlan.zhihu.com/p/71544809</p>
<p>逻辑：新建hexo分支，保存hexo博客源码到github的hexo
分支，然后默认待渲染的HTML项目放到master分支</p>
<h3 id="generate-and-deploy-in-one-command">Generate and deploy in one
command</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure>
<h2 id="attention">Attention!</h2>
<p>注意hexo和node版本号必须要匹配，否则会出现编译公式显示问题，以及无法部署到github上等问题（例如<a
href="https://evestorm.github.io/posts/430/">ERR_INVALID_ARG_TYPE</a>）。升级hexo配置有点麻烦，可以暂时降级node到合适的版本号，例如Hexo
4.2.1应该使用Node
12.18.3，这样可以快速修复或者继续使用之前的hexo项目。</p>
<p>降低版本方法参考：https://blog.csdn.net/weixin_46316234/article/details/121475363</p>
<ol type="1">
<li><p>安装node版本管理模块 n</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo npm install n -g</span><br></pre></td></tr></table></figure></li>
<li><p>可以根据自己的需要选择安装版本[可选] 2.1 安装稳定版</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo n stable</span><br></pre></td></tr></table></figure>
<p>2.2 安装最新版</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo n latest</span><br></pre></td></tr></table></figure>
<p>2.3 版本降级/升级</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo n 版本号   &#x2F;&#x2F; 8.16.0   &#x2F;   12.8.3</span><br></pre></td></tr></table></figure>
<p>在主题文件夹--&gt;package.json--&gt;搜索“version"，得知当前我使用的Next主题版本号为"version":
"7.7.0"</p>
<p>Hexo版本和官方最新落后情况可以使用命令 <code>npm outdate</code>
查询：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309120950382.png" alt="image-20230912上午94927638" style="zoom: 67%;" /></p></li>
</ol>
<hr />
<p>以下搬运自我的CSDN博客 <a
href="https://blog.csdn.net/xiaojiajia007/article/details/104105250">hexo实用技巧随记</a></p>
<h1 id="关于hexo">关于hexo</h1>
<h2 id="默认起始页定制">默认起始页定制</h2>
<p>hexo默认起始页是发表的博客罗列，我懒得整了。想要自定义起始页可以参考这个博文：https://qingchen1995.gitee.io/2021/10/20/2110-Change-Blog-Index/</p>
<h2 id="安装hexo及主题">安装hexo及主题</h2>
<p>安装hexo，可以参考：<a
href="https://ryanluoxu.github.io/2017/11/24/%E7%94%A8-Hexo-%E5%92%8C-GitHub-Pages-%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">用
Hexo 和 GitHub Pages 搭建博客</a></p>
<p>解决latex公式渲染的问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
<p>然后到配置的主题next下的配置文件
/Users/lijia/科研&amp;学习/github博客/themes/next/_config.yml，更改配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  # Default (true) will load mathjax &#x2F; katex script on demand.</span><br><span class="line">  # That is it only render those page which has &#96;mathjax: true&#96; in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax &#x2F; katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: false</span><br><span class="line"></span><br><span class="line">  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br><span class="line">    # See: https:&#x2F;&#x2F;mhchem.github.io&#x2F;MathJax-mhchem&#x2F;</span><br><span class="line">    mhchem: false</span><br></pre></td></tr></table></figure>
<h2 id="行内数学公式显示异常">行内数学公式显示异常</h2>
<p>未知问题：有时在typora中显示正常的行内数学公式，在hexo中就显示异常了，比如下面一个情况<img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124330.png"
alt="在这里插入图片描述" /></p>
<p><strong>暂时的解决办法</strong>：</p>
<ul>
<li><p>通过尝试发现，对于有些显示不正常的行内公式<span
class="math inline">\(X=\left(x_{1}, x_{2}, \ldots,
x_{n}\right)\)</span>我们只需要在前面或者后面一个$符号插入一个空格,
或者在前后都插入一个空格，公式渲染显示就正常了。</p></li>
<li><p><del>而且有时例如需要( x_{1} 在(后插入一个空格。</del></p></li>
<li><p>有可能是node版本和hexo不兼容出现的编译问题</p></li>
</ul>
<h2 id="更改markdown信息头模板">更改markdown信息头模板</h2>
<p>默认的信息头不全，我们把markdown模板
/Users/lijia/科研&amp;学习/github博客/scaffolds/post.md
里面最开始的信息更改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">description: 点击阅读前文前, 首页能看到的文章的简短描述 </span><br><span class="line">categories:</span><br></pre></td></tr></table></figure>
<h2 id="添加搜索功能">添加搜索功能</h2>
<p>首先在blog文件夹目录下安装依赖
<code>npm install hexo-generator-searchdb --save</code></p>
<p>然后在站点配置文件中_config.yml中增加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>最后在主题配置文件（_config.yml），修改local_search的值如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;flashlab&#x2F;hexo-generator-search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="关于next主题">关于next主题</h1>
<p>首先进入本地blog目录，使用git clone，把主题项目克隆下来,
<code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code>
然后在站点配置文件中更改theme选项为 next.</p>
<h2 id="更改主题细节设置">更改主题细节设置</h2>
<h3 id="页面配置">页面配置</h3>
<p>设置开始页面显示，设置头像，设置社交媒体链接等等，
可以参考这个博客：<a
href="http://www.zhangblog.com/2019/06/16/hexo04/">Linux下使用
github+hexo 搭建个人博客04-next主题优化</a> 或者参考这个文章：<a
href="https://zhuanlan.zhihu.com/p/25959864">Hexo+Next配置Blog</a></p>
<h3 id="更改默认字体大小">更改默认字体大小</h3>
<p>主题默认的字体太大了，一个页面放不下很多内容，根据我们创建的路径，在/Users/lijia/科研&amp;学习/github博客/themes/next/source/css/_variables/base.styl
中找到font size，把font-size-base默认值更改即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Font size</span><br><span class="line">$font-size-base           &#x3D; (hexo-config(&#39;font.enable&#39;) and hexo-config(&#39;font.global.size&#39;) is a &#39;unit&#39;) ? unit(hexo-config(&#39;font.global.size&#39;), em) : 14px;</span><br></pre></td></tr></table></figure>
<h3 id="网址缩略图">网址缩略图</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124411.png"
alt="在这里插入图片描述" />
如果我们想要更改网址栏左侧的缩略图标，我们先制作好图标，放入对应的
<code>/Users/lijia/科研&amp;学习/github博客/themes/next/source/images</code>
文件夹下，然后更改主题（我们用的是next)下的配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: &#x2F;images&#x2F;flower-16x16.png</span><br><span class="line">  medium: &#x2F;images&#x2F;flower.ico # 建议使用ico图标而不是png图片，以防兼容性问题</span><br><span class="line">  apple_touch_icon: &#x2F;images&#x2F;flower.png # apple-touch-icon-next.png</span><br><span class="line">  #safari_pinned_tab: &#x2F;images&#x2F;flower.svg #logo.svg</span><br><span class="line">  #android_manifest: &#x2F;images&#x2F;manifest.json</span><br><span class="line">  #ms_browserconfig: &#x2F;images&#x2F;browserconfig.xml</span><br></pre></td></tr></table></figure>
<h3
id="接入评论系统以及文章阅读次数显示">接入评论系统以及文章阅读次数显示</h3>
<p>参考： http://www.zhangblog.com/2019/06/16/hexo05/
http://www.zhangblog.com/2019/06/16/hexo06/ 使用 <a
href="https://leancloud.cn/dashboard/applist.html#/apps">LeanCloud</a>，可以同时实现评论留言系统以及文章阅读次数统计。
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Valine</span><br><span class="line"># For more information: https:&#x2F;&#x2F;valine.js.org, https:&#x2F;&#x2F;github.com&#x2F;xCss&#x2F;Valine</span><br><span class="line">valine:</span><br><span class="line">  enable: true</span><br><span class="line">  appid: hcHrBNtFBdqhBMIhjL67LT0t-gzGzoHsz # Your leancloud application appid</span><br><span class="line">  appkey: Pe1HEgFp3AUcNCQ7dpQ3HRE4 # Your leancloud application appkey</span><br><span class="line">  notify: false # Mail notifier</span><br><span class="line">  verify: false # Verification code</span><br><span class="line">  placeholder: 欢迎讨论留言！ # Comment box placeholder</span><br><span class="line">  avatar: mm # Gravatar style</span><br><span class="line">  guest_info: nick,mail,link # Custom comment header</span><br><span class="line">  pageSize: 10 # Pagination size</span><br><span class="line">  language: # Language, available values: en, zh-cn</span><br><span class="line">  visitor: true # Article reading statistic # 这里控制显示文章阅读次数</span><br></pre></td></tr></table></figure> 设置完成后文章的开头就会显示如下： <img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124192.png"
alt="在这里插入图片描述" /> 增加了 Views 和
Valine，而博客文章下面就有了留言板。</p>
<h3 id="side-bar添加访问者ip地区地图">Side bar添加访问者ip地区地图</h3>
<p>使用clustrmaps，选择插件风格，拷贝网站出现的脚本 javascript代码</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">'text/javascript'</span> id=<span class="string">'clustrmaps'</span> src=<span class="string">'//cdn.clustrmaps.com/map_v2.js?cl=fff&amp;w=a&amp;t=n&amp;d=QL- 巴拉巴拉省略号 -xB3pA6RxWIY'</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>粘贴到Next主题下某个位置，参考链接中给出了两个可插入上述代码的配置文件，这里我们选择了其中一个位置的配置文件<code>/***jlee.github.io/themes/next/layout/_macro/sidebar.swig</code>粘贴代码，插入如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;aside class&#x3D;&quot;sidebar&quot;&gt;</span><br><span class="line">    &lt;div class&#x3D;&quot;sidebar-inner&quot;&gt;</span><br><span class="line"></span><br><span class="line">      &#123;%- set display_toc &#x3D; page.toc.enable and display_toc %&#125;</span><br><span class="line">      &#123;%- if display_toc %&#125;</span><br><span class="line">        &#123;%- set toc &#x3D; toc(page.content, &#123; &quot;class&quot;: &quot;nav&quot;, list_number: page.toc.number, max_depth: page.toc.max_depth &#125;) %&#125;</span><br><span class="line">        &#123;%- set display_toc &#x3D; toc.length &gt; 1 and display_toc %&#125;</span><br><span class="line">      &#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">      &lt;!-- Insert clustrmaps.com, the following single line is inserted by jialee--&gt;</span><br><span class="line">      &lt;script type&#x3D;&#39;text&#x2F;javascript&#39; id&#x3D;&#39;clustrmaps&#39; src&#x3D;&#39;&#x2F;&#x2F;cdn.clustrmaps.com&#x2F;map_v2.js?cl&#x3D;fff&amp;w&#x3D;a&amp;t&#x3D;n&amp;d&#x3D;QL- 巴拉巴拉省略号 -xB3pA6RxWIY&#39;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure>
<p>最后我们来看一下效果</p>
<p><img src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309121038585.png" alt="image-20230912上午103806357" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>测试专用</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>算法刷题</title>
    <url>/2020/08/25/%E7%BC%96%E7%A8%8B%E5%88%B7%E9%A2%98/</url>
    <content><![CDATA[<h1 id="oj系统使用">OJ系统使用</h1>
<h2 id="输入输出">输入输出</h2>
<h3 id="stdin.readline输入">stdin.readline输入</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = sys.stdin.readline().strip()  <span class="comment"># 不像input自动去除\n，这里需要用手动strip去除换行符</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:  <span class="comment"># 遇到数据结尾跳出</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        lines = line.split()  <span class="comment"># 变成列表</span></span><br><span class="line">        print(int(lines[<span class="number">0</span>]) + int(lines[<span class="number">1</span>]))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(lines)</span></span><br></pre></td></tr></table></figure>
<h3 id="stdin.readlines一次性输入">stdin.readlines一次性输入</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">lines=sys.stdin.readlines()  <span class="comment"># 一次性读完文件</span></span><br><span class="line">print(lines)</span><br></pre></td></tr></table></figure>
<h3 id="list-map输入成int">list map输入成int</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data=list(map(int,input().split()))</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<h3 id="input函数循环输入">input()函数循环输入</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = []</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        str1 = input()  <span class="comment"># input()会自动把接受字符末尾的\n(换行符)给去掉</span></span><br><span class="line">        <span class="keyword">if</span> str1 == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        lines.append(str1)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">print(lines)</span><br><span class="line">print(<span class="string">","</span>.join(lines))  <span class="comment"># 用","分隔lines里面的元素组成字符串</span></span><br></pre></td></tr></table></figure>
<h3
id="print函数控制打印后不换号而是输出空格">print()函数控制打印后不换号，而是输出空格</h3>
<p>例如：print(x, end=' ') # 使用end参数控制</p>
<h2 id="循环输入带来的调试误区">循环输入带来的调试误区</h2>
<p>注意因为循环输入中使用了
<code>except: break</code>导致调试时如果报错运行到了这里直接break，而不是爆出原来的错误，<font color="#dd00dd">调试的时候应该把这个except去除从而可以暴露出bug！</font></p>
<h1 id="字符串题">字符串题</h1>
<h2 id="ascii码">ASCII码</h2>
<p>ord()
函数获取字符的Unicode，但是最前面的是ASCII码，因此可以用ord()获得字符的ASCII码。</p>
<p>chr()获取ASCII码对应的字符</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; ord(&#39;A&#39;)</span><br><span class="line">65</span><br><span class="line">&gt;&gt;&gt; ord(&#39;中&#39;)</span><br><span class="line">20013</span><br><span class="line">&gt;&gt;&gt; chr(66)</span><br><span class="line">&#39;B&#39;</span><br><span class="line">&gt;&gt;&gt; chr(25991)</span><br><span class="line">&#39;文&#39;</span><br></pre></td></tr></table></figure>
<p><strong>UTF-8</strong>是一种 Unicode
的实现方式。对于英语字母，<strong>UTF-8</strong> 编码和
<strong>ASCII</strong> 码是相同的。</p>
<h2 id="前缀后缀">前缀后缀</h2>
<p>使用<strong>startswith() and
endswith()</strong>代替切片进行序列前缀或后缀的检查</p>
<p>比如用<code>if foo.startswith(‘bar’):</code> 会优于
<code>if foo[:3] == ‘bar’:</code></p>
<p>print('-' *
80)打印一串‘--------------------------------------------------------------------------------‘</p>
<h2 id="字符串计数">字符串计数</h2>
<p>写出一个程序，接受一个由字母和数字组成的字符串，和一个字符，然后输出输入字符串中含有该字符的个数。不区分大小写。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">line  =input().lower()</span><br><span class="line">m = input().lower()</span><br><span class="line">print( line.count(m))</span><br></pre></td></tr></table></figure>
<h2 id="最大回文字符子串">最大回文字符子串</h2>
<p>除了暴力循环判断，也可以使用动态规划算法求解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">        ans = <span class="string">""</span></span><br><span class="line">        <span class="comment"># 枚举子串的长度 l+1</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 枚举子串的起始位置 i，这样可以通过 j=i+l 得到子串的结束位置</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">                j = i + l</span><br><span class="line">                <span class="keyword">if</span> j &gt;= len(s):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> l == <span class="number">0</span>:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span>  <span class="comment"># 针对长度为1的子串</span></span><br><span class="line">                <span class="keyword">elif</span> l == <span class="number">1</span>:  <span class="comment"># 针对长度为2的子串</span></span><br><span class="line">                    dp[i][j] = (s[i] == s[j])</span><br><span class="line">                <span class="keyword">else</span>:  <span class="comment"># 针对长度大于等于3的子串，其他情形都能最终拆解成这三种情况</span></span><br><span class="line">                    dp[i][j] = (dp[i + <span class="number">1</span>][j - <span class="number">1</span>] <span class="keyword">and</span> s[i] == s[j])</span><br><span class="line">                <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> l + <span class="number">1</span> &gt; len(ans):</span><br><span class="line">                    ans = s[i:j+<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h2 id="strip-和-split-区别">strip() 和 split() 区别</h2>
<p>从英语单词的意义上，</p>
<p><strong>strip: 脱去，去除</strong></p>
<p><strong>split: 分开，分裂</strong></p>
<p>===================================</p>
<p>str.split()) 默认去除所有空格符，包括空格，换行\n, 制表符\t
，即('\n', '\r', '\t', ‘
')，<font color="#dd00dd">并且返回切割之后的字符串list</font></p>
<p>str.strip('L')) 移除字符串<strong>头和尾</strong>
指定的字符（默认为空格或换行符），或给定去除的字符序列，比如此处移除头尾所有L（包括重复L），<font color="#dd00dd">返回的依然是字符串</font></p>
<p>另外还有str.rstrip用于删除字符串末尾的空白字符，str.lstrip删除左边的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">str = <span class="string">"LLLine1-a\tbcdef\nLine2-abc \nLine4-abcddd"</span></span><br><span class="line">print(str.split())  <span class="comment"># 默认去除所有空格符，包括空格，换行\n, 制表符\t</span></span><br><span class="line">print(str.strip(<span class="string">'L'</span>)) <span class="comment">#移除字符串头尾指定的字符（默认为空格或换行符）或字符序列，比如此处移除头尾所有L（包括重复L）</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">'LLLine1-a'</span>, <span class="string">'bcdef'</span>, <span class="string">'Line2-abc'</span>, <span class="string">'Line4-abcddd'</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">ine1-a	bcdef</span><br><span class="line">Line2-abc </span><br><span class="line">Line4-abcddd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="string">'2135432145e'</span></span><br><span class="line">print(a.split(<span class="string">'21'</span>))  <span class="comment"># 21前面为空也会形成一个元素，即空“”</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">''</span>, <span class="string">'3543'</span>, <span class="string">'45e'</span>]</span><br></pre></td></tr></table></figure>
<h2 id="正则模块re.split">正则模块re.split</h2>
<p>re.split模块比str.split()函数强大的多！</p>
<p>我们也可以轻松地指定一个字符范围，像[0-9]代表的含意与\d就是完全一致的：一位数字；同理[a-z0-9A-Z_]也完全等同于\w（如果只考虑英文的话），\w代表
匹配字母或数字或下划线或汉字, \s 匹配任意的空白符</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">s = <span class="string">'1,2,3,4,a,5,6,7\n,8,b,9,10,11,12'</span></span><br><span class="line"></span><br><span class="line">t=re.split(<span class="string">',[a-c],'</span>, s, flags=re.IGNORECASE)</span><br><span class="line">print(t) <span class="comment"># 以 ',a,'或者',b,'或者',c,'为分隔符</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">'1,2,3,4'</span>, <span class="string">'5,6,7\n,8'</span>, <span class="string">'9,10,11,12'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = [x <span class="keyword">for</span> i <span class="keyword">in</span> s.split(<span class="string">',a,'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> i.split(<span class="string">',b,'</span>)]</span><br><span class="line">print(y)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">'1,2,3,4'</span>, <span class="string">'5,6,7\n,8'</span>, <span class="string">'9,10,11,12'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># re中的split支持多个分隔符，不同分隔符之间用|分开，或者全放在[]中</span></span><br><span class="line">print(re.split(<span class="string">',[a-b],|\n,|10'</span>, s))   <span class="comment"># 单引号</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">'1,2,3,4'</span>, <span class="string">'5,6,7'</span>, <span class="string">'8'</span>, <span class="string">'9,'</span>, <span class="string">',11,12'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 两个字符以上切割需要放在 [ ] 中</span></span><br><span class="line">print(re.split(<span class="string">'[89b]'</span>, s))  <span class="comment"># 8, 9, b为分隔符</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">'1,2,3,4,a,5,6,7\n,'</span>, <span class="string">','</span>, <span class="string">','</span>, <span class="string">',10,  11, 12'</span>]</span><br></pre></td></tr></table></figure>
<p>re模块的re.sub 字符替换功能</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">s = <span class="string">'  1,2 3,4,a,5,6,7\n,8,b,9,10,  11, 12  '</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 去除字符串中所有引号，空白符，9，a, b</span></span><br><span class="line"><span class="comment">#  substitue成空字符串‘’，也就是把这些模式的字符去除</span></span><br><span class="line">print(re.sub(<span class="string">"['\"','\'','\s', 9, a, b]"</span>, <span class="string">''</span>, s))  <span class="comment"># 注意[]外用双引号！这样包住转移字符时看起来更加简明</span></span><br><span class="line"><span class="comment"># 或者其实好像也不需要有逗号把这些字符分开</span></span><br><span class="line">print(re.sub(<span class="string">"['\"''\'','\s'9ab]"</span>, <span class="string">''</span>, s))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">12345678101112</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">12345678101112</span></span><br></pre></td></tr></table></figure>
<h2 id="数字转换">数字转换</h2>
<h3 id="进制转换函数-int-hex-oct">进制转换函数 int(), hex(), oct()</h3>
<p><strong>十六进制 到 十进制</strong></p>
<p>使用 <strong>int()</strong> 函数
，<strong>第一个参数是字符串</strong> '0Xff'
,<strong>第二个参数是说明，这个字符串是几进制的数</strong>。
转化的结果是一个十进制数。</p>
<p>&gt;&gt;&gt; int('0xf',16) 15</p>
<p><strong>二进制 到 十进制</strong></p>
<p>&gt;&gt;&gt; int('10100111110',2) 1342</p>
<p><strong>八进制 到 十进制</strong></p>
<p>&gt;&gt;&gt; int('17',8) 15</p>
<p><strong>十进制 转 十六进制</strong></p>
<p>&gt;&gt;&gt; hex(1033) '0x409'</p>
<p><strong>八进制到 十六进制</strong></p>
<p>就是 八进制先转成 十进制， 再转成 十六进制。</p>
<p>&gt;&gt;&gt; hex(int('17',8)) '0xf'</p>
<p><strong>十进制转二进制</strong></p>
<p>&gt;&gt;&gt; bin(10) '0b1010'</p>
<p><strong>十六进制转 二进制</strong></p>
<p>十六进制-&gt;十进制-&gt;二进制</p>
<p>&gt;&gt;&gt; bin(int('ff',16))</p>
<h1 id="数论题">数论题</h1>
<h2 id="最大公约数和最小公倍数">最大公约数和最小公倍数</h2>
<h3 id="判断是否是质数">判断是否是质数</h3>
<p>质数又称素数，指在大于1的自然数中，除了1和该数自身外，无法被其他自然数整除的数（也可定义为只有1与该数本身两个正因数的数）</p>
<p>注意，质数的定义是大于1，所以1不在考虑范围内。2=1*2，只有1和它本身两个因子，所以2是质素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isprime</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> m == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, int(m**<span class="number">0.5</span>) + <span class="number">1</span>):  <span class="comment"># 注意，一定要加1！   </span></span><br><span class="line">        <span class="keyword">if</span> m % i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3
id="更相减损失术和辗转相除法求最大公约数">更相减损失术和辗转相除法求最大公约数</h3>
<p><strong>原理</strong></p>
<p>首先证明更相减损失术：a｜b，a｜c，则a｜(b+c) ：</p>
<p>b=a×k（k∈Z），c=a×l（l∈Z） 则，b+c=ak+al=a（k+l），其中k+l∈Z
命题得证。</p>
<p>也就是说gcd(b, c) = gcd(c+b, b) = gcd(c-b, b) = a。</p>
<p>容易看出辗转相除法是更相减损失术的快速计算方法</p>
<p><strong>代码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(m, n)</span>:</span></span><br><span class="line">  <span class="string">"""辗转相除法"""</span></span><br><span class="line">    a = max(m, n)</span><br><span class="line">    b = min(m, n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> a % b == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> gcd(b, a % b)</span><br><span class="line"></span><br><span class="line">print(gcd(<span class="number">21</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure>
<p>应用最大公约数求最小公倍数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min_multiple</span><span class="params">(m, n)</span>:</span></span><br><span class="line">    out = m * n / gcd(m, n)</span><br><span class="line">    <span class="keyword">return</span> int(out)</span><br></pre></td></tr></table></figure>
<h2 id="数字拆分有范围限制">数字拆分，有范围限制</h2>
<p>这个问题其实是动态规划章节中【盘子放苹果问题】的变体，参加动态规划章节</p>
<p>这篇博客说的很多
https://cloud.tencent.com/developer/article/1109283，但是下面一个博客更清晰</p>
<p>https://www.cnblogs.com/radiumlrb/p/5797168.html
并且给出了完整代码。</p>
<blockquote>
<p>将整数N分成K个整数的和且每个数大于等于A 小于等于B 求有多少种分法</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">breakup</span><span class="params">(number, k, a, b)</span>:</span>  </span><br><span class="line">  <span class="comment"># 把数字number分解成k个大于等于a小于等于b的整数，有可能分解数中没有a或者b</span></span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 寻找递归出口</span></span><br><span class="line">    <span class="keyword">if</span> number &lt; a:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    temp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(a, b+<span class="number">1</span>):  <span class="comment"># ### 固定上界，循环下界</span></span><br><span class="line">      	<span class="comment"># 假设分解的数字中至少有一个i: 而其他分解的数字中可能有i也可能无i</span></span><br><span class="line">        <span class="comment"># 在下一轮循环中，当前的i值迭代加1了，这样循环遍历不同的i保证子问题不会重复</span></span><br><span class="line">        <span class="comment"># 即下一个循环中划分下界变成了i+1</span></span><br><span class="line">        temp += breakup(number-i, k<span class="number">-1</span>, i, b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        m, k, a, b = map(int, input().split())</span><br><span class="line">        t = breakup(m, k, a, b)</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>C/C++实现</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Dynamics</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> k, <span class="keyword">int</span> <span class="built_in">min</span>)</span> <span class="comment">//将n分为k个整数 最小的大于等于min,最大不超过B</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(n &lt; <span class="built_in">min</span>) <span class="keyword">return</span> <span class="number">0</span>;<span class="comment">//当剩下的 比min小,则不符合要求 返回0</span></span><br><span class="line">    <span class="keyword">if</span>(k == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> t = <span class="built_in">min</span>; t &lt;= B; t++)</span><br><span class="line">    &#123;</span><br><span class="line">     sum += Dynamics(n-t, k<span class="number">-1</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>  sum;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实上述这个问题也可以用“将n划分成不大于m的划分法g(n,
m)”这个问题的解来间接地解决，g(n, b) - g(n, a)</p>
<h1 id="排列组合题">排列组合题</h1>
<h2 id="路径走法">路径走法</h2>
<blockquote>
<p>请编写一个函数（允许增加子函数），计算n x
m的棋盘格子（n为横向的格子数，m为竖向的格子数）沿着各自边缘线从左上角走到右下角，总共有多少种走法，要求不能走回头路，即：只能往右和往下走，不能往左和往上走。</p>
</blockquote>
<p>这个题有两个思路，一个用排列组合的数学角度，一个用递归的角度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span><span class="params">(n)</span>:</span></span><br><span class="line">    dot = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">        dot = dot * i </span><br><span class="line">    <span class="keyword">return</span> dot</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        [n,m] = list(map(int,input().split()))</span><br><span class="line">        <span class="comment"># 一定需要走m+n步，从中选择m步是向下走的组合数，剩下n步都是向右走的</span></span><br><span class="line">        k = factorial(m+n)/(factorial(m)*factorial(n))</span><br><span class="line">        print(int(k))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>用递归的角度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(n, m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> m == <span class="number">1</span> <span class="keyword">or</span> n == <span class="number">1</span>:  <span class="comment"># 考虑递归的出口，格子只有一列或者一行的时候走法</span></span><br><span class="line">        <span class="keyword">return</span> m + n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 每个阶数都有两个方向到达（n, m)</span></span><br><span class="line">        <span class="keyword">return</span> f(n - <span class="number">1</span>, m) + f(n, m - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        [n, m] = list(map(int, input().split()))</span><br><span class="line">        out = f(n, m)</span><br><span class="line">        print(int(out))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="跳台阶">跳台阶</h2>
<h3 id="青蛙随意跳">青蛙随意跳</h3>
<blockquote>
<p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p>
</blockquote>
<p>排列组合角度解析：</p>
<p>因为每一步都可以选择跳1～n个台阶，但是最后一个台阶必须跳上，所以一共有
<span class="math inline">\(2^{n-1}\)</span> 种跳法。</p>
<p>或者通过递推关系式子推导： <span class="math display">\[
\begin{equation}
\begin{array}{l}
f(n)=f(n-1)+f(n-2)+\ldots+f(1)+1 \\
=f(n-1)+[f(n-2)+\ldots+f(1)+1] \\
=f(n-1)+f(n-1) \\
=2 * f(n-1) \\
=2 *(n-1)
\end{array}
\end{equation}
\]</span> 递归方法/动态规划角度解析：略</p>
<h3 id="青蛙会两种跳">青蛙会两种跳</h3>
<blockquote>
<p>一只青蛙一次可以跳上 1 级台阶，也可以跳上 2 级。求该青蛙跳上一个 n
级的台阶总共有多少种跳法</p>
</blockquote>
<p><strong>一般递归方法</strong>，n较大时计算量爆炸，python无法顺利出结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myjump</span><span class="params">(n)</span>:</span>  <span class="comment"># 表示到达n个台阶时有myjump(n)种方法</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">2</span> :</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> myjump(n<span class="number">-1</span>) + myjump(n<span class="number">-2</span>)  <span class="comment"># 当前问题拆解成了两个子问题</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># lru_cache可以缓解这个问题</span></span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="meta">@functools.lru_cache(256)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myjump</span><span class="params">(n)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">2</span> :</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> myjump(n<span class="number">-1</span>) + myjump(n<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>从底向上的想法，更快，不会栈溢出</strong>，即动态规划方法升级版</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hejump</span><span class="params">(n)</span>:</span></span><br><span class="line">    f1 = <span class="number">1</span></span><br><span class="line">    f2 = <span class="number">2</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, n+<span class="number">1</span>):</span><br><span class="line">        sum = f1 + f2  <span class="comment"># </span></span><br><span class="line">        f1 = f2</span><br><span class="line">        f2 = sum</span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure>
<p>print(hejump(100)) print(myjump(100))</p>
<h1 id="排序">排序</h1>
<h2 id="sorted-函数">sorted() 函数</h2>
<h3 id="字典排序">字典排序</h3>
<p>使用 sorted()
函数对字典进行排序时，可以先使用<strong>d.items()</strong>把字典变成<strong>可迭代的对象</strong>，就变成了</p>
<p>[(key1, value1), (key2, value2)...]，</p>
<p>然后再将list中的元组按照第一个元素即key或者第二个value作为排序依据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sort_d &#x3D; sorted(d.items(), key&#x3D;lambda x: x[0])</span><br></pre></td></tr></table></figure>
<p>sorted()中的key可以接受函数作为参数，此处接受了匿名lambda函数。</p>
<p>注意，<font color="#00dd00">字典的key不单单可以是str类型，也可以是int类型</font>，但是根据key索引value时类型只能用对应的数据类型，如下面例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myd &#x3D; &#123;&#39;tom&#39;: 2, 4: 5&#125;</span><br><span class="line"></span><br><span class="line">print(myd.get(4))  # 如果是print(myd.get(‘4’))错误，因为此时的key是int类型</span><br><span class="line">print(myd.get(&#39;tom&#39;))</span><br></pre></td></tr></table></figure>
<h2 id="经典排序算法">经典排序算法</h2>
<h3 id="快排">快排</h3>
<h1 id="递归">递归</h1>
<h2 id="斐波那契的兔子">斐波那契的兔子</h2>
<blockquote>
<p>有一对兔子，从出生后第3个月起每个月都生一对兔子（雄雌），小兔子长到第三个月后每个月又生一对兔子，假如兔子都不死，问每个月的兔子总数为多少？</p>
</blockquote>
<p>一定要注意处理边界值，所有的不能用递推公式求出的情况都是初始值，比如rabit_num(1)
= abit_num(0) +
abit_num(-1)数组索引为负，这种情况就不能用递推关系式，它应该是初始条件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这个装饰器用来换成中间结果，用于重复计算时的加速</span></span><br><span class="line"><span class="meta">@functools.lru_cache(maxsize=256)  # Least-recently-used cache decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabit_num</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 一个月前兔子的数量 + 两个月前兔子（在这个月能够生兔子的那些兔子）的数量</span></span><br><span class="line">    <span class="keyword">return</span> rabit_num(n - <span class="number">1</span>) + rabit_num(n - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        m = int(input())</span><br><span class="line"></span><br><span class="line">        print(rabit_num(m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h1 id="动态规划">动态规划</h1>
<p>动态规划与分治法类似，都是把大问题拆分成小问题，<strong>通过寻找大问题与小问题的递推关系</strong>，解决一个个小问题，最终达到解决原问题的效果。但不同的是，分治法在子问题和子子问题等上被重复计算了很多次，而动态规划则具有记忆性，<strong>通过填写表</strong>把所有已经解决的子问题答案纪录下来，在新问题里需要用到的子问题可以直接提取，避免了重复计算，从而节约了时间，所以在问题满足最优性原理之后，用动态规划解决问题的核心就在于填表，表填写完毕，最优解也就找到。</p>
<h2 id="二维数组的动态规划">二维数组的动态规划</h2>
<p><strong>定义数组元素的含义；找出数组元素间的关系；找出初始值</strong></p>
<p><strong>搞清楚数组元素的含义，求解问题时一定要明确，我们的目标是把大问题分解成若干子问题来求解！</strong>领悟套路解题套路：https://zhuanlan.zhihu.com/p/91582909</p>
<h3 id="盘子放苹果">盘子放苹果</h3>
<blockquote>
<p>题目描述</p>
<p>把M个同样的苹果放在N个同样的盘子里，允许有的盘子空着不放，问共有多少种不同的分法？（用K表示）5，1，1和1，5，1
是同一种分法。</p>
<p>输入</p>
<p>每个用例包含二个整数M和N。0&lt;=m&lt;=10，1&lt;=n&lt;=10。</p>
</blockquote>
<p><strong>递归解法</strong></p>
<p>对于下面递归出口的说明</p>
<ol type="1">
<li><p>当n=1时，所有苹果都必须放在一个盘子里，所以返回１；</p></li>
<li><p>当没有苹果可放时，定义为１种放法；</p></li>
</ol>
<p>　　　　递归的两条路，第一条n会逐渐减少，终会到达出口n==1;</p>
<p>　　　　第二条m会逐渐减少，因为n&gt;m时，我们会return
count(m,m)　所以终会到达出口m==0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(m,n)</span>:</span> <span class="comment">#m为多少个苹果，n为多少个盘子</span></span><br><span class="line">    <span class="comment">#1. 盘子多，苹果少，即n&gt;m，count(m,n)=count(m,m)</span></span><br><span class="line">    <span class="comment">#2. 盘子少，苹果多，即n&lt;=m,又分两种情况：</span></span><br><span class="line">    <span class="comment">#  （1）有至少一个空盘子：count(m,n)=count(m,n-1)</span></span><br><span class="line">    <span class="comment">#  （2）没有空盘子：count(m,n)=count(m-n,n)  # 只要每个盘子先放一个就能保证</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 边界条件要考虑全面</span></span><br><span class="line">    <span class="keyword">if</span> m==<span class="number">0</span> <span class="keyword">or</span> n==<span class="number">1</span>:  <span class="comment"># m==0考虑的是count(0, n)的情况，恰好一个盘子放一个苹果</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> m&lt;n:</span><br><span class="line">        <span class="keyword">return</span> count(m,m)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> count(m,n<span class="number">-1</span>)+count(m-n,n) <span class="comment"># 没有空盘子事件的补事件是：至少一个空盘子</span></span><br><span class="line">         </span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        l=input().split()</span><br><span class="line">        m=int(l[<span class="number">0</span>])</span><br><span class="line">        n=int(l[<span class="number">1</span>])</span><br><span class="line">        print(count(m,n))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p><strong>动态规划解法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_apple</span><span class="params">(m, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    dp = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m + <span class="number">1</span>)]</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># python中的引用和拷贝：mutable values可以in-place更改内存内容，</span></span><br><span class="line">    <span class="comment"># 而immutable values不能原地更改，实际上是新生成了value然后返回赋值</span></span><br><span class="line">    <span class="comment"># str, tuple是不可更改的，list, dict等和用户定义的数据结构一般都是可改的</span></span><br><span class="line">    <span class="comment"># 下面生成list的方式其实存在内存共享的问题，直接在原始内存上发生了改变</span></span><br><span class="line">    <span class="comment"># dp = [[0] * (n + 1)] * (m + 1)  # 不能这样初始化！！</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment">#</span></span><br><span class="line">        dp[<span class="number">1</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment">#  为了迎合之后的状态转移方程，比如dp[3-3][2]=dp[0][2]</span></span><br><span class="line">        dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; j:</span><br><span class="line">                dp[i][j] = dp[i][i]</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 两个子事件：至少有一个空盘子；没有空盘子</span></span><br><span class="line">                dp[i][j] = dp[i][j - <span class="number">1</span>] + dp[i - j][j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        m, n = map(int, input().split())</span><br><span class="line">        t = put_apple(m, n)</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>如果题目变成不能有空盘子，那么我们仍然可以利用上面的代码，只是调用上面函数时变成count(m-n,
n)即可，先每个盘子放一个苹果，其他的可以随便放，允许有空盘子。</p>
<p>同时上面这个问题和下面这个问题等价：</p>
<h3 id="正整数划分-或-没有空盘子的放苹果">正整数划分 或
没有空盘子的放苹果</h3>
<blockquote>
<p>把数字M划分成K个正整数和，有多少种划分方法？</p>
</blockquote>
<p>设dp [i] [j]代表把数字 i 划分成 j
个正整数的划分方法数，则这件事可以分解为两个子问题：<font color="#dd00dd">至少</font>有一个盘子放了一个1；所有的盘子放的数全部都大于1。那么dp[i]
[j] = dp[i-1] [j-1] +dp[i-j] [j]。
<strong>第二项正确的保障是根据我们对数组元素的定义来的</strong>，因为dp
[i] [j] 表示每个盘子至少放1个苹果不能有空盘
（等价于分解数全部为正整数），所以我们先每个盘子放一个1，这样就保证剩下的苹果放入的时候dp[i-j]
[j] 每个盘子又至少放了一个1.</p>
<p>动态规划解法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">breakup</span><span class="params">(m, n)</span>:</span>  <span class="comment"># m个苹果，放入n个盘子，每个盘子不能为空</span></span><br><span class="line"></span><br><span class="line">    dp = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; j:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># 不做任何处理，初始化为0了</span></span><br><span class="line">            <span class="keyword">elif</span> j == i:  <span class="comment"># 此时只有一种划分，一个盘子里必须有一个苹果</span></span><br><span class="line">                dp[i][j] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">              <span class="comment"># 两个子问题： 至少有一个分解数是1， 所有的分解数全部大于1</span></span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + dp[i-j][j]  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        m, k = map(int, input().split())</span><br><span class="line">        t = breakup(m, k)</span><br><span class="line">        print(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3
id="正整数划分要求划分出来的每个数都不大于k">正整数划分，要求划分出来的每个数都不大于k</h3>
<blockquote>
<p>整数m划分成最大数不超过n的若干整数之和的方案数</p>
</blockquote>
<p><strong>这个问题当n==m时就变成了“整数m划分成若干正整数之和的方案数”</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_apple</span><span class="params">(m, n)</span>:</span>  <span class="comment"># 把 m 划分成若干不大于 n 的若干正整数和的方案数</span></span><br><span class="line">    dp = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment"># 为了迎合之后的状态转移方程，比如dp[3-3][3]=dp[0][3]，此时其实它本身3就是一个划分</span></span><br><span class="line">        dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):   <span class="comment"># 最大数不超过1的正整数只有一种划分，即 m个 1</span></span><br><span class="line">        dp[i][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):   <span class="comment"># 数字1的划分只有它本身一种划分</span></span><br><span class="line">        dp[<span class="number">1</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; j:</span><br><span class="line">                dp[i][j] = dp[i][i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">              <span class="comment"># 至少有一个划分数等于j ; 所有划分数都小于于j</span></span><br><span class="line">                dp[i][j] = dp[i - j][j] + dp[i][j - <span class="number">1</span>]  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        a, b = map(int, input().split())  <span class="comment"># b是允许一个盘子中放置的最大数</span></span><br><span class="line">        t = put_apple(a, b)</span><br><span class="line">        print(t)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>从代码角度来看，和“把M个同样的苹果放在N个同样的盘子里，允许有的盘子空着不放”貌似是等价的</p>
<h3
id="正整数划分要求划分出来的每个数不能相同">正整数划分，要求划分出来的每个数不能相同</h3>
<blockquote>
<p>将n划分为若干个不同的正整数 ，注意：划分数不同</p>
</blockquote>
<p><u>这一题仍然可以参考上面的代码，在上一题如果k=m，则就是划分数可以相同时的方案数目</u>，只需要k设成这个数本身即可。设dp[i]
[j]
表示把数字i划分成划分数不超过j的方案数，因此来找子问题。至少略有区别：</p>
<p>分析：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dp[n][m]&#x3D; dp[n][m-1]+ dp[n-m][m-1]  其中dp[n][m]表示整数 n 划分成不同划分数的方案数，且每个划分数不大于 m。</span><br><span class="line"></span><br><span class="line">同样划分情况分为两种情况：</span><br><span class="line">　　a.划分中每个数都小于m,相当于每个数不大于 m-1,划分数为 dp[n][m-1].</span><br><span class="line">　　b.划分中有一个数为 m.在n中减去m,剩下相当对n-m进行划分，并且每一个数不大于m-1 （因为题目要求不能有相同数，所以剩下的划分不能再有m），故划分数为 dp[n-m][m-1]</span><br></pre></td></tr></table></figure>
<h3
id="把数字n划分成若干个奇数偶数的方案数">把数字n划分成若干个奇数/偶数的方案数</h3>
<p>解法有些特殊，构造了两个二维数组，设<code>f[i][j]</code>表示将数<code>i</code>分成<code>j</code>个正奇数，<code>g[i][j]</code>表示将数i分成
j 个正偶数。这里没有限制划分成多少个，那么可能划分成1，2， 3，...,
n个奇数，那么答案就是 <code>f[i][j]</code>把j从1开始遍历到n相加.</p>
<p>https://blog.csdn.net/qq_40691051/article/details/103216117</p>
<p><code>f[i][j]</code>和<code>g[i][j]</code>之间是有关系的，如果我们先从被划分的数
i 中拿出 j 个1，然后把剩下的 i - j 分解成 j
个正奇数的方案数应该和将数i分成 j 个正偶数的方案数相同，即
<code>g[i][j] = f[i-j][j]</code></p>
<p><code>f[i][j]</code>可以分解成划分数包含1（即至少一个1）和不包含1这两个子问题，那么有
<code>f[i][j]=f[i-1][j] + g[i-j][j]</code>;
其中前者是在奇数划分时先拿出一个1保证至少划分数里有一个1，后者是偶数划分时先拿出j个1放入，这样保证之后划分数每个都会大于1（得益于<code>f[i][j]</code>数组元素的定义）</p>
<p><font color="#dddd00">最后
<code>f[n][m]</code>即为所求</font>。边界条件还没弄清，<font color="#dddd00">下面代码没有经过充分测试</font></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n, k = list(map(int, input().split()))</span><br><span class="line"></span><br><span class="line">f = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]  <span class="comment"># 若干个正奇数划分，最大划分成n个全是1的划分数</span></span><br><span class="line">g = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]  <span class="comment"># 若干个正偶数划分</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>):</span><br><span class="line">    f[i][i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">g[<span class="number">2</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, n+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i):</span><br><span class="line">        g[i][j] = f[i-j][j]</span><br><span class="line">        f[i][j] = f[i<span class="number">-1</span>][j<span class="number">-1</span>] + g[i-j][j]</span><br><span class="line"></span><br><span class="line">print(f[n][k])</span><br><span class="line">res = sum(f[n])</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<h3 id="棋盘格通过路径的数字之和">棋盘格通过路径的数字之和</h3>
<blockquote>
<p>给定一个包含非负整数的 m x n
网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小（经过的路径途中的单元格上的数字的和）。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">举例：arr存储了m*n网格中每个单元上的数字</span><br><span class="line">输入:</span><br><span class="line">arr = [</span><br><span class="line">  [<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">  [<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>],</span><br><span class="line">  [<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line">]</span><br><span class="line">输出: <span class="number">7</span></span><br><span class="line">解释: 因为路径 <span class="number">1</span>→<span class="number">3</span>→<span class="number">1</span>→<span class="number">1</span>→<span class="number">1</span> 的总和最小。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>解法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 作者：jyd</span></span><br><span class="line"><span class="comment"># 链接：https://leetcode-cn.com/problems/minimum-path-sum/solution/zui-xiao-lu-jing-he-dong-tai-gui-hua-gui-fan-liu-c/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用了直接在arr位置上覆盖的方式节省内存空间，</span></span><br><span class="line"><span class="comment"># 因为我们只需要保留到达最后一个单元格经过数字之和</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minPathSum</span><span class="params">(grid: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(grid)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(grid[<span class="number">0</span>])):</span><br><span class="line">            <span class="keyword">if</span> i == j == <span class="number">0</span>:  <span class="comment"># 就是起点，无需计算通过数字的和</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> i == <span class="number">0</span>:  <span class="comment"># 当只有上边是矩阵边界时, 只能从左面来</span></span><br><span class="line">                grid[i][j] = grid[i][j - <span class="number">1</span>] + grid[i][j]</span><br><span class="line">            <span class="keyword">elif</span> j == <span class="number">0</span>: <span class="comment"># 当只有左边是矩阵边界时： 只能从上面来</span></span><br><span class="line">                grid[i][j] = grid[i - <span class="number">1</span>][j] + grid[i][j]</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 当左边和上边都不是矩阵边界时</span></span><br><span class="line">                grid[i][j] = min(grid[i - <span class="number">1</span>][j], grid[i][j - <span class="number">1</span>]) + grid[i][j]</span><br><span class="line">    <span class="keyword">return</span> grid[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = minPathSum(arr)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<h3 id="编辑距离">编辑距离</h3>
<blockquote>
<p>问题描述： 给定两个单词 word1 和 word2，计算出将 word1 转换成 word2
所使用的最少操作数 。 你可以对一个单词进行如下三种操作： ---插入一个字符
---删除一个字符 ---替换一个字符</p>
</blockquote>
<p><strong>解答</strong></p>
<p>定义数组元素的含义，这一步看似简单其实对于问题的建模理解非常重要</p>
<p><code>dp[i][j]</code> 代表着word1的<font color="#00dd00">前 i
个字符</font>转换成word2的<font color="#00dd00">前 j
个字符</font>所需要的最少操作步数，那么就出现了二维数组问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">    n1 = len(word1)</span><br><span class="line">    n2 = len(word2)</span><br><span class="line">    <span class="comment"># 因为存在空字符串，所以存储矩阵大小应该是(n1+1) * (n2+1)</span></span><br><span class="line">    dp = [[<span class="number">0</span>] * (n2 + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n1 + <span class="number">1</span>)] </span><br><span class="line">    <span class="comment"># 第一行</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n2 + <span class="number">1</span>):</span><br><span class="line">        dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 第一列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n1 + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">0</span>] = dp[i<span class="number">-1</span>][<span class="number">0</span>] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n1 + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n2 + <span class="number">1</span>):</span><br><span class="line">        		<span class="comment"># 一共两种大情况，具体四种小情况</span></span><br><span class="line">            <span class="keyword">if</span> word1[i<span class="number">-1</span>] == word2[j<span class="number">-1</span>]:  <span class="comment"># 已经相等，不需要改动了。注意这里的索引</span></span><br><span class="line">            <span class="comment"># 第 i 个字符对应下标是 i-1</span></span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 删减字符或插入字符或更改字符</span></span><br><span class="line">                dp[i][j] = min(dp[i][j<span class="number">-1</span>], dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j<span class="number">-1</span>]) + <span class="number">1</span></span><br><span class="line">    <span class="comment">#print(dp)</span></span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = minDistance(<span class="string">'iamoksklal'</span>, <span class="string">'thmsklat'</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<h3 id="完全背包问题">完全背包问题</h3>
<p>非常好的一个讲解博客：https://www.cnblogs.com/christal-r/p/dynamic_programming.html</p>
<blockquote>
<p>有n
个物品，它们有各自的重量和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和？</p>
</blockquote>
<p>定义状态变量（使用的数组元素的意义）<strong>V(i, j)</strong>
为当前包的容量为 j （也就是已经挑选的物品总重量不超过 j ），已经决策了前
i
个物品是否放入后的最佳策略所对应的<strong>价值</strong>。如果还不明白这个含义，那换一句表述：该状态变量表示在前
i 件物品中选择若干件放在可用容量为 j
的背包中（<strong>这个背包的容量指的是我们使用的背包最大容量，不一定把包装满</strong>），可以取得的最大价值。</p>
<p><strong>求解</strong></p>
<p>设包的最大容量为C，每一件物品的重量为 <span
class="math inline">\(w_{i}\)</span> ，其价值为 <span
class="math inline">\(p_{i}\)</span> ，下面寻找状态转移方程。对于当前第
i 个物品，我们决策是否应该把它放进包里，只有两种可能：</p>
<ol type="1">
<li><p>包的容量小于该物品的重量 <span
class="math inline">\(j&lt;w_{i}\)</span> ，此物品不能放入包。则此时对前
i 个物品处理的最佳策略对应的价值和前 i-1 个物品和包容量为 j
的决策结果的最优值相同，即 <span class="math inline">\(\ V(i, j)= V(i-1,
j)\)</span></p></li>
<li><p>包的容量大于当前物品重量 <span
class="math inline">\(j&gt;=w_{i}\)</span>
，包可以放下此物品。根据我们定义的状态变量<strong>V(i, j)</strong>
的含义，<font color="#dd00dd">当包容量为j时，最优策略可能并没有放入第i个物品，即不一定非要把包全部填满才能达到价值最大。</font>那么对于第i个物品，我们有两种可能的决策，放入和不放入。<strong>也就是说
$ V(i,j)$ 这个问题显然和 $ V(i-1, j-w_{i})$ 和 $ V(i-1, j)$
这两个子问题有关</strong>。不要忘记动态规划其实就是在<font color="#00dd00">逐步填写表，表填完了，最优解也就得到了。</font></p>
<p>例如下图中，我们有3种物品，他们的重量和价格分别是1, 2, 3 kg和60, 100,
120，而包的最大容量为5。</p>
<p><img src="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200831120007.png" alt="image-20200830103029967" style="zoom:50%;" /></p>
<ul>
<li><p>不放入当前的第 i 个商品的话 <span class="math inline">\(V(i, j)=
V(i-1, j)\)</span>；</p></li>
<li><p>放入当前第 i 个商品的话 <span class="math inline">\(V(i, j)=
V(i-1, j)+p_{i}\)</span>。</p></li>
</ul>
<p>那么我们选择当前两种决策中最优的那个，即 $ V(i, j)= max(V(i-1, j),
V(i-1, j-w_{i})+p_{i})$ 。而只有当 <span class="math inline">\(V(i, j)=
V(i-1, j-w_{i})+p_{i}\)</span> 时，才有“取第 i 件物品”发生。</p></li>
</ol>
<p>现在我们知道了最大价值，但还不知道由哪些商品组成，故要根据最优解回溯解的构成。</p>
<p><strong>回溯</strong></p>
<p>根据我们填表的过程可以方向得出我们取了哪些物品，回溯的算法描述摘录自上面的博客链接。</p>
<ol type="1">
<li><p>V(i,j)=V(i-1,j)时，说明没有选择第i
个商品，则回到V(i-1,j)；</p></li>
<li><p>V(i,j)=V(i-1,j-w(i))+p(i)实时，说明装了第i个商品，该商品是最优解组成的一部分，随后我们得回到装该商品之前的最优策略对应的解，即回到V(i-1,j-w(i))；</p></li>
<li><p>一直遍历到i＝0结束为止，所有解的组成都会找到。</p></li>
</ol>
<p><strong>代码：</strong></p>
<h2 id="一维数组的动态规划">一维数组的动态规划</h2>
<h3 id="剪绳子">剪绳子</h3>
<blockquote>
<p>给你一根长度为n的绳子，请把绳子剪成m段（m、n都是整数，n&gt;1并且m&gt;1），每一段的长度记为k[0],k[1],...k[m].请问k[0]xk[1]x...xk[m]可能
的最大乘积是多少?例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18.</p>
</blockquote>
<p>解析：这个问题和前面的背包问题有些不同，背包问题分解的子问题之和以前的解以及当前物品的决策有关。而剪绳子不光与之前切割过的绳子结果有关，剩余部分的绳子切割方式会一起决定切割分段的乘积结果。如果是利用动态规划的方式去做，我们考虑对于一个长度为n的绳子，假设切分后的最大乘积为f(n)，我们先来考虑第一刀，长度为n的绳子把它切成长度为i和n-i的两段（因为由题意至少切割一次），这两段各自的绳子切分最大值为f(i)和f(n-i)，如此一来就找了子问题分解，即
f(n) = max{f(i), f(n-i)}, 其中 1 &lt;= i &lt;=n-1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cutRope</span><span class="params">(number)</span>:</span></span><br><span class="line">    <span class="comment"># write code here</span></span><br><span class="line">    <span class="comment"># if number==1:</span></span><br><span class="line">    <span class="comment">#     return 0</span></span><br><span class="line">    <span class="comment"># elif number==2:</span></span><br><span class="line">    <span class="comment">#     return 1</span></span><br><span class="line">    <span class="comment"># elif number==3:</span></span><br><span class="line">    <span class="comment">#     return 2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> number &lt;= <span class="number">3</span>:  <span class="comment"># 边缘情况，不能使用下面的状态转移方程</span></span><br><span class="line">        <span class="keyword">return</span> number - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 初始化数组， 对应绳子长度为1, 2, 3</span></span><br><span class="line">    <span class="comment"># 绳子不断切割，当切割到长度为1,2,3时，不能继续切割，直接返回1,2.3</span></span><br><span class="line">    prod = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]  <span class="comment"># 【其实对于长度为1， 2，3的子绳子，不分是最大的】！！！。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>, number + <span class="number">1</span>):</span><br><span class="line">        max = <span class="number">0</span>  <span class="comment"># 每一种长度的绳子的最优值</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i // <span class="number">2</span> + <span class="number">1</span>):  <span class="comment"># 剪法遍历到中间点就行了，以后后面一半到方法与前面一半到剪法效果一样</span></span><br><span class="line">            pro = prod[j] * prod[i - j]</span><br><span class="line">            <span class="keyword">if</span> pro &gt; max:</span><br><span class="line">                max = pro</span><br><span class="line">        prod.append(max)  <span class="comment"># 记录长度的绳子的最优值</span></span><br><span class="line">    <span class="keyword">return</span> prod[number]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cutRope(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<p>当然了，这道题也可以通过贪心的策略来解答
https://blog.csdn.net/lc199408/article/details/80929108</p>
<h3 id="最长上升子序列-lis">最长上升子序列 (LIS)</h3>
<p>举个例子：求A={2 7 1 5 6 4 3 8 9}的最长上升子序列。我们定义d(i)
(i∈[1,n])来表示前i个数以A[i]结尾的最长上升子序列长度。</p>
<p>https://blog.csdn.net/lxt_Lucia/article/details/81206439</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alist = [<span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(alist))]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(alist)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i):</span><br><span class="line">        <span class="keyword">if</span> alist[i] &gt; alist[j]: <span class="comment"># 从前到后遍历并覆盖到达A[i]的最长上升子序列</span></span><br><span class="line">            dp[i] = max(dp[j] + <span class="number">1</span>, dp[i])  </span><br><span class="line"></span><br><span class="line">LIS_len = max(dp)</span><br><span class="line"></span><br><span class="line">print(LIS_len)</span><br></pre></td></tr></table></figure>
<h1 id="dfs-和-bfs">DFS 和 BFS</h1>
<h2 id="图的遍历基础实现">图的遍历基础实现</h2>
<p>忘记在哪里看到的模板了.... 想起了再添加链接</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque  <span class="comment"># deque是双向队列</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        self.order = []  <span class="comment"># visited order 顶点已经访问过了，放入order</span></span><br><span class="line">        self.neighbor = &#123;&#125;  <span class="comment"># 存储图的数据结构</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="string">"""适用于给出一个顶点和与之相邻的所有顶点是list数据结构"""</span></span><br><span class="line">        key, val = node</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(val, list):</span><br><span class="line">            print(<span class="string">'node value should be a list'</span>)</span><br><span class="line">            <span class="comment"># sys.exit('failed for wrong input')</span></span><br><span class="line"></span><br><span class="line">        self.neighbor[key] = val  <span class="comment"># 每个顶点都有可能与多个顶点相连</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edges</span><span class="params">(self, n, edges: List[List[int]])</span>:</span></span><br><span class="line">        <span class="string">"""适用于给出图的顶点数目以及图中存在的所有边的数据结构"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(edges[<span class="number">0</span>], list):</span><br><span class="line">            print(<span class="string">'edges value should be a list'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            self.neighbor[i] = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _edge <span class="keyword">in</span> edges:  <span class="comment"># todo</span></span><br><span class="line">            self.neighbor[_edge[<span class="number">0</span>]].append(_edge[<span class="number">1</span>])</span><br><span class="line">            self.neighbor[_edge[<span class="number">1</span>]].append(_edge[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># #########################################</span></span><br><span class="line">    <span class="comment"># #################  BFS  #################</span></span><br><span class="line">    <span class="comment"># #########################################</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">breadth_first</span><span class="params">(self, root)</span>:</span>  <span class="comment"># root 为访问的起始顶点</span></span><br><span class="line">        <span class="keyword">if</span> root != <span class="literal">None</span>:</span><br><span class="line">            search_queue = deque()  <span class="comment"># 双向队列</span></span><br><span class="line">            search_queue.append(root)</span><br><span class="line"></span><br><span class="line">            visited = []</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'root is None'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> search_queue:</span><br><span class="line">            person = search_queue.popleft()  <span class="comment"># 下一层顶点放在了队列的右侧，而当前层的顶点会从左侧弹出</span></span><br><span class="line">            <span class="keyword">if</span> person <span class="keyword">not</span> <span class="keyword">in</span> self.order:</span><br><span class="line">                self.order.append(person)  <span class="comment"># 当前顶点已经访问过了，放入order</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 只有该顶点有子顶点才继续往下搜索，把它下一层的顶点放入队列</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">not</span> person <span class="keyword">in</span> visited) <span class="keyword">and</span> (person <span class="keyword">in</span> self.neighbor.keys()):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 向队列右侧添加该顶点的邻接顶点，而这些顶点处于下一层，应该在本层遍历后再考虑</span></span><br><span class="line">                search_queue += self.neighbor[person]</span><br><span class="line">                <span class="comment"># visited 保存了有孩子的所有父亲顶点</span></span><br><span class="line">                visited.append(person)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># #########################################</span></span><br><span class="line">    <span class="comment"># #################  DFS  #################</span></span><br><span class="line">    <span class="comment"># #########################################</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">depth_first</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root != <span class="literal">None</span>:</span><br><span class="line">            search_queue = deque()</span><br><span class="line">            search_queue.append(root)</span><br><span class="line"></span><br><span class="line">            visited = []</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'root is None'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> search_queue:</span><br><span class="line">            person = search_queue.popleft()  <span class="comment"># 每次循环优先将节点弹出</span></span><br><span class="line">            <span class="keyword">if</span> person <span class="keyword">not</span> <span class="keyword">in</span> self.order:</span><br><span class="line">                self.order.append(person)</span><br><span class="line">            <span class="comment"># 只有该顶点有子顶点并且该顶点之前没有被访问过，才继续往下搜索，把它下一层的顶点放入队列</span></span><br><span class="line">            <span class="comment"># 这是因为对于一般的图，可能后面层的某个顶点又与该顶点有连接（有环），那么该顶点在那一层的循环中</span></span><br><span class="line">            <span class="comment"># 又再一次被加入 search_queue。不过此处的实现依然会把这个顶点弹出两次</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">not</span> person <span class="keyword">in</span> visited) <span class="keyword">and</span> (person <span class="keyword">in</span> self.neighbor.keys()):</span><br><span class="line">                tmp = self.neighbor[person]</span><br><span class="line">                tmp.reverse()  <span class="comment"># 为了配合后面appendleft, 从队列左侧依次插入</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> index <span class="keyword">in</span> tmp:</span><br><span class="line">                    search_queue.appendleft(index)</span><br><span class="line"></span><br><span class="line">                visited.append(person)</span><br><span class="line">                <span class="comment"># self.order.append(person)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.order = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">node_print</span><span class="params">(self)</span>:</span>  <span class="comment"># 依次打印访问的顶点内容</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> self.order:</span><br><span class="line">            print(index, end=<span class="string">'  '</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    g = Graph()</span><br><span class="line">    g.add_node((<span class="string">'1_key'</span>, [<span class="string">'one_key'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]))</span><br><span class="line">    g.add_node((<span class="string">'one_key'</span>, [<span class="string">'first_key'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br><span class="line">    g.add_node((<span class="string">'first_key'</span>, [<span class="string">'1'</span>, <span class="string">'second'</span>, <span class="string">'third'</span>, <span class="string">'1_key'</span>]))</span><br><span class="line"></span><br><span class="line">    g.breadth_first(<span class="string">'1_key'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'breadth search first:'</span>)</span><br><span class="line">    print(<span class="string">'  '</span>, end=<span class="string">'  '</span>)</span><br><span class="line">    g.node_print()</span><br><span class="line">    g.clear()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n\ndepth search first:'</span>)</span><br><span class="line">    print(<span class="string">'  '</span>, end=<span class="string">'  '</span>)</span><br><span class="line">    g.depth_first(<span class="string">'1_key'</span>)</span><br><span class="line">    g.node_print()</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>
<h2 id="二叉树的遍历">二叉树的遍历</h2>
<p>https://leetcode-cn.com/problems/binary-tree-inorder-traversal/solution/die-dai-he-di-gui-by-powcai</p>
<h3 id="中序遍历">中序遍历</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root: TreeNode)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(node)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">                <span class="keyword">return</span> []</span><br><span class="line">            helper(node.left)</span><br><span class="line">            ann.append(node.val)</span><br><span class="line">            helper(node.right)</span><br><span class="line">            </span><br><span class="line">        ann = []</span><br><span class="line">        helper(root)</span><br><span class="line">        <span class="keyword">return</span> ann</span><br></pre></td></tr></table></figure>
<h2 id="验证二叉树搜索">验证二叉树搜索</h2>
<p>https://leetcode-cn.com/problems/validate-binary-search-tree/</p>
<h2 id="读取列表为二叉树">读取列表为二叉树</h2>
<h2 id="若干典型例题">若干典型例题</h2>
<h3 id="无环无向图连通分量的数目">无环无向图连通分量的数目</h3>
<p>如果使用上述基础实现，可以怎么做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gg = Graph()</span><br><span class="line">   n = <span class="number">5</span></span><br><span class="line">   edges = [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">   <span class="comment"># edges = [[0, 1], [1, 2], [3, 4]]</span></span><br><span class="line">   gg.add_edges(n, edges)</span><br><span class="line"></span><br><span class="line">   res = <span class="number">0</span></span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">       <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> gg.order:</span><br><span class="line">           res += <span class="number">1</span></span><br><span class="line">           gg.depth_first(i)</span><br><span class="line">   print(res)</span><br></pre></td></tr></table></figure>
<h3 id="岛屿的数量">岛屿的数量</h3>
<blockquote>
<p>给你一个由 '1'（陆地）和
'0'（水）组成的的二维网格，请你计算网格中岛屿的数量。岛屿总是被水包围，并且每座岛屿只能由水平方向或竖直方向上相邻的陆地连接形成。每座岛屿只能由水平和/或竖直方向上相邻的陆地连接而成。</p>
<p>输入: [ ['1','1','1','1','0'], ['1','1','0','1','0'],
['1','1','0','0','0'], ['0','0','0','0','0']] 输出: 1</p>
<p>输入: [ ['1','1','0','0','0'], ['1','1','0','0','0'],
['0','0','1','0','0'], ['0','0','0','1','1']] 输出: 3</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span>  <span class="comment"># 四连通区域个数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numIslands</span><span class="params">(self, grid: List[List[str]])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> grid: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        row = len(grid)</span><br><span class="line">        col = len(grid[<span class="number">0</span>])</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(i, j)</span>:</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            如何避免在方格中的重复遍历呢？答案是标记已经遍历过的格子。以岛屿问题为例，</span></span><br><span class="line"><span class="string">            我们需要在所有值为 1 的陆地格子上做 DFS 遍历。每走过一个陆地格子，</span></span><br><span class="line"><span class="string">            就把格子的值改为 2，这样当我们遇到 2 的时候，就知道这是遍历过的格子了。</span></span><br><span class="line"><span class="string">            也就是说，每个格子可能取三个值：</span></span><br><span class="line"><span class="string">            0 —— 海洋格子</span></span><br><span class="line"><span class="string">            1 —— 陆地格子（未遍历过）</span></span><br><span class="line"><span class="string">            2 —— 陆地格子（已遍历过）</span></span><br><span class="line"><span class="string">            链接：https://leetcode-cn.com/problems/number-of-islands/solution/dao-yu-lei-wen-ti-de-tong-yong-jie-fa-dfs-bian-li-/</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            grid[i][j] = <span class="string">"2"</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> [[<span class="number">-1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">-1</span>], [<span class="number">0</span>, <span class="number">1</span>]]:  <span class="comment"># 左右上下遍历</span></span><br><span class="line">                tmp_i = i + x</span><br><span class="line">                tmp_j = j + y</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= tmp_i &lt; row <span class="keyword">and</span> <span class="number">0</span> &lt;= tmp_j &lt; col <span class="keyword">and</span> grid[tmp_i][tmp_j] == <span class="string">"1"</span>:</span><br><span class="line">                    dfs(tmp_i, tmp_j)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="string">"1"</span>:  <span class="comment"># 从是岛屿的点出发，开始遍历四邻接区域</span></span><br><span class="line">                    dfs(i, j)</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure>
<h3 id="所有岛屿中面积最大的那个">所有岛屿中面积最大的那个</h3>
<p>题解：https://leetcode-cn.com/problems/max-area-of-island/solution/</p>
<p>是上一个问题的变体，记录下每个连通区域的面积即可，但是注意#
python的形式参数传入过程有坑，数字对象不允许引用，只有可变数据结构才允许引用直接修改原始值，详情可见：</p>
<p>http://www.ityouknow.com/python/2020/01/07/python-function_parameter-112.html</p>
<h3 id="所有岛屿中周长最大的那个">所有岛屿中，周长最大的那个</h3>
<blockquote>
<p>输入: [[0,1,0,0], [1,1,1,0], [0,1,0,0], [1,1,0,0]]</p>
<p>输出: 16</p>
</blockquote>
<p>解释: 它的周长是下面图片中的 16 个黄色的边：</p>
<p><img src="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200917115650.png" alt="image-20200917115639749" style="zoom:33%;" /></p>
<p><strong>分析</strong>：如果岛屿cell和水（0）相邻或者和边界相邻，那么就会多出一条边</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.current_area = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">islandPerimeter</span><span class="params">(self, grid: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> grid: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        row = len(grid)</span><br><span class="line">        col = len(grid[<span class="number">0</span>])</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        max_perimenter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(i, j)</span>:</span></span><br><span class="line"></span><br><span class="line">            grid[i][j] = <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> [[<span class="number">-1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">-1</span>], [<span class="number">0</span>, <span class="number">1</span>]]:  <span class="comment"># 左右上下遍历</span></span><br><span class="line">                tmp_i = i + x</span><br><span class="line">                tmp_j = j + y</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= tmp_i &lt; row <span class="keyword">and</span> <span class="number">0</span> &lt;= tmp_j &lt; col <span class="keyword">and</span> grid[tmp_i][tmp_j] == <span class="number">1</span>:</span><br><span class="line">                    dfs(tmp_i, tmp_j)</span><br><span class="line">                <span class="keyword">if</span> tmp_i &gt;= row <span class="keyword">or</span> tmp_i &lt; <span class="number">0</span>:</span><br><span class="line">                    self.current_area += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> tmp_j &lt; <span class="number">0</span> <span class="keyword">or</span> tmp_j &gt;= col:</span><br><span class="line">                    self.current_area += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= tmp_i &lt; row <span class="keyword">and</span> <span class="number">0</span> &lt;= tmp_j &lt; col <span class="keyword">and</span> grid[tmp_i][tmp_j] == <span class="number">0</span>:</span><br><span class="line">                    self.current_area += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="number">1</span>:  <span class="comment"># 从是岛屿的点出发，开始遍历四邻接区域</span></span><br><span class="line">                    self.current_area = <span class="number">0</span>  </span><br><span class="line">                    dfs(i, j)</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">                    max_perimenter = max(max_perimenter, self.current_area)</span><br><span class="line">        <span class="keyword">return</span> max_perimenter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s = Solution()</span><br><span class="line">arr=[[<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">res=s.islandPerimeter(arr)</span><br><span class="line">print(res)  <span class="comment"># &gt;&gt;&gt; 4</span></span><br></pre></td></tr></table></figure>
<h3 id="朋友圈">朋友圈</h3>
<p>链接：https://leetcode-cn.com/problems/friend-circles</p>
<blockquote>
<p>班上有 N
名学生。其中有些人是朋友，有些则不是。他们的友谊具有是传递性。如果已知 A
是 B 的朋友，B 是 C 的朋友，那么我们可以认为 A 也是 C
的朋友。所谓的朋友圈，是指所有朋友的集合。</p>
<p>给定一个 N * N 的矩阵 M，表示班级中学生之间的朋友关系。如果M[i][j] =
1，表示已知第 i 个和 j
个学生互为朋友关系，否则为不知道。你必须输出所有学生中的已知的朋友圈总数。</p>
<p>例如输入： [[1,1,0], [1,1,0], [0,0,1]] 输出：2 解释：已知学生 0
和学生 1 互为朋友，他们在一个朋友圈。
第2个学生自己在一个朋友圈。所以返回 2 。</p>
</blockquote>
<p><strong>分析</strong>：把寻找岛屿个数代码稍稍修改一下即可，只是此时遍历应该是逐行遍历，而不是上下左右四个位置了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCircleNum</span><span class="params">(self, M: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        grid = M</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> grid:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        row = len(grid)</span><br><span class="line">        col = len(grid[<span class="number">0</span>])</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(i, j)</span>:</span></span><br><span class="line">            grid[i][j] = <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(col):</span><br><span class="line">                tmp_i = i</span><br><span class="line">                tmp_j = y</span><br><span class="line">                <span class="keyword">if</span> grid[tmp_i][tmp_j] == <span class="number">1</span>:</span><br><span class="line">                    grid[tmp_i][tmp_j] = <span class="number">2</span></span><br><span class="line">                    dfs(tmp_j, tmp_i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="number">1</span>:</span><br><span class="line">                    dfs(i, j)</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cnt</span><br><span class="line"></span><br><span class="line">arr = [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">s = Solution()</span><br><span class="line">res = s.findCircleNum(arr)</span><br><span class="line">print(arr)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<h1 id="小知识">小知识</h1>
<h3 id="lambda函数">lambda函数</h3>
<p>lambda函数也叫匿名函数，即没有具体名称的函数，它允许快速定义单行函数，类似于C语言的宏，可以用在任何需要函数的地方。这区别于def定义的函数。
lambda函数有一些应用，但常常可以被普通函数取代，例如下面的可以点定义def
mutiply_2(x): return x * 2然后用mutiply_2 取代lambda x: x*2</p>
<p><img
src="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200831120017.png" /></p>
<h3 id="缓存中间结果用来加速计算">缓存中间结果用来加速计算</h3>
<p>使用python中的lru_cache</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import functools</span><br><span class="line"></span><br><span class="line"># 下面这个装饰器用来换成中间结果，用于重复计算时的加速</span><br><span class="line">@functools.lru_cache(maxsize&#x3D;256)  # Least-recently-used cache decorator</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>找工作</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch实用指南</title>
    <url>/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/84784982">Source</a></p>
<h1 id="网络模型构建">网络模型构建</h1>
<h2 id="nn.sequential和nn.modulelist的区别">1.
nn.Sequential和nn.ModuleList的区别</h2>
<p>简而言之就是，nn.Sequential类似于Keras中的贯序模型，它是Module的子类，在构建数个网络层之后会自动调用forward()方法，从而有网络模型生成。而nn.ModuleList仅仅类似于pytho中的list类型，只是将一系列层装入列表，并没有实现forward()方法，因此也不会有网络模型产生的副作用。两者使用的一个很好的例子如链接：<a
href="https://www.cnblogs.com/hellcat/p/8477195.html"
class="uri">https://www.cnblogs.com/hellcat/p/8477195.html</a></p>
<p>另外需要注意的是<strong>，网络中需要训练的参数一定要被正确地注册，比如如果使用了普通list,
dict等，之后一定要用nn.Sequential或者nn.ModuleList包装一下；甚至在定义网络时，网络的一个attribute是一个list,
list里面是一个或者多个子网络Module类别，也依然需要用nn.ModuleList替换掉这个普通的list，这样才能将模型参数和子网络模型参数顺利被优化器识别</strong>。否则，运行时不会报错，但是没有被注册的参数将不会被训练！并且，只有被正确注册之后，我们用model.cuda()，这些参数才会被自动迁移到GPU上，否则只会停留在CPU上。</p>
<h2 id="nn.modulelist可以由多维下标索引但用嵌套的list初始化时需注意">2.
nn.ModuleList可以由多维下标索引，但用嵌套的list初始化时需注意</h2>
<p>注意：
比如下面self.outs定义了具有二维索引的modulelist，需要注意的是，内层list也要加nn.ModuleList包装，这样内层list内部就是可迭代的Module
subclass对象，
<strong>否则内层就是普通的list，不满足输入参数的类型要求</strong>，pytorch不能正确识别它们是可训练的模型参数，会报错。</p>
<pre><code>class PoseNet(nn.Module):
    def __init__(self, nstack, inp_dim, oup_dim, bn=False, increase=128, **kwargs):
        &quot;&quot;&quot; Pack or initialize the trainable parameters of the network&quot;&quot;&quot;
        super(PoseNet, self).__init__()
        self.pre = nn.Sequential(
            Conv(3, 64, 7, 2, bn=bn),
            Conv(64, 128, bn=bn),
            nn.MaxPool2d(2, 2))

        self.outs = nn.ModuleList(
            [nn.ModuleList([Conv(inp_dim, oup_dim, 1, relu=False, bn=False) for j in range(4)]) for i in range(nstack)])</code></pre>
<h1 id="网络结构可视化">网络结构可视化</h1>
<h2 id="网络结构可视化-1">1. 网络结构可视化</h2>
<pre><code>def make_dot(var, params=None):
    &quot;&quot;&quot; Produces Graphviz representation of PyTorch autograd graph
    Blue nodes are the Variables that require grad, orange are Tensors
    saved for backward in torch.autograd.Function
    Args:
        var: output Variable
        params: dict of (name, Variable) to add names to node that
            require grad (TODO: make optional)
    &quot;&quot;&quot;
    if params is not None:
        assert isinstance(params.values()[0], Variable)
        param_map = {id(v): k for k, v in params.items()}

    node_attr = dict(style=&#39;filled&#39;,
                     shape=&#39;box&#39;,
                     align=&#39;left&#39;,
                     fontsize=&#39;12&#39;,
                     ranksep=&#39;0.1&#39;,
                     height=&#39;0.2&#39;)
    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=&quot;12,12&quot;))
    seen = set()

    def size_to_str(size):
        return &#39;(&#39; + (&#39;, &#39;).join([&#39;%d&#39; % v for v in size]) + &#39;)&#39;

    def add_nodes(var):
        if var not in seen:
            if torch.is_tensor(var):
                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=&#39;orange&#39;)
            elif hasattr(var, &#39;variable&#39;):
                u = var.variable
                name = param_map[id(u)] if params is not None else &#39;&#39;
                node_name = &#39;%s\n %s&#39; % (name, size_to_str(u.size()))
                dot.node(str(id(var)), node_name, fillcolor=&#39;lightblue&#39;)
            else:
                dot.node(str(id(var)), str(type(var).__name__))
            seen.add(var)
            if hasattr(var, &#39;next_functions&#39;):
                for u in var.next_functions:
                    if u[0] is not None:
                        dot.edge(str(id(u[0])), str(id(var)))
                        add_nodes(u[0])
            if hasattr(var, &#39;saved_tensors&#39;):
                for t in var.saved_tensors:
                    dot.edge(str(id(t)), str(id(var)))
                    add_nodes(t)

    add_nodes(var.grad_fn)
    return dot</code></pre>
<p>使用以上代码的例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># plot the model</span><br><span class="line"># net &#x3D; PoseNet(nstack&#x3D;4, inp_dim&#x3D;256, oup_dim&#x3D;68)</span><br><span class="line"># x &#x3D; Variable(torch.randn(1, 3, 512, 512))  # x的shape为(batch，channels，height，width)</span><br><span class="line"># y &#x3D; net(x)</span><br><span class="line"># g &#x3D; make_dot(y)</span><br><span class="line"># g.view()</span><br></pre></td></tr></table></figure>
<h2 id="类似于keras-打印网络每层输出的形状shape">2. 类似于keras,
打印网络每层输出的形状shape</h2>
<p>更新：推荐使用增强版工具 <a
href="https://github.com/nmhkahn/torchsummaryX"><strong>torchsummaryX</strong></a>，它可以同时给出输出shape，参数数目，以及乘加运算数目等</p>
<p>Improved visualization tool of <a
href="https://github.com/sksq96/pytorch-summary">torchsummary</a>. Here,
it visualizes kernel size, output shape, # params, and Mult-Adds. Also
the torchsummaryX can handle RNN, Recursive NN, or model with multiple
inputs.</p>
<hr />
<p>使用模仿keras中的summary()函数，<strong>torchsummary</strong> <a
href="https://www.jianshu.com/p/97c626d33924">转载自</a></p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">device &#x3D; torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # PyTorch v0.4.0</span><br><span class="line">model &#x3D; Net().to(device)</span><br><span class="line"></span><br><span class="line">summary(model, (1, 28, 28))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">            Conv2d-1           [-1, 10, 24, 24]             260</span><br><span class="line">            Conv2d-2             [-1, 20, 8, 8]           5,020</span><br><span class="line">         Dropout2d-3             [-1, 20, 8, 8]               0</span><br><span class="line">            Linear-4                   [-1, 50]          16,050</span><br><span class="line">            Linear-5                   [-1, 10]             510</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 21,840</span><br><span class="line">Trainable params: 21,840</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.00</span><br><span class="line">Forward&#x2F;backward pass size (MB): 0.06</span><br><span class="line">Params size (MB): 0.08</span><br><span class="line">Estimated Total Size (MB): 0.15</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="pytorch中layer的输出shape的尺寸取整">3.
pytorch中layer的输出shape的尺寸取整</h2>
<p>默认使用的是向下取整(floor)，如：</p>
<pre><code>self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)  # (batch_size, 512, 38, 38)

# (H + 2*p - d(ks - 1) - 1) / 2 + 1
# (38 + 12 - 6*(3 - 1) -1 ) / 2 + 1 = 19.5 向下取整 19
self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # (batch_size, 1024, 19, 19)</code></pre>
<p>Maxpooling层也是默认使用向下取整。如果想使用向上取整(ceil)，需要设置取整模式
ceil_mode=True， 默认是False</p>
<pre><code>nn.MaxPool2d(kernel_size=2, stride=2),  # (batch_size, 256, 37, 37), 想变成38*38可以使用　ceil_mode=True</code></pre>
<h2 id="超级给力的网络结构可视化工具netron-和-hiddenlayer">4.
超级给力的网络结构可视化工具：Netron 和 hiddenlayer</h2>
<p>前者是一款在浏览器中使用的可视化工具，可以使用pip安装，然后在命令行中输入netron或者netron
-b [model file]。需要把模型转换onnx模型。</p>
<pre><code>import torch.onnx

net = Hourglass2(2, 32, 1, Residual)
dummy_input = Variable(torch.randn(1, 32, 128, 128))
torch.onnx.export(net, dummy_input, &quot;model.onnx&quot;)</code></pre>
<p>后者是在jupyter notebook内使用的，例子如下：</p>
<p>Netron: <a href="https://github.com/lutzroeder/netron"
class="uri">https://github.com/lutzroeder/netron</a></p>
<p>hiddenlayer: <a
href="https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb">https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb</a></p>
<h2 id="计算网络模型的参数量和浮点运算数">5.
计算网络模型的参数量和浮点运算数</h2>
<p>使用第三方库thop</p>
<pre><code>from thop import profile
from thop import clever_format

dummy_input = torch.randn(1, 256, 128, 128)
flops, params = profile(MyNetwork, inputs=(dummy_input,))
flops, params = clever_format([flops, params], &quot;%.3f&quot;)
print(flops, params)</code></pre>
<h1 id="tensor的操作">Tensor的操作</h1>
<h2 id="tensor.view和tensor.permute-permute变换">１.
Tensor.view和Tensor.permute (permute:变换)</h2>
<p>torch中的view类似与numpy中的reshape，但不同的是前者会与变换后的tensor共享内存，而后者不共享不会影响原始数组。PyTorch在0.4版本以后提供了<strong><code>reshape</code></strong>方法，实现了类似于
<code>tensor.contigous().view(*args)</code>的功能，如果不关心底层数据是否使用了新的内存，则使用<strong><code>reshape</code></strong>方法更方便。</p>
<p>torch中的permute类似与numpy中的transpose.
<strong>注意：</strong>view只能用在contiguous的variable上。如果在view之前用了transpose,
permute等，需要用contiguous()来返回一个contiguous copy。</p>
<p>一个在SSD中的例子：</p>
<pre><code> y_loc = self.loc_layers[i](x)
            batch_size = y_loc.size(0)  # int
            # 此处y_loc的shape是(batch_size, anchor*4, Hi, Wi), pytorch的数据结构为(N, C, H, W)
            y_loc = y_loc.permute(0, 2, 3, 1).contiguous()
            # 此处y_loc的shape是(batch_size, Hi, Wi, anchor*4)
            # 要先把4放到最后，然后再改变shape 变成 ##### (batch_size, anchor_all_number, 4) ######,  anchor_all_number代表anchor的总数
            # permute可以对任意高维矩阵进行转置. 但没有 torch.permute() 这个调用方式， 只能 Tensor.permute()。
            # view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。
            y_loc = y_loc.view(batch_size, -1, 4)</code></pre>
<h2 id="若前面有一个tensor输入需要梯度则后面的输出也需要梯度">２.
若前面有一个tensor输入需要梯度，则后面的输出也需要梯度</h2>
<pre><code>x = torch.zeros((1), requires_grad=True)
# 若前面有一个输入需要梯度，则后面的输出也需要梯度。有的版本这里是默认值false
# 注：　Tensor变量的requires_grad的属性默认为False,若一个节点requires_grad被设置为True，那么所有依赖它的节点的requires_grad都为True。</code></pre>
<h2
id="tensor之间要是同一个数据类型dtype才能运算因此有时需要进行类型转换">３.
Tensor之间要是同一个数据类型<strong>dtype</strong>才能运算，因此有时需要进行类型转换</h2>
<p>比如即便都是int类型，但是一个是int16，一个是int32也需要先转换然后才能进行运算。使用Tensor.<code>to(torch.float32)进行转换。</code></p>
<pre><code># 因为loc_loss是float32，而num_matched_box是int64，没办法直接除所以转换一下
# 这里是不会损失数据的，因为假如batch_size=32,每个图片8732个，就只有8732*32=279424
# num_matched_boxes最大的值不会超过float32的表示范围的
num_matched_boxes = num_matched_boxes.to(torch.float32)  # Tensor dtype and/or device 转换
loc_loss /= num_matched_boxes   # 除以的是正样本的数目</code></pre>
<h2 id="tensor的clone和copy_的区别">４.
Tensor的clone和copy_的区别：</h2>
<p>copy_()不会追踪梯度，而clone会追踪并进行梯度的反向传播</p>
<p>Unlike copy_(), clone is recorded in the computation graph. Gradients
propagating to the cloned tensor will propagate to the original
tensor.</p>
<h2 id="tensor初始化">５. Tensor初始化</h2>
<h3 id="a.-torch.tensor和torch.from_numpy效果不同">a.
torch.tensor和torch.from_numpy()效果不同</h3>
<p>torch.tensor会重新拷贝原始数据，返回新的数据。如果不想拷贝，即内存相关联，对numpy
array来说可以使用torch.from_numpy()。</p>
<p>可以直接用list数据进行初始化，并且对list中某一个元素是tuple还是list都无所谓，如：</p>
<p>x= [(1,2,3,4), [5,6,7,8]] # x[0]是tuple而x[1]是list torch.tensor(x)
Out[20]: tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8]])</p>
<h2 id="data和detach的区别">６. data和detach()的区别</h2>
<p>推荐使用detach()，这样万一需要在反向传播时需要记录变量，可以报错指出，避免Tensor.data没有报错，但是计算错误的情况。</p>
<p><a href="https://zhuanlan.zhihu.com/p/38475183"
class="uri">https://zhuanlan.zhihu.com/p/38475183</a></p>
<blockquote>
<p><em>"However, .data can be unsafe in some cases. Any changes on
x.data wouldn’t be tracked by autograd, and the computed gradients would
be incorrect if x is needed in a backward pass. A safer alternative is
to use x.detach(), which also returns a Tensor that shares data with
requires_grad=False, but will have its in-place changes reported by
autograd if x is needed in backward."</em></p>
</blockquote>
<p><strong>Any in-place change on x.detach() will cause errors when x is
needed in backward, so .detach() is a safer way for the exclusion of
subgraphs from gradient computation. <a
href="https://github.com/pytorch/pytorch/issues/6990"
class="uri">https://github.com/pytorch/pytorch/issues/6990</a></strong></p>
<h2 id="pytorch中损失函数对tensor操作的reducesize_average参数说明">7.
pytorch中损失函数对tensor操作的reduce,size_average参数说明</h2>
<p>参考：<a
href="https://blog.csdn.net/u013548568/article/details/81532605"
class="uri">https://blog.csdn.net/u013548568/article/details/81532605</a></p>
<p>以及 <a href="https://zhuanlan.zhihu.com/p/91485607"
class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>size_average是说是不是对一个batch里面的所有的数据求均值</p>
<hr />
<p><strong>Reduce </strong> <strong>size_average </strong> * 意义* True
True 对batch里面的数据取均值loss.mean() True False
对batch里面的数据求和loss.sum() False – returns a loss per batch element
instead, 这个时候忽略size_average参数</p>
<hr />
<p>reduction : 可选的参数有：‘none’ | ‘elementwise_mean’ | ‘sum’,
正如参数的字面意思</p>
<hr />
<p>假设输入和target的大小分别是NxCxWxH，那么一旦reduce设置为False，loss的大小为NxCxWxH，返回每一个元素的loss</p>
<p><strong>reduction代表了上面的reduce和size_average双重含义，这也是文档里为什么说reduce和size_average要被Deprecated
的原因</strong></p>
<p>例子：</p>
<pre><code>import torch
import torch.nn as nn

# ----------------------------------- MSE loss

# 生成网络输出 以及 目标输出
output = torch.ones(2, 2, requires_grad=True) * 0.5
target = torch.ones(2, 2)

# 设置三种不同参数的L1Loss
reduce_False = nn.MSELoss(size_average=True, reduce=False) # 等效于reduction=&#39;none&#39;
size_average_True = nn.MSELoss(size_average=True, reduce=True) # 等效于reduction=&#39;mean&#39;
size_average_False = nn.MSELoss(size_average=False, reduce=True) # 等效于reduction=&#39;sum&#39;

o_0 = reduce_False(output, target)
o_1 = size_average_True(output, target)
o_2 = size_average_False(output, target)

print(&#39;\nreduce=False, 输出同维度的loss:\n{}\n&#39;.format(o_0))
print(&#39;size_average=True，\t求平均:\t{}&#39;.format(o_1))
print(&#39;size_average=False，\t求和:\t{}&#39;.format(o_2))</code></pre>
<p>输出：</p>
<pre><code>reduce=False, 输出同维度的loss:
tensor([[0.2500, 0.2500],
        [0.2500, 0.2500]], grad_fn=&lt;MseLossBackward&gt;)

size_average=True，  求平均:    0.25

size_average=False， 求和: 1.0</code></pre>
<h2 id="将tensor以及model迁移至cuda上">8.
将tensor以及model迁移至cuda上</h2>
<p><strong>将数据迁移到cuda上必须reassign，tensor.cuda()不是in-place操作，而是返回一个新的在cuda上的tensor。而网络模型不需要reassign.</strong></p>
<h3 id="a.-迁移tensor">a. 迁移tensor</h3>
<p><strong>问题：</strong>Hi, this works,
<code>a = torch.LongTensor(1).random_(0, 10).to("cuda")</code>. but this
won’t work:</p>
<p><strong>回答：</strong></p>
<p>If you are pushing tensors to a device or host, <strong>you have to
reassign them:</strong></p>
<pre><code>a = a.to(device=&#39;cuda&#39;)</code></pre>
<h3 id="b.-迁移模型">b. 迁移模型</h3>
<p><code>nn.Module</code>s push all parameters, buffers and submodules
recursively and don’t need the assignment.</p>
<blockquote>
<p>model.cuda()</p>
</blockquote>
<h2 id="对feature-map-即也是tensor做尺寸上的缩放">9. 对feature map
(即也是tensor)做尺寸上的缩放</h2>
<blockquote>
<p><code>torch.nn.functional.``interpolate</code>(<em>input</em>,
<em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>,
<em>align_corners=None</em>)</p>
</blockquote>
<p>默认的<em>align_corners=None就是和Opencv中的缩放规则保持一致，默认使用几何中心对齐，以此消除量化误差（或者说</em>计算出的灰度值也相对于源图像偏左偏上）<em>。</em></p>
<p>若做缩放，需要在缩放后图像 的位置上找到对应的 原始图像位置上
的像素值，有以下</p>
<p>SrcX=(dstX+0.5)* (srcWidth/dstWidth) -0.5 SrcY=(dstY+0.5) *
(srcHeight/dstHeight)-0.5</p>
<p>具体参考我的另一篇博客：</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/100150726"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/100150726</a></p>
<h2
id="注册参数--模型的普通类成员变量和pytorch中自动注册的parameter或者buffer区别">10.
注册参数--模型的普通类成员变量和Pytorch中自动注册的Parameter或者buffer区别</h2>
<p><a href="https://zhuanlan.zhihu.com/p/89442276"
class="uri">https://zhuanlan.zhihu.com/p/89442276</a></p>
<p>同时参考 第一节#网络模型构建中nn.ModuleList</p>
<p>模型中需要保存下来的参数包括两种:</p>
<p>一种是反向传播需要被optimizer更新的，称之为 parameter
一种是反向传播不需要被optimizer更新，称之为
buffer，它只能在forward中被更新。</p>
<p>第一种参数我们可以通过 model.parameters()
返回；第二种参数我们可以通过 model.buffers()
返回。因为我们的模型保存的是 state_dict 返回的
OrderDict，所以这两种参数不仅要满足是否需要被更新的要求，还会被保存到OrderDict。而<strong>普通的类成员变量属性是无法自动保存到模型的
OrderDict中去的。</strong></p>
<p>模型进行设备移动时，模型中注册的参数(Parameter和buffer)会同时进行移动，比如使用model.cuda()之后注册的参数parameter和buffer会自动迁移到cuda上去，<strong>而普通成员变量不会自动设备移动</strong>。</p>
<h2 id="tensor的缩放">11. Tensor的缩放</h2>
<p><a
href="https://discuss.pytorch.org/t/how-do-i-interpolate-directly-on-tensor/23081/3">一个讨论</a></p>
<p>使用functional.interpolate函数对Tensor进行缩放，注意，bicubic插值算法只能对4-D
Tensor正常操作，如果是3-D操作，需要先扩展纬度之后再进行。下面例子中，hmps是一个shape=(N,
C, H,
W)的张量，bicubic默认会对<font color="#dddd00">最后两个维度进行缩放插值</font>，而batch
size and channels (dim0,
dim1)不变。即把张量的空间分辨率（长和宽）放大。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sizeHW &#x3D; (args.square_length, args.square_length)  # 设square_length是hmps的4倍</span><br><span class="line">hmps1 &#x3D; torch.nn.functional.interpolate(hmps, size&#x3D;sizeHW, mode&#x3D;&quot;bicubic&quot;)</span><br><span class="line">hmps2 &#x3D; torch.nn.functional.interpolate(hmps, scale_factor&#x3D;4, mode&#x3D;&quot;bicubic&quot;)</span><br><span class="line">t &#x3D; (hmps1&#x3D;&#x3D;hmps2).all() &gt;&gt;&gt; 将输出一个为True的Tensor</span><br></pre></td></tr></table></figure>
<h2 id="tensor的contiguousstoragestride">12.
Tensor的contiguous、storage、stride</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/64551412">PyTorch中的contiguous</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/101434655" target="_blank" rel="noopener">Pytorch中的Size,
storage offset, stride概念</a></p>
<p><strong><code>contiguous</code></strong>直观的解释是<strong>Tensor底层一维数组元素的存储顺序与Tensor按<font color="#dddd00">行优先</font>一维展开的元素顺序是否一致</strong>。pytorch中的storage指的是连续的内存块，而tensor则是映射到storage的视图，他把单条的内存区域映射成了n维的空间视图。size是tensor的维度，storage
offset是数据在storage中的开头索引，stride是storage中对应于tensor的相邻维度间第一个索引的跨度。示例详情见链接。</p>
<h2
id="如何判断索引或者切片是对原始tensor的view共享内存还是copy">如何判断索引或者切片是对原始Tensor的view（共享内存）还是copy?</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/81352299">umpy
array 以及pytorch tensor 的索引（切片索引，整型索引）</a></p>
<h3 id="简而言之">简而言之</h3>
<p>基本的切片索引（slice）是对原始数组的一个view，会影响原始数组的。而后者情况比较复杂，不过是basic
indexing，则依然是原始数组的一个view，如果是advanced
indexig，会用原始数组创建一个新的数组，不会影响原始数据。</p>
<p>x[1, 3:8], x[2:5,
6:9]是基本slice索引，是view共享内存的；而x[1,2]是基本integer索引,
x[[1,2], [1,4]]是高级integer索引。</p>
<h3
id="针对pytorch-tensor可以通过data_ptr查看第一个元素地址是否相同来判断">针对pytorch
tensor可以通过data_ptr()查看第一个元素地址是否相同来判断</h3>
<p>full list of view ops in PyTorch</p>
<blockquote>
<ul>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.as_strided"><code>as_strided()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach"><code>detach()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.diagonal"><code>diagonal()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand"><code>expand()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as"><code>expand_as()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.movedim"><code>movedim()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.narrow"><code>narrow()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"><code>permute()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.select"><code>select()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze"><code>squeeze()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.transpose"><code>transpose()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t"><code>t()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T"><code>T</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.real"><code>real</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.imag"><code>imag</code></a></li>
<li><code>view_as_real()</code></li>
<li><code>view_as_imag()</code></li>
<li><a
href="https://pytorch.org/docs/stable/named_tensor.html#torch.Tensor.unflatten"><code>unflatten()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unfold"><code>unfold()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze"><code>unsqueeze()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"><code>view()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as"><code>view_as()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unbind"><code>unbind()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.split"><code>split()</code></a></li>
<li><code>split_with_sizes()</code></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.chunk"><code>chunk()</code></a></li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.indices"><code>indices()</code></a>
(sparse tensor only)</li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.values"><code>values()</code></a>
(sparse tensor only)</li>
</ul>
<p>It’s also worth mentioning a few ops with special behaviors:</p>
<ul>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape"><code>reshape()</code></a>,
<a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape_as"><code>reshape_as()</code></a>
and <a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.flatten"><code>flatten()</code></a>
can return either a view or new tensor, user code shouldn’t rely on
whether it’s view or not.</li>
<li><a
href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous"><code>contiguous()</code></a>
returns <strong>itself</strong> if input tensor is already contiguous,
otherwise it returns a new contiguous tensor by copying data.</li>
</ul>
</blockquote>
<h3 id="numpy中的判断准则">Numpy中的判断准则</h3>
<p>Pytorch的行为是模仿Numpy的，numpy提供了详细的说明，什么时候是view，什么时候是copy:</p>
<p>https://numpy.org/doc/stable/reference/arrays.indexing.html</p>
<blockquote>
<p>Advanced indexing is triggered when the selection object,
<em>obj</em>, is a non-tuple sequence object, an <a
href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray"><code>ndarray</code></a>
(of data type integer or bool), or a tuple with at least one sequence
object or ndarray (of data type integer or bool). There are two types of
advanced indexing: integer and Boolean.</p>
<p>Advanced indexing always returns a <em>copy</em> of the data
(contrast with basic slicing that returns a <a
href="https://numpy.org/doc/stable/glossary.html#term-view">view</a>).</p>
<p>具体说明参加上面链接</p>
</blockquote>
<h1 id="pytorch训练数据准备">pytorch训练数据准备</h1>
<h2 id="dataloader-类">1. DataLoader 类</h2>
<h3 id="参数说明-摘录自">参数说明 <a
href="https://blog.csdn.net/weixin_42236288/article/details/80893882%C2%A0">摘录自</a></h3>
<p>1. dataset：加载的数据集(Dataset对象) 2. batch_size：batch size 3.
shuffle:：是否将数据打乱 4. sampler： 样本抽样，后续会详细介绍 5.
num_workers：使用多进程加载的进程数，0代表不使用多进程 6. collate_fn：
<strong>如何将多个样本数据拼接成一个batch</strong>，一般使用默认的拼接方式即可，即默认调用default_collate，但是如果数据异常往往无法自动处理而报错
7. pin_memory：是否将数据保存在pin memory区，pin
memory中的数据转到GPU会快一些 8.
drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</p>
<h3 id="对于-pin_memory-的解释摘录自">对于 pin_memory 的解释：<a
href="https://oldpan.me/archives/pytorch-to-use-multiple-gpus">摘录自</a></h3>
<p><strong>pin_memory就是锁页内存</strong></p>
<blockquote>
<p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。
主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。显卡中的显存全部是锁页内存,当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>
</blockquote>
<h3
id="collate_fn的作用和默认的default_collate">collate_fn的作用，和默认的default_collate</h3>
<p>这个函数的决定<strong>如何将多个样本数据拼接成一个batch</strong>，一般使用默认的拼接方式即可，即默认调用default_collate，它会自动地把__getitem__生成的单个张量，数字，字符串，列表，字典等进行串联拼接成batch的数据。但是如果数据异常往往无法自动处理而报错。比如如果我们读取图片失败，default_collate自动处理时就会报错：</p>
<blockquote>
<p>TypeError: batch must contain tensors, numbers, dicts or lists; found
&lt;class 'NoneType'&gt;</p>
</blockquote>
<p>这个时候需要靠我们自定义collate_fn，返回的batch数据会自定清理掉不合法的数据，并且我们还可以通过自己的collate_fn自由地对dataloader生产的batch数据做各种选择处理。</p>
<h2 id="多进程读取hdf5文件支持的不好以及解决办法">2.
多进程读取HDF5文件支持的不好以及解决办法</h2>
<p>DataLoader中多进程高效处理hdf5文件：</p>
<p><a
href="https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643">摘录自</a></p>
<p><strong>My recommendations:</strong></p>
<blockquote>
<ul>
<li>Use HDF5 in version 1.10 (better multiprocessing handling),</li>
<li>Because an opened HDF5 file isn’t pickleable and to send Dataset to
workers’ processes it needs to be serialised with pickle, you can’t open
the HDF5 file in <code>__init__</code>. Open it in
<code>__getitem__</code>and <strong>store as the singleton!</strong>. Do
not open it each time as it introduces huge overhead.</li>
<li>Use <code>DataLoader</code> with <code>num_workers</code> &gt; 0
(reading from hdf5 (i.e. hard drive) is slow) and
<code>batch_sampler</code> (random access to hdf5 (i.e. hard drive) is
slow).</li>
</ul>
</blockquote>
<p><strong>Sample code:</strong></p>
<pre><code>class H5Dataset(torch.utils.data.Dataset):
    def __init__(self, path):
        self.file_path = path
        self.dataset = None
        with h5py.File(self.file_path, &#39;r&#39;) as file:
            self.dataset_len = len(file[&quot;dataset&quot;])

    def __getitem__(self, index):
        if self.dataset is None:
            self.dataset = h5py.File(self.file_path, &#39;r&#39;)[&quot;dataset&quot;]
        return self.dataset[index]

    def __len__(self):
        return self.dataset_len</code></pre>
<p><strong>如何安装HDF5 1.10以及对应的python hdf5的包呢？ 查看<a
href="https://blog.csdn.net/xiaojiajia007/article/details/87873443">我的另一个博客</a></strong></p>
<p><strong>使用命令行环境变量HDF5_DIR=/usr/local/hdf5 pip install
h5py。具体如下：</strong></p>
<p>Then you should be fine. Install HDF5 1.10 from source into somewhere
you want to. The .tar is here:
https://www.hdfgroup.org/HDF5/release/obtainsrc5110.html Follow the
install readme but basically you just need to give it a directory with:
&gt; ./configure --prefix=/usr/local/h5py before you make.</p>
<p>Now install with you anaconda version of python. You may want to make
a separate environment using conda but that's your call.</p>
<p>Remove the h5py you have with anaconda using &gt; conda uninstall
h5py or &gt; pip uninstall h5py</p>
<p>Then use pip to reinstall h5py but pointing to the HDF5 library you
made from source. From here: http://docs.h5py.org/en/latest/build.html
<strong>&gt; HDF5_DIR=/usr/local/hdf5 pip install h5py</strong></p>
<p>Then you should be good. Open up a python terminal and test if you
can use SWMR mode: &gt; import h5py &gt; f = h5py.File("./swmr.h5", 'a',
libver='latest', swmr=True)</p>
<h2 id="多进程准备数据随机种子seed的问题">3.
多进程准备数据<strong>随机种子seed</strong>的问题</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/87881231">参见我另一个博客</a></p>
<h2 id="如何加速训练数据准备并载入gpu训练">4.
如何加速训练数据准备并载入GPU训练</h2>
<p>参考一个知乎博客，data_prefetcher： <a
href="https://zhuanlan.zhihu.com/p/80695364"
class="uri">https://zhuanlan.zhihu.com/p/80695364</a></p>
<p>以及Pytorch论坛上的一个讨论： <a
href="https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee">https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee</a></p>
<h1 id="pytorch训练阶段">Pytorch训练阶段</h1>
<h2 id="stochastic-weight-averaging-in-pytorch">1. Stochastic Weight
Averaging in PyTorch</h2>
<p>这是一种model weight
average策略，类似于模型集成，常常用来刷指标，提高模型的泛化精度。详细说明请见我的单独博客：</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/90748115"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90748115</a></p>
<h2 id="通过梯度积累变相增大batch-size">2. 通过梯度积累变相增大batch
size</h2>
<p><a
href="https://www.zhihu.com/question/303070254/answer/573037166">详情请见
PyTorch中在反向传播前为什么要手动将梯度清零？ - Pascal的回答 - 知乎</a>
但是需要注意的是，因为BN层的参数是在
forward()阶段更新的，这样积累梯度并没有增大BN layers的实际batch
size。可以通过减少BN层的 momentum
值，让BN层动态更新统计参数时能够记住更长。</p>
<h1 id="pytorch-测试阶段">Pytorch 测试阶段</h1>
<h2 id="正确的测试预测时间计时代码">1.
正确的测试（预测）时间计时代码</h2>
<pre><code>torch.cuda.synchronize() # 等待当前设备上所有流中的所有核心完成
start = time.time() 
result = model(input) 
torch.cuda.synchronize() 
end = time.time()</code></pre>
<p>在pytorch里面，程序的执行都是异步的。如果没有torch.cuda.synchronize()
，测试的时间会很短，因为执行完end=time.time()程序就退出了，后台的cu也因为python的退出退出了，如果采用torch.cuda.synchronize()
，代码会同步cu的操作，等待gpu上的操作都完成了再继续成形end =
time.time()</p>
<p>如果将代码改为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start &#x3D; time.time()</span><br><span class="line">result &#x3D; model(input)</span><br><span class="line">print(result)</span><br><span class="line">end &#x3D; time.time()</span><br><span class="line">1234</span><br></pre></td></tr></table></figure>
<p>这时候会发祥第三段代码和第二段代码的时间是类似的，因为第三段代码会等待gpu上的结果执行完传给print函数，所以真个的时间就和第二段同步的操作的时间基本上是一致的了，将print(result)换成result.cpu()结果是一致的惹。</p>
<p>原文：https://blog.csdn.net/u013548568/article/details/81368019</p>
<h2 id="训练测试两个阶段需要注意设置不同状态-参考">2.
训练，测试两个阶段需要注意设置不同状态 <a
href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/10">参考</a></h2>
<h3 id="a.-model.train和model.val">a. model.train()和model.val()</h3>
<p>比如BN和Dropout</p>
<p>During eval <code>Dropout</code> is deactivated and just passes its
input. During the training the probability <code>p</code> is used to
drop activations. Also, the activations are scaled with
<code>1./p</code> as otherwise the expected values would differ between
training and eval.</p>
<pre><code>drop = nn.Dropout()
x = torch.ones(1, 10)

# Train mode (default after construction)
drop.train()
print(drop(x))

# Eval mode
drop.eval()
print(drop(x))</code></pre>
<h3
id="b.-测试val时不光要设置model.eval-为了防止内存爆炸应该追加torch.no_grad">b.
测试（val)时不光要设置<code>model.eval()</code>
，为了防止内存爆炸，应该追加<code>torch.no_grad()</code></h3>
<ul>
<li><code>model.eval()</code> will notify all your layers that you are
in eval mode, that way, batchnorm or dropout layers will work in eval
model instead of training mode.</li>
<li><code>torch.no_grad():</code> impacts the autograd engine and
deactivate it. It will reduce memory usage and speed up computations but
you won’t be able to backprop (which you don’t want in an eval script).
注意，<code>torch.no_grad()是</code>context manager。</li>
</ul>
<h2 id="dropout里需要设置训练标志位否则会踩坑">3.
Dropout里需要设置训练标志位，否则会踩坑</h2>
<h3
id="使用f.dropout-nn.functional.dropout-的时候需要设置它的可选参数training-state">使用F.dropout
( nn.functional.dropout )的时候需要设置它的可选参数training state</h3>
<p>这个状态参数与模型整体的一致，否则就是out=out，没有效果，具体说明见链接
<a
href="https://www.zhihu.com/question/67209417/answer/302434279">查看</a></p>
<pre><code>Class DropoutFC(nn.Module):
   def __init__(self):
       super(DropoutFC, self).__init__()
       self.fc = nn.Linear(100,20)

   def forward(self, input):
       out = self.fc(input)
       out = F.dropout(out, p=0.5, training=self.training) # set dropout&#39;s training sate
       return out

Net = DropoutFC()
Net.train()

# train the Net
#作者：雷杰
#链接：https://www.zhihu.com/question/67209417/answer/302434279</code></pre>
<h3
id="或者直接使用nn.dropout即利用包装后的layer">或者直接使用nn.Dropout()，即利用包装后的layer</h3>
<p>nn.Dropout()实际上是对F.dropout的一个包装,
也将self.training传入了)</p>
<pre><code>Class DropoutFC(nn.Module):
  def __init__(self):
      super(DropoutFC, self).__init__()
      self.fc = nn.Linear(100,20)
      self.dropout = nn.Dropout(p=0.5)

  def forward(self, input):
      out = self.fc(input)
      out = self.dropout(out)
      return out
Net = DropoutFC()
Net.train()

# train the Net</code></pre>
<h2 id="多gpu模型权重的保存与加载">4. 多GPU模型权重的保存与加载</h2>
<p>Instead of deleting the “module.” string from all the state_dict
keys, you can save your model with:
<code>torch.save(model.module.state_dict(), path_to_file)</code> instead
of <code>torch.save(model.state_dict(), path_to_file)</code>
<strong><em>that way you don’t get the “module.” string to begin
with…</em></strong></p>
<pre><code># original saved file with DataParallel
state_dict = torch.load(&#39;myfile.pth.tar&#39;)
# 把所有的张量加载到CPU中
# torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage)

# create new OrderedDict that does not contain `module.`
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
model.load_state_dict(new_state_dict)

############## 还有一个可用的封装更好的函数
# 加载模型，解决命名和维度不匹配问题,解决多个gpu并行
def load_state_keywise(model, model_path):
    model_dict = model.state_dict()
    pretrained_dict = torch.load(model_path, map_location=&#39;cpu&#39;)
    key = list(pretrained_dict.keys())[0]
    # 1. filter out unnecessary keys
    # 1.1 multi-GPU -&gt;CPU
    if (str(key).startswith(&#39;module.&#39;)):
        pretrained_dict = {k[7:]: v for k, v in pretrained_dict.items() if
                           k[7:] in model_dict and v.size() == model_dict[k[7:]].size()}
    else:
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           k in model_dict and v.size() == model_dict[k].size()}
    # 2. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)
    # 3. load the new state dict
    model.load_state_dict(model_dict)

 ################## 更简单直接的方式 ##################
# Instead of deleting the “module.” string from all the state_dict keys, you can save your model with:

torch.save(model.module.state_dict(), path_to_file)
# instead of

torch.save(model.state_dict(), path_to_file)

# that way you don’t get the “module.” string to begin with…</code></pre>
<h2 id="恢复保存的优化器状态optimizer-checkpoint-resume继续优化">5.
恢复保存的优化器状态(optimizer checkpoint resume)，继续优化</h2>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/88417329"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/88417329</a></p>
<h2 id="载入模型权重gpu内存被额外占用的bug解决">6.
载入模型权重GPU内存被额外占用的bug解决</h2>
<h3
id="分布式多进程中的这种情况的现象是对于同一个python进程pid号相同会两次占用固定大小的gpu内存">分布式/多进程中的这种情况的现象是，对于同一个python进程（pid号相同）会两次占用固定大小的gpu内存</h3>
<p>观察到的现象是python进程多于预期应有的进程数。比如我们单机多卡分布式训练，已经完成了网络模型的in-place参数设备转换，即network.cuda()，现在我们有4块GPU，我们在程序中的每一个进程分配一块GPU时本来应该只有4个进程，每个进程占用一定的GPU显存，但实际情况如所示：</p>
<pre><code>Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 1291 G /usr/lib/xorg/Xorg 153MiB |
| 0 2549 G fcitx-qimpanel 14MiB |
| 0 21740 G compiz 138MiB |
| 0 22840 C /home/jia/.virtualenvs/phoenix/bin/python 6097MiB | 
| 0 22841 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22842 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22843 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 23207 G /opt/teamviewer/tv_bin/TeamViewer 24MiB |
| 0 23985 G .../Software/pycharm-2019.2.4/jbr/bin/java 12MiB |
| 1 22841 C /home/jia/.virtualenvs/phoenix/bin/python 6129MiB |
| 2 22842 C /home/jia/.virtualenvs/phoenix/bin/python 6227MiB |
| 3 22843 C /home/jia/.virtualenvs/phoenix/bin/python 6229MiB</code></pre>
<p>原因：在同一个cuda上之后不使用的内存将会被自动销毁并回收，但是对于不同GPU之间目前没有自动的内存管理机制??，如果某一个进程在cuda0上实例化的tensor
x，在另一个使用cuda2的进程中使用了，但cuda2上的进程并没有对tensor
x进行内存销毁回收，造成GPU内存的占用。</p>
<p>解决办法：在当前进程中销毁不在同一个cuda上的内存垃圾，或者载入权重时使用torch.load(model_path,
<strong>map_location='cpu'</strong>)</p>
<h3
id="gpu预训练保存的权值可以直接载入到cpu下的网络模型network中并且载入之后network的参数会移到预训练权值所在的device上">GPU预训练保存的权值可以直接载入到CPU下的网络模型network中，并且载入之后network的参数会移到预训练权值所在的device上</h3>
<p>如下图所示：</p>
<p><img
src="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20201215222559.png" /></p>
<p>这个行为挺诡异，按照正常的设计逻辑，本来CPU的模型直接载入GPU预训练权值应该会因为device不同而报错（cpu,
cuda0)但结果并没有，可以成功载入，并且载入之后CPU下的模型network的device也变成cuda0了。甚至我们可以仅仅载入某一layer的权值，那么这一layer的weight.data将变到cuda0上，而其没有载入更改的layer的weight.data仍然在cpu上！</p>
<p>解决办法同上一种情况，把GPU预训练权值map到cpu上之后再network.load_state_dict()。</p>
<h1 id="pytorch的内存优化和加速">Pytorch的内存优化和加速</h1>
<p><strong>有一个 pytorch提速指南： <a
href="https://zhuanlan.zhihu.com/p/39752167"
class="uri">https://zhuanlan.zhihu.com/p/39752167</a></strong></p>
<p><strong>可以参考 <a
href="https://blog.csdn.net/jacke121/article/details/81329679%C2%A0">原文</a></strong></p>
<h2 id="使用inplace减少内存开辟从而压缩内存需求">1.
使用inplace减少内存开辟，从而压缩内存需求</h2>
<p>对于in-place operation的解读，见：<a
href="https://blog.csdn.net/u012436149/article/details/80819523"
class="uri">https://blog.csdn.net/u012436149/article/details/80819523</a></p>
<p>以及：<a
href="https://blog.csdn.net/york1996/article/details/81835873"
class="uri">https://blog.csdn.net/york1996/article/details/81835873</a></p>
<p>如，ReLu(inplace=True)</p>
<p>在官方问文档中由这一段话：</p>
<blockquote>
<p>如果你使用了in-place
operation而没有报错的话，那么你可以确定你的梯度计算是正确的。<strong>因为Pytorch在内存占用和执行速度上做了很多算法优化，哪些需要保留梯度不能使用in-place覆盖就显得不那么显而易见了，不能单纯地用原始梯度反向传播过程来决定。</strong></p>
</blockquote>
<p>inplace只是可以节省存储tensor的内存，但是PYTORCH中的自动微分机制仍然能够追踪，对于内存来说inplace可能是同一个对象，但是对于autograd来说，依然是两个不同的对象。
一个例子：<a
href="https://discuss.pytorch.org/t/why-relu-inplace-true-does-not-give-error-in-official-resnet-py-but-it-gives-error-in-my-code/21004/3">resnet</a></p>
<blockquote>
<p><strong><code>inplace</code> means that it will not allocate new
memory and change tensors inplace</strong>. <strong>But from the
autograd point of view, you have two different tensors (even though they
actually share the same memory)</strong>. One is the output of conv (or
batchnorm for resnet) and one is the output of the relu.</p>
</blockquote>
<h2 id="torch.backends.cudnn.benchmark-true">2.
torch.backends.cudnn.benchmark = True</h2>
<p>在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</p>
<h2 id="torch.cuda.empty_cache">3. torch.cuda.empty_cache()</h2>
<p>因为每次迭代都会引入点临时变量，会导致训练速度越来越慢，基本呈线性增长。开发人员还不清楚原因，但如果周期性的使用torch.cuda.empty_cache()的话就可以解决这个问题。</p>
<h2 id="使用checkpoint分阶段计算这样可以在显卡上放下更大的网络">4.
使用checkpoint分阶段计算，这样可以在显卡上放下更大的网络</h2>
<p>知乎回答的一个例子：https://www.zhihu.com/question/274635237/answer/574193034</p>
<h2 id="尝试nvidia-apex-16位浮点数扩展">5. 尝试Nvidia Apex
16位浮点数扩展</h2>
<p>温馨提示：我的另一篇博客<a
href="https://blog.csdn.net/xiaojiajia007/article/details/84784982">pip
install, python setup.py, egg-info的说明--以Nvidia Apex安装为例</a></p>
<h3 id="clean-the-old-install-before-rebuilding">Clean the old install
before rebuilding:</h3>
<blockquote>
<p>pip uninstall apex cd apex_repo_dir rm -rf build (if present) rm -rf
apex.egg-info (if present)</p>
</blockquote>
<h3 id="install-package">Install package：</h3>
<blockquote>
<p>pip install -v --no-cache-dir --global-option="--cpp_ext"
--global-option="--cuda_ext" ./</p>
<p># --no-cache-dir 清除安装缓存文件</p>
</blockquote>
<p>或者</p>
<blockquote>
<p>python setup.py install --cuda_ext --cpp_ext</p>
</blockquote>
<h3
id="ps-如果遇到cuda版本不兼容的问题解决办法见若pytorch升级到1.3cuda10.1则没有这个error了">ps:
如果遇到Cuda版本不兼容的问题，解决办法见：（若pytorch升级到1.3，cuda10.1则没有这个error了）</h3>
<p><a
href="https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952">https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952</a></p>
<p>如果没有出现其他error，可以直接使用上面链接的建议，删除版本检查抛出的报错。</p>
<p>讨论：</p>
<p><a href="https://github.com/NVIDIA/apex/issues/350"
class="uri">https://github.com/NVIDIA/apex/issues/350</a></p>
<p><a href="https://github.com/NVIDIA/apex/pull/323"
class="uri">https://github.com/NVIDIA/apex/pull/323</a></p>
<h2 id="pytorch内存泄露僵尸进程解决办法-原文链接">6.
Pytorch内存泄露（僵尸进程）解决办法 <a
href="https://blog.csdn.net/liuyifang0810680/article/details/79628394%C2%A0">原文链接</a></h2>
<p>nvidia-smi 发现内存泄露问题，即没有进程时，内存被占用</p>
<blockquote>
<p>fuser -v /dev/nvidia* 发现僵尸进程</p>
<p>ps x |grep python|awk '{print $1}'|xargs kill 杀死所有僵尸进程</p>
</blockquote>
<p>命令解读：</p>
<p>ps x: show all process of current user</p>
<p>grep python: to get process that has python in command line</p>
<p>awk '{print $1}': to get the related process pidxargs kill`: to kill
the process</p>
<p>note: make sure you don’t kill other processes! do ps x |grep python
first.</p>
<h2 id="相关的进程和内存管理bash-cmd-命令行命令">7.
相关的进程和内存管理bash cmd (命令行命令）</h2>
<p>nvidia-smi -l xxx
监控GPU，动态刷新信息（默认5s刷新一次），按Ctrl+C停止，可指定刷新频率，以秒为单位；</p>
<p>watch -n 1 nvidia-smi <strong>实时监控GPU</strong>； watch -n 1
lscpu实时监控CPU，watch是周期性的执行下个程序 ps -elf进程查看，
<strong>ps -elf | grep python
查看Python子进程</strong>，这个也是命令比较实用，能够用在监视其他基于python解释器运行的进程，
kill -9 [PID]杀死进程PID。</p>
<blockquote>
<blockquote>
<p><strong>watch -n 5 -t -d=cumulative 'command'</strong></p>
</blockquote>
<p>watch是周期性的执行下个程序，并全屏显示执行结果</p>
<p>-n 每隔5秒周期执行一次</p>
<p>-t 开头的间隔时间和信息等不显示</p>
<p><strong>-d=cumulative 发生变动的地方高亮</strong></p>
</blockquote>
<h2 id="如何才能使用-tensor-core">8. 如何才能使用 Tensor Core</h2>
<p><strong>Convolutions</strong>: For cudnn versions 7.2 and ealier,
<span class="citation" data-cites="vaibhav0195">@vaibhav0195</span> is
correct: input channels, output channels, and batch size should be
multiples of 8 to use tensor cores. However, this requirement is lifted
for cudnn versions 7.3 and later. <strong>For cudnn 7.3 and later, you
don't need to worry about making your channels/batch size multiples of 8
to enable Tensor Core use</strong>.</p>
<p><strong>GEMMs (fully connected layers)</strong>: For matrix A x
matrix B, where A has size [I, J] and B has size [J, K], I, J, and K
must be multiples of 8 to use Tensor Cores. This requirement exists for
all cublas and cudnn versions. This means that for <strong>bare fully
connected layers, the batch size, input features, and output features
must be multiples of 8</strong>, and** for RNNs, you usually (but not
always, it can be architecture-dependent depending on what you use for
encoder/decoder) need to have batch size, hidden size, embedding size,
and dictionary size as multiples of 8.**</p>
<h2
id="apex的fused-adam的特点是模型参数更新迭代得比pytorch中原生的adam快"><strong>9.
Apex的Fused
Adam的特点是模型参数更新迭代得比Pytorch中原生的Adam快</strong></h2>
<p>What is the difference between FusedAdam optimizer in Nvidia AMP
package with the Adam optimizer in Pytorch?</p>
<p><a
href="https://discuss.pytorch.org/t/fusedadam-optimizer-in-nvidia-amp-package/47544">摘录自</a></p>
<blockquote>
<p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries
out optimizer.step() by looping over parameters, and launching a series
of kernels for each parameter. This can require hundreds of small
launches that are mostly bound by CPU-side Python looping and kernel
launch overhead, resulting in poor device utilization. Currently, the
FusedAdam implementation in Apex flattens the parameters for the
optimization step, then carries out the optimization step itself via a
fused kernel that combines all the Adam operations. In this way, the
loop over parameters as well as the internal series of Adam operations
for each parameter are fused such that optimizer.step() requires only a
few kernel launches.</p>
<p>The current implementation (in Apex master) is brittle and only works
with Amp opt_level O2. I’ve got a WIP branch to make it work for any
opt_level (<a href="https://github.com/NVIDIA/apex/pull/351"
class="uri">https://github.com/NVIDIA/apex/pull/351</a>). I recommend
waiting until this is merged then trying it.</p>
</blockquote>
<h1 id="pytorch-使用陷阱易错点"><strong>Pytorch
使用陷阱，易错点</strong></h1>
<h2
id="tensor.expand-expand_as是共享内存的只是原始数据的一个视图-view并没有在扩展的axis上有新的数据复制牵一发动全身"><strong>1.
Tensor.expand, expand_as是共享内存的，只是原始数据的一个视图
view，并没有在扩展的axis上有新的数据复制，牵一发动全身！</strong></h2>
<p><strong>为了避免对 expand()
后对某个channel操作会影响原始tensor的全部元素，需要使用clone()</strong></p>
<p>如果没有clone()，对mask_miss的某个通道赋值后，所有通道上的tensor都会变成1！</p>
<blockquote>
<p># Notice! expand does not allocate more memory but just make the
tensor look as if you expanded it. # You should call .clone() on the
resulting tensor if you plan on modifying it #
https://discuss.pytorch.org/t/very-strange-behavior-change-one-element-of-a-tensor-will-influence-all-elements/41190</p>
</blockquote>
<pre><code>mask = mask_miss.expand_as(sxing).clone()            # type: torch.Tensor
mask[:, :, -2, :, :] = 1   # except for person mask channel</code></pre>
<h2 id="损失计算图因为pytorch的动态机制越来越大直到耗尽内存">2.
损失计算图因为Pytorch的动态机制越来越大，直到耗尽内存</h2>
<p>摘录自</p>
<p>常见的原因有</p>
<h3
id="在循环中使用全局变量当做累加器且累加梯度信息">在循环中使用全局变量当做累加器，且累加梯度信息</h3>
<p>举个例子，下面的代码中</p>
<pre><code>total_loss=0
for i in range(10000):
  optimizer.zero_grad()
  output=model(input)
  loss=criterion(output)
  loss.backward() # 计算的梯度自动叠加到各个权重的grad上，并且计算完成后销毁计算图！！！
  optimizer.step()
  total_loss+=loss
  #这里total_loss是跨越循环的变量，起着累加的作用，
  #loss变量是带有梯度的tensor，会保持历史梯度信息，在循环过程中会不断积累梯度信息到tota_loss，占用内存</code></pre>
<p>以上例子的修正方法是在循环中的最后一句修改为：</p>
<p>total_loss+=float(loss)</p>
<p>或者 total_loss += loss.item() #
tensor.item()是取张量的python数值</p>
<p>利用类型变换解除梯度信息，这样，多次累加不会累加梯度信息。</p>
<h3 id="局部变量逗留导致内存泄露">局部变量逗留导致内存泄露</h3>
<p>局部变量通常在变量作用域之外会被Python自动销毁，在作用域之内，不需要的临时变量可以使用del
x来销毁。</p>
<h3
id="list数据类型不断append增长了计算图大小">list数据类型，不断append增长了计算图大小</h3>
<h2 id="pytorch中的batch-normalization-layer踩坑">3. Pytorch中的Batch
Normalization layer踩坑</h2>
<p>详情查看我的另一篇博客：<a
href="https://blog.csdn.net/xiaojiajia007/article/details/90115174"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90115174</a></p>
<h2
id="优化器的权值衰减weight_decay项导致的隐蔽bug即是的网络的权值不断减少至0">4.
优化器的权值衰减weight_decay项导致的隐蔽bug，即是的网络的权值不断减少至0</h2>
<p>摘录自：<a href="https://zhuanlan.zhihu.com/p/91485607"
class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>我们都知道weight_decay指的是权值衰减，（<strong>注意：<font color="#dddd00">权值衰减不等价于在原损失的基础上加上一个L2惩罚项！</font>
具体说明见下面那条笔记</strong>），使得模型趋向于选择更小的权重参数，起到正则化的效果。但是我经常会忽略掉这一项的存在，从而引发了意想不到的问题。</p>
<p>这次的坑是这样的，在训练一个ResNet50的时候，网络的高层部分layer4暂时没有用到，因此也并不会有梯度回传，于是我就放心地将ResNet50的所有参数都传递给Optimizer进行更新了，想着layer4应该能保持原来的权重不变才对。但是实际上，尽管layer4没有梯度回传，但是weight_decay的作用仍然存在，它使得layer4权值越来越小，趋向于0。后面需要用到layer4的时候，发现输出异常（接近于0），才注意到这个问题的存在。</p>
<p>虽然这样的情况可能不容易遇到，但是还是要谨慎：暂时不需要更新的权值，一定不要传递给Optimizer，避免不必要的麻烦。</p>
<h2 id="l2正则不等于权值衰减">5. L2正则不等于权值衰减</h2>
<p>权值衰减（Weight
Decay）：在网络权值通过损失函数更新后，直接再减去权值本身的一个倍数，可以写成
W(t+1)’ = W(t+1)-W(t)；</p>
<p>而 L2正则（L2
Regulation）：在原有的算是函数基础上，添加了网络权值平方和*一个倍数，L'
=
L+1/2∑w^2，注意在参数更新，对L'求关于某个分量的导数时其他参数视作常数，导数为0。</p>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/104045066"
class="uri">https://blog.csdn.net/xiaojiajia007/article/details/104045066</a></p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111126481.png" /></p>
<p>在Pytorch中，对于SGD优化器，两者是等效的，但是对于Adam优化器，两者作用有差别，对于Adam会有耦合的错误。</p>
<p>我看到有的开源项目中(<a
href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/utils/utils.py#L60">链接</a>)，SGD使用weight
decay，而Adam中没有使用weight decay。</p>
<p>具体分析见下面两个文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40814046"
class="uri">https://zhuanlan.zhihu.com/p/40814046</a>，</p>
<p><a href="https://zhuanlan.zhihu.com/p/63982470"
class="uri">https://zhuanlan.zhihu.com/p/63982470</a></p>
<h2 id="pytorch中的优化器weight-decay默认对bias偏置也起作用不合理">6.
Pytorch中的优化器weight decay默认对bias(偏置)也起作用，不合理</h2>
<p>添加偏置是有必要的：</p>
<p>https://zhuanlan.zhihu.com/p/158739701</p>
<blockquote>
<p>一般来说，我们只会对神经网络的<strong>权值</strong>进行正则操作，使得权值具有一定的稀疏性[21]或者控制其尺寸，使得其不至于幅度太大，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是<strong>调节每一层超平面的方向</strong>（因为<img
src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D"
alt="[公式]" />就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是<strong>调节每一层超平面的平移长度的</strong>，如果你对偏置进行了正则，那么我们的<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。</p>
</blockquote>
<p>解决方法不止一种</p>
<p>例如进行weight和bias参数过滤：https://www.cnblogs.com/lart/p/10672935.html</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.opti &#x3D; optim.SGD(</span><br><span class="line">    [</span><br><span class="line">        # 不对bias参数执行weight decay操作，weight decay主要的作用就是通过对网络</span><br><span class="line">        # 层的参数（包括weight和bias）做约束（L2正则化会使得网络层的参数更加平滑）达</span><br><span class="line">        # 到减少模型过拟合的效果。</span><br><span class="line">        &#123;&#39;params&#39;: [param for name, param in self.net.named_parameters()</span><br><span class="line">                    if name[-4:] &#x3D;&#x3D; &#39;bias&#39;],</span><br><span class="line">         &#39;lr&#39;: 2 * self.args[&#39;lr&#39;]&#125;,</span><br><span class="line">        &#123;&#39;params&#39;: [param for name, param in self.net.named_parameters()</span><br><span class="line">                    if name[-4:] !&#x3D; &#39;bias&#39;],</span><br><span class="line">         &#39;lr&#39;: self.args[&#39;lr&#39;],</span><br><span class="line">         &#39;weight_decay&#39;: self.args[&#39;weight_decay&#39;]&#125;</span><br><span class="line">    ],</span><br></pre></td></tr></table></figure>
<h2 id="torch.sqrt在0处的左导数没有定义会返回nan换用-torch.norm">7.
torch.sqrt()在0处的左导数没有定义，会返回nan，换用 torch.norm()</h2>
<p>例如： # https://github.com/pytorch/pytorch/issues/2421 # norm =
torch.sqrt((x1 - t1)**2 + (x2 - t2)**2)</p>
<p><code>norm = (torch.stack((x1, x2)) - torch.stack((t1, t2))).norm(dim=0)</code></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>python</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>英语写作</title>
    <url>/2020/05/15/%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E6%98%93%E9%94%99%E7%82%B9/</url>
    <content><![CDATA[<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/77971927"
title="Permalink to 英语写作_论文,写作_xiaojiajia007的博客-CSDN博客">Source</a></p>
<h2 id="and-but作连词-什么时候加逗号">1. and, but作连词
什么时候加逗号</h2>
<p><strong>1).
and连接两个以上的单词、词组或子句；连接两个相同的单词或短语,以强调某事物的程度、暗示某事继续发生或在一段时间内不断增加；连接两件相继发生的事件；连接两个从句,表示因果关系时不加.</strong></p>
<p>He lay on his back and looked up at the sky. 他仰卧着观看天空。</p>
<p>I'm going to write good jokes and become a good comedian.</p>
<p><strong>三个以上的词（短语）并列的场合，通常只在最后一个词（短语）前加and，这种情形，and之前加不加逗号都可以。如：</strong></p>
<p>I visited London, Paris<strong>(,)</strong> and
Rome.我游览过伦敦、巴黎和罗马。《英汉多功能词典 》</p>
<p>In that room there were a chair, a table<strong>(,)</strong> and a
bed.
在那房间里面有一张椅子、一张桌子和一张床。《21世纪英汉汉英双向词典》</p>
<p>▲<strong>但省略and之前的逗号的情况较多。如</strong>：</p>
<p>Joan was rich, beautiful and proud. 琼非常有钱,
漂亮且庄重。《简明英汉词典》</p>
<p>one woman, two men and three children 一个女人﹑ 两个男人及三个孩子
《牛津高阶英汉双解学习词典》</p>
<p>He was tall, dark and handsome.
他身材高大、皮肤黝黑、长相帅气。《朗文当代英语词典》</p>
<p><strong>2).
连接两句话,第二句话的意思是第一句的延伸时加逗号,</strong></p>
<p>如：You could only really tell the effects of the disease in the long
term, and five years wasn't long enough.</p>
<h3 id="另外一个解释"><strong>另外一个解释：</strong></h3>
<p><strong>and, but
连接两个独立句子的时候,需要逗号,如果是两个词组或是短语或是各种从句不用逗号.</strong></p>
<p><strong>当有多个并列成分时，最后的并列连词and 或 or
之前一般是不加逗号的。不过近年来，越来越多的作者在表达时出现and
前加逗号的情况，特别是在美国。</strong></p>
<p><strong>另外，当书写长而复杂的句子，特别是有多个并列结构嵌套时，作者往往可在and前加逗号来帮助读者理清句子。</strong></p>
<p>例如：there will be television chat shows hosted by robots,
<strong>and cars </strong>with polllution monitors that will disable
them when they offend. 加上and之后，句子变得更加清晰了。</p>
<p>For hit to be re-elected, what is essential is not that his policy
works<strong>, but that </strong>the public believe that it does.
(并列表语从句）</p>
<h3
id="as-well-as-不等同于and.-a-as-well-as-b侧重在于a他们地位不平等另外谓语动词单复数由a决定">As
well as 不等同于and. A as well as
B侧重在于A,他们地位不平等。另外谓语动词单复数由A决定。</h3>
<p>具体可参考： <a href="https://www.jianshu.com/p/6e9e11a784c2"
class="uri">https://www.jianshu.com/p/6e9e11a784c2</a></p>
<h3 id="关于but的其他用法">关于but的其他用法</h3>
<p>如 can/cannot but (只能/不得不）， all but
（除了...之外都，几乎），but for （=without，如果没有，若不是）， but
that （若非），but then （=on the other hand, 另一方面），aything
but（绝不）， 见《考研英语必背500句》P58.</p>
<h3 id="and-but是否能够置于居首">And, But是否能够置于居首</h3>
<p>如果前后有关系的，可以大些并且置于句首。但是建议不要太多，适量换成moreover,additionally
(in addition), however.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111125853.png" /></p>
<p><a
href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171330.jpeg">备份图片地址</a></p>
<h2 id="while-作为连词要不要加逗号"><strong>2. while
作为连词要不要加逗号</strong></h2>
<p>加逗号的一般表示转折,例如：some people believe soming is harmful to
health, while others consider smoking is helpful for reducing working
presure. 不加逗号一般表示“同时...” 例如：I worked for one hour outside
while it was raining.</p>
<p>表示转折也可不用逗号的，如下面两句就摘自《牛津高阶英汉双解词典》的
while 词条：</p>
<p>I drink black coffee <strong>while</strong> he prefers it
withcream.我爱喝清咖啡而他喜欢加奶油的。</p>
<p>English is understood all over the world <strong>while</strong>
Turkish is spoken by only a few people outsideTurkey itself.
英语世界通行，但土耳其语离开本国就很少有人说了。</p>
<h2
id="such-as-for-example-and-so-on-etc-i.e.-et-al点击打开链接-具体看链接"><strong>3.
such as, for example, and so on, etc, i.e., et al<a
href="https://wenku.baidu.com/view/05a1e58f58f5f61fb7366696.html">点击打开链接
（具体看链接）</a></strong></h2>
<h3
id="such-as-不论有举多少例并列成分的最后一个前面要加and.-需要注意的是如果结尾是etc.就不用加了因为etc.相当于and-so-on.">such
as 不论有举多少例,并列成分的最后一个前面要加and.
需要注意的是,如果结尾是etc.就不用加了,因为etc.相当于and so on.</h3>
<p>such as beijing, shanghai, xiamen and xian (或者 xiamen, and
xian)</p>
<p>Human pose estimation refers to the task of recognizing postures by
localizing body keypoints (head, shoulders, elbows, wrists, knees,
ankles, etc.) from images.</p>
<h3
id="因为such-as是对前面的复数名词部分起列举作用若全部列举出要改用namelythat-is-或者i.e意思为即">因为such
as是对前面的复数名词部分起列举作用,若全部列举出,要改用namely，that is
或者i.e,意思为“即”。</h3>
<p>比如 He knows four languages, such as Chinese, English, Japanese and
German,应将such as改成namely(或i. e. )及后面加逗号, 即He knows four
languages, namely, Chinese, English, Japanese and German.</p>
<p>He knows four languages, that is, Chinese, English, Japanese and
German.</p>
<h3
id="当使用-such-as-for-example-e.g.-表示泛泛地举几个例子时读者已理解后面接着的会是一些不完整的列举-因此不要再加上-and-so-on-或-etc.-等">当使用
such as ，for example (e.g.)
表示泛泛地举几个例子时,读者已理解后面接着的会是一些不完整的列举,
因此不要再加上 and so on 或 etc. 等!</h3>
<p>它们就已经包含“等等”,如果再加etc. 或and so on,就画蛇添足了。</p>
<p>Writing instructors focus on a number of complex skills that require
extensive practice, e. g. , organization, clear expression, and logical
thinking.</p>
<p><strong>for
example用来举例说明某一论点或情况,一般只举同类人或物中的"一个"为例,作插入语,可位于句首、句中或句末。</strong></p>
<p>Cryptography operations, for example, decryption or signing, in a
given period only involve the corresponding temporary secret key without
further access to the helper.</p>
<p>A lot of people here, for example, Mr John, would rather have coffee.
这儿的许多人，例如约翰先生，宁愿喝咖啡</p>
<h3
id="like也常用来表示举例可与such-as互换但such-as用于举例可以分开使用此时不可与like互换">like也常用来表示举例，可与such
as互换。但such as用于举例可以分开使用，此时不可与like互换。</h3>
<p>Some warm-blooded animals，like/such as the cat，the dog or the
wolf，do not need to hibernate.</p>
<p>He has several <strong>such</strong> reference books <strong>as
(分开了)</strong> dictionaries and handbooks.
他有几本像字典、手册之类的参考书。</p>
<h2 id="冠词"><strong>4. 冠词 </strong></h2>
<p><a
href="http://blog.sina.com.cn/s/blog_5e0022550100ctz2.html">一个比较全的说明</a></p>
<h3
id="通常情况下复数名词物质名词专有名词前是不加冠词的但如果是特指的话复数名词前就要加the">通常情况下复数名词，物质名词，专有名词前是不加冠词的。<strong>但如果是特指的话，复数名词前就要加the</strong>。</h3>
<p>例如：The books on the table are mine. The water in the glass is
hot.</p>
<h3
id="具体的来说什么时候不需要加冠词"><strong>具体的来说，什么时候不需要加冠词，</strong></h3>
<p>作者：未某人
链接：https://www.zhihu.com/question/20321498/answer/22869588 来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<ol type="1">
<li><strong>某事物和其他事物比较时，要加定冠词；自身与自身比较，不用定冠词。</strong></li>
</ol>
<p>以前老师都说形容词最高级前要加冠词 The lake is the deepest in the
world.</p>
<p>这一句加了定冠词，因为其实后面的名词省略了，完整说法是the deepest
(lake) in the world。再请看下面两个例句：</p>
<p>The lake is deepest at this point. 湖水在这一点最深</p>
<p>I feel happiest when my life is maddest.</p>
<p>以上两句最高级都没有定冠词，因为此处 deepest，happiest，maddest
都只是一般的形容词，你在后面没法再补全一个名词，没有名词被省略。既然只是形容词当然没必要加定冠词了。</p>
<p>2.<strong>部分名词作补语起到形容词作用</strong>，该名词词义不再是其所指对象本身，而是所指对象的特征，此时该名词相当于形容词，故<strong>无需冠词</strong>。请看例句</p>
<p>He is fool enough to marry her.</p>
<p>He is no fool.</p>
<p>I'm not philosopher enough to think out a solution.</p>
<p>以上 fool 并非表示笨蛋，而是笨这个特质。Philosopher
也不是哲学家，而是聪明这个特质</p>
<p>3.<strong>复数普通名词前有无 the 的区别：有 the 表示全部，没有 the
表示绝大部分（即有例外）</strong>。硬要解释的话，应该是有了 the
以后将限制的名词视为一个整体，整体中包括所有个体，没有 the
的话就只是单纯的名词复数，表示多个。我自己说出来都觉得很抽象不好理解，看例句吧~</p>
<p>Americans think that they can win the war. 很多美国人认为可以赢</p>
<p>The Americans are an active people.
美国人是活跃的民族（指所有美国人）</p>
<p>Students of our school are diligent. 我们学校学生大多很勤奋</p>
<p>The students of our school are diligent. 所有学生都勤奋</p>
<p>4.He came to the town. （他对于这座城是个陌生人）他来到城里</p>
<p>He came to town. （他和这座城市有关系）他来到城里</p>
<p>为什么这里没有 the 显得他对于这座城很熟悉，而有了 the
显得陌生？我个人的理解是没有 the 的时候，town
就像是一个专有名词，一说他去了城里，大家都知道他去的是哪座城；有了 the
说明需要特指一下是“这座城”，既然需要特指，说明这座城和他没有特定关系。</p>
<p>5.World War II 无冠词 the Second World War 有冠词</p>
<p>6.though/as
引导让步状语从句，名词前省冠词；但若有形容词修饰，不省略冠词</p>
<p>Child as/though he is, he knows much about the society.</p>
<p>A small child as he is, he can solve the problem.</p>
<p>7.<strong>(泛指的物质名称）</strong>，中文中说某某人是个当官的好材料这个当官的好材料英文就是
leader material，同理，踢球的料就是 football material，这些 XXX material
之<strong>前都不加冠词的。当然如果不是泛指，而是特指，那么还是要加 the
的。</strong></p>
<p>8.在新闻标题，标志语，广告词，商品标签等中，为了节约篇幅，会有意省略冠词。</p>
<p>9.<strong>大多数专有名词前不要加冠词：</strong></p>
<p><strong>专有地区名称不加：</strong> Do you know <strong>Nanjing
Road</strong> in Shanghai？你知道上海的南京路吗？</p>
<p><u>海洋、山脉、河流前要加the</u>，the Yangtze River
(待确定，记忆方法，这些都有“动”的感觉)</p>
<p><u>海岛、湖泊、山峰不加the</u>, Mount Everst
(待确定，记忆方法，这些都有“禁止”的感觉)</p>
<p><strong>但是由某些由普通名构成的专有名词前要加定冠词the</strong> the
People's Republic of China 中华人民共和国 the United States of America
美利坚合众国 the Ming Dynasty 明朝 the Great Wall长城 the Great Cultural
Revolution文化大革命</p>
<p>10.<strong>用作称呼语或表示头衔的名词前不要加冠词：</strong> What are
you reading，Boy？孩子，你在读什么？ He is head of the
factory．他是工厂的厂长。</p>
<p>但如果还有修饰语，就要加the，如 the 32nd President/Chirman of ...</p>
<p>11.【<strong>泛指的】抽象名词前不加冠词</strong>： Life is always
presenting new things to
children．生活总是不断地呈现给孩子们新的东西。</p>
<p><strong>12. 一些特例</strong></p>
<p><strong>mankind(人类)</strong></p>
<p>人是一个不可数的集合名词，不用<a
href="https://baike.baidu.com/item/%E5%A4%8D%E6%95%B0/13131232">复数</a>形式，也不连用冠词。如：This
is an invention that benefits mankind. 这是一项造福人类的发明。Mankind
has its own problems. 人类有自己的问题。注：mankind 表示“mankind
人(类)”时，虽不可数，但有时却可以表示复数意义，尤其是当其<a
href="https://baike.baidu.com/item/%E8%A1%A8%E8%AF%AD">表语</a>是复数时。如：Mankind
are intelligent animals. 人是理智的动物。</p>
<p><strong>13. 乐器前要加 the, 球类前不加the</strong></p>
<p>play the violin (乐器）</p>
<p>play football (球类）</p>
<p>14. <strong>大写缩略词前要不要定冠词the分为两种情况</strong></p>
<p><strong>如果缩略词可以单独发音，不加the</strong>。比如NASA，NATO，FIFA，TOEFL，APEC；</p>
<p><strong>如果不能单独发音，需加‘the’</strong>，比如 the NBA, the FBI,
the HBO.</p>
<p><strong>15. 这里补充一个总结
（看不看无所谓，前面机会全包括了）</strong></p>
<p>转载自：英文论文写作有哪些需要注意的细节？ - 春时粟的回答 - 知乎
https://www.zhihu.com/question/46825717/answer/652234107</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111125432.png"
alt="preview" />
<figcaption aria-hidden="true">preview</figcaption>
</figure>
<p><a
href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171358.jpeg">备份图片地址</a></p>
<h3 id="定冠词基本用法">定冠词基本用法:</h3>
<p>1）特指某人、某事 Wellington is the capital of New Zealand.
惠灵顿是新西兰的首都。 2）指世上独一无二的事物 We have friends all over
the world .我们的朋友遍天下。 The moon goes around the earth
.月亮绕着地球转。 The sun is rising in the east .太阳在东方冉冉升起。
3）重提前文中提到过的人或事物（即文中第二次出现的人或事物） He,
suddenly, saw an isolated house at the foot of the mountain. And
curiosity made him approaching the house.
他突然看到山脚下有一栋孤独的房子；好奇心驱使他向那栋房子走了过去。
4）说话人和听话人都熟悉的人或事物 Be sure to bring me the book when you
come next time. 你下次来一定要将那本带给我。
5）用于序数词、形容词的最高级形式、和表示方位的名词前 Thanksgiving Day
is on the 4th Thursday in November. 感恩节在每年十一月的第四个星期四。
Changjiang is the longest river in China. 长江是中国最长的河流。 Japan
lies to the east of China .日本位于中国的东面。 He is one the most
famous football stars in the world. 他是世界最著名的足球明星之一。</p>
<p>6）间或用于单数的可数名词前表示泛指 The compass was invented in
China.指南针是中国发明的。 The horse is a useful animal
.马是有用的动物。 The tiger is in danger of extinct .老虎有绝种的危险。
The monkey is a clever animal. 猴是一种聪明的动物。
注：这种"泛指"是从整个属类的意义上说，而不是"用一个人或物来说明整个属类的
特点"。也就是说，属前者情况时加用定冠词表示泛指，属后者情况时则加不定冠词表示泛指。</p>
<p>7）用于某些由普通名构成的专有名词前 the People's Republic of China
中华人民共和国 the United States of America 美利坚合众国 the Ming
Dynasty 明朝 the Great Wall长城 the Great Cultural
Revolution文化大革命</p>
<p>8）用于某些词组中。这种用法是约定束成的，我们只有遵从而无旁的选择。
in the morning ( afternoon , evening )上午（下午，晚上） go to the
cinema 看电影 on the whole总体上 to the best of就……所及 the sane as
和……一样 out of the question不可能的 on the one hand一方面 on the other
hand 另一方面 on the average一般说来 on the contrary相反地 in the least
一点，丝毫 in the long run从长远来看 in the event of 万一 in the final
analysis归根结底</p>
<p>9）定冠词+形容词使形容词名词化 We always stand for the oppressed and
the exploited. 我们永远支持受压迫、受剥削的人们。 The aged are well
taken care of in the community. 在这个社区，老人得到了很好的照顾。 She
was fond of writing about the unusual. 她喜欢写一些古怪的题材。 The
school for the deaf and the blind is just newly built.
那所聋哑人学校是刚刚新建的。</p>
<h3
id="元音开头的单数名词前用不定冠词an">元音开头的单数名词前用不定冠词an</h3>
<p>注意在元音开头的名词前用an，/j/ 和
/w/叫做半元音，这两个都被归类为辅音！</p>
<p><strong>A</strong> <strong>European</strong> （[ˌjʊrəˈpiːən] ） is a
person who comes from Europe.</p>
<p>字母u其实是辅音开头[juː] ！</p>
<h2 id="时态问题"><strong>5. 时态问题 </strong></h2>
<h3
id="美国人的文章里大多全用一般现在时">美国人的文章里，大多全用一般现在时。</h3>
<p><strong><a
href="http://blog.csdn.net/lcj_cjfykx/article/details/27173883">点击打开链接</a></strong></p>
<h3 id="if-引导的真实条件句中的时态"><strong>if
引导的真实条件句中的时态</strong></h3>
<p>除了if引导强调 【<strong>个人主观意愿，如愿望请求命令】 </strong>和
【<strong>与事实不相符的假设】
</strong>的虚拟语气外，if引导的真实的条件句用于陈述语气，假设的情况可能发生。</p>
<p><a href="http://ishare.iask.sina.com.cn/f/34zCicivVbQ.html"
class="uri">http://ishare.iask.sina.com.cn/f/34zCicivVbQ.html</a></p>
<h3
id="suggest-表示建议时用虚拟语气表明或者暗示的意思时不用虚拟语气">suggest
表示建议时用虚拟语气，表明或者暗示的意思时不用虚拟语气</h3>
<p>当suggest表示建议的时候，后面的宾语从句要用虚拟语气，用should
do的形式（should可以省略）。</p>
<p>当suggest 作“表明、暗示”或“使人想到”时，不用虚拟语气。</p>
<p>President Macron of France has just <strong>suggested</strong> that
Europe (should) build its own military 这里是虚拟语气的一种用法。</p>
<p>His expression suggests that he has had a good
journey.　他的表情表明他旅途很愉快。（这里就是根据证明表明，并不是虚拟语气）</p>
<h3
id="if引导虚拟语气考研英语500句p6--17th和p22--78th">if引导虚拟语气：《考研英语500句》P6
-17th和P22 -78th</h3>
<p>分为对现在/将来的虚拟，过去的虚拟，不同的句式固定搭配would或者should，需要注意。虚拟语气把事态都往后推了一步，例如条件句中把将来的shall
--&gt; should，be --&gt; were
to。这里可以把情态动词shall/will/may等看作助动词，自然就有过去的时态了。</p>
<h3
id="主句和从句中的动作发生有先后关系时需要注意两者的时态">主句和从句中的动作发生有先后关系时，需要注意两者的时态</h3>
<p>比如：After he had finished his homework, he went to bed.
做完作业后，他上床睡觉了。</p>
<p>She felt humble just as she had (felt) whe she <strong>had fisrt
taken</strong> a good look at herself.
此句中，as引导方式状语从句，when引导时间状语从句。看她自己发生在感觉之前，所以是过去的过去。</p>
<p>注意： before, after 引导的时间状语从句中，由于 before 和 after
本身已表达了动作的先后关系，若主、从句表示的动作紧密相连，则主、从句都用一般过去时。如：
After he closed the door, he left the
classroom.他关上了门，离开了教室。</p>
<h3 id="瞬时性动词与延续性动词的区别">瞬时性动词与延续性动词的区别</h3>
<p>瞬时性动词又叫点动词，<u>也是可以用于现在完成时态</u>，</p>
<p>例如：The train has arrived. Have you bought the bok?</p>
<p>但由于动作是瞬时间完成的所以<strong>不能与for或since引出的时间状语连用。因此要留心</strong>后面的时间状语表示的是一段时间还是一个时间点**，这往往决定了瞬时性动词与延续性动词的选择。例如</p>
<p>His fatherhas died for two years. (误)</p>
<p>His fatherhas been dead for two years. (对)</p>
<p>错：When did she know my e-mall address?</p>
<p>对：When did she get to know my e-maail
address?她什么时候知道我的e-mail地址?
不能延续的动作，这种动作发生后立即结束</p>
<h2 id="respectively-individually-each-separatelly">6. respectively,
individually, each, separatelly</h2>
<p>each每个,各自地 adj, adv,</p>
<p>eg. They cut the cakein two andatehalf each</p>
<p>hese apples willgohalf apoundeach.</p>
<p>respectively各自地,
前面提到了一种顺序，后面的与前面的一一对应，用于句尾</p>
<p>individually个别地,个性的</p>
<p>separately分别地,分开地</p>
<h2
id="将句子的main-idea核心放在开头其他表示时间条件等放在后面"><strong>7.
将句子的main
idea核心放在开头，其他表示时间，条件等放在后面</strong></h2>
<h2 id="英语写作检查软件">8. 英语写作检查软件</h2>
<p><a
href="http://blog.csdn.net/chuminnan2010/article/details/21811117">点击打开链接</a></p>
<h2 id="不应该否定词的缩写而应该使用完整形式">9.
不应该否定词的缩写，而应该使用完整形式</h2>
<p>如cannot (特殊的一个，中间没有空格）， is not, do not</p>
<h2 id="主谓一致"><strong>10. 主谓一致</strong></h2>
<h3
id="both...and...两者都...连接名词或代词作主语时谓语动词用复数形式不受就近原则的限制如">1.
both...and...“两者都...”，连接名词或代词作主语时，<a
href="https://www.baidu.com/s?wd=%E8%B0%93%E8%AF%AD%E5%8A%A8%E8%AF%8D&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBrj6dnA7BryckP1bzrycL0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnHmknjcYnWDv">谓语动词</a>用复数形式，不受‘<a
href="https://www.baidu.com/s?wd=%E5%B0%B1%E8%BF%91%E5%8E%9F%E5%88%99&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBrj6dnA7BryckP1bzrycL0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnHmknjcYnWDv">就近原则</a>’的限制。如：</h3>
<p>Both you and I are students. Both Li Ming and Wei Hua are good at
English.</p>
<h3
id="and-连接两个成分作主语时谓语要用复数形式"><code>2. and 连接两个成分作主语时，谓语要用复数形式。</code></h3>
<p><code>如，You and she are on duty today.</code></p>
<h3
id="主谓一致采用就近原则的情况"><code>3.</code><strong>主谓一致</strong><code>采用就近原则的情况：</code></h3>
<p><code>1). 当连词或连词短语连接两个成分，而重点强调的是“其中之一”的时候，要采用就近原则。</code></p>
<p><strong>or</strong>, either...or..., neither...nor..., not only...but
also...等连接主语时，谓语动词以后面的那个主语为主。 如，</p>
<p><code>You or he is right.</code></p>
<p><code>Either your teacher or your classmates were there.</code></p>
<p><code>Neither you nor he is able to finish the work in an hour.</code></p>
<p><code>Not only your parents but also I am proud of you . Not only he but also all his family are keen on concerts</code></p>
<p><strong>2). and
连接两个成分作主语时，谓语要用复数形式。</strong>如，You and she are on
duty today.</p>
<p><strong>3). there be A and B 句型，there
be的be动词由靠的近的A决定！</strong></p>
<p>There <strong>is</strong> <strong><em>a book</em></strong> and
<em>two pens</em> in my bag.</p>
<p>4). as well as
连接主语时，谓语动词的单复数必须与前面那个保持一致。</p>
<p><strong>这是因为as well as 不等同于and! A as well as B
其实侧重在A</strong>，相当于not only B, but alse A.
这样理解的话谓语单复数就和重要的A保持一致了。</p>
<p>比如：</p>
<p>John, as well as Mary, come to the party. <strong>错</strong></p>
<p><strong>John</strong>,as well as Mary, <strong>comes</strong> to the
party. 对</p>
<ol start="5" type="1">
<li><strong>rather than
连接两个名词或代词作主语时</strong>，<strong>谓语动词应与rather than
前面的名词或代词在</strong>人称和数上保持一致（我感觉还是因为其实侧重在前者，所以和前者保持一致）。</li>
</ol>
<p><strong>You rather than</strong> Smith <strong>are</strong> going to
go camping. 是你而不是我要去要野营。</p>
<h3 id="有同位语时谓语动词与主语保持一致.">4.
有同位语时,谓语动词与主语保持一致.</h3>
<p><strong>We</strong> <em>each</em> <strong>have</strong> a cellphone：
<em>each是we的同位语,</em>我们每个人的意思,谓语动词的数取决于主语,不取决于同位语each.</p>
<p>但若是Each of us……each 就是主语
后面谓语动词就是单数同位语：一个名词(或其它形式)对另一个名词或代词进行...</p>
<h3 id="more-than-one"><strong>5. more than one</strong></h3>
<p>分析如下：</p>
<p><strong>1). more than one +单数名词 作主语时, 尽管从意义上看是复数,
谓语动词用单数 (要从形式一致来考虑).</strong></p>
<p>例如: More than one boy【has read】the story. 注意 more than one
boy中的boy用的是单数。</p>
<p>不止一个男孩已经读了这个故事。</p>
<p>2). more than one +数词＋复数名词 作主语时,谓语动词用复数.</p>
<p>例如: More than one thousand men and women【are working】in this
factory.</p>
<p>有1000多男女工人在这家工厂做工。</p>
<p>3). more + 复数名词 + than one 作主语时,谓语动词用复数.</p>
<p>例如：More boys than one【have read】the story.</p>
<p>6. a set of + 复数名词 + 谓语动词用单复数都有可能</p>
<p>当“a set of + 复数名词”作主语时，谓语动词用单数或复数都有可能</p>
<p>A set of guidance notes <strong>is</strong> provided to assist
applicants in completing the form.
有一系列注意事项，指导申请人填写表格</p>
<p>An important set of ideas <strong>have</strong> been advanced by the
biologist Rupert Sheldrake.
生物学家鲁珀特•谢尔德雷克已提出了一整套重要的观点。</p>
<h2 id="否定句和疑问句中any接不可数名词或者可数名词的复数">11.
否定句和疑问句中any接不可数名词或者可数名词的复数</h2>
<p>如， You can't go out <strong>without any shoes</strong></p>
<p>any 也可以用在if或者whether之后，或者某些动词之后，表示
任何的，任一的，此时与可数名词单数使用。</p>
<p>如， take any book you like. Any color is OK. Any teacher will tell
you that.</p>
<h2 id="of-的前后名词的单复数">12. of 的前后名词的单复数</h2>
<p>The teller mechine can accepts any dinomination of coins and notes
为什么这里的coin,note用复数，而dinomination是单数形式？</p>
<p>domination前面有any修饰
英语里面的可数名词一般很少单独以单数出现，要么有冠词，要么用复数
这里的coins和notes都是泛指，所以用复数</p>
<h2 id="并列结构前后一致的问题以及省略用法">13.
并列结构，前后一致的问题以及省略用法</h2>
<h3 id="rather-than">rather (than)</h3>
<p>1．<strong>rather than后面一般是 rather than do sth</strong>.
<strong>跟情态动词would,should,will等连用构成固定搭配</strong>,有时rather
than可以分开,意为“宁可”、“与其……倒不如”. 此时,"<strong>rather than+ do
sth</strong>". 例如： I'd rather than go there by
air．我宁愿乘飞机去那里.</p>
<p>I'd rather have a quiet night, reading my favorite book. 这里would
rather意思是“宁愿、宁可、更、最好、还是为好”，后接动词原形,常省略为’d
rather.</p>
<p>This instrict should <strong>be</strong> encouraged rather than
<strong>(be)</strong> laughed at. be动词</p>
<p>I'd rather walk than ride a bike.</p>
<p>2．作准<strong>并列连词</strong>,相等于and
not,意义为：是...,而不是...:可以+doing 或 to do.例如： 1）He was engaged
in writing "rather than" reading the newspaper．他正忙着写东西,而不是
在读报纸."rather than+ doing" 2）He is to be pitied rather than to be
disliked．他应该得到怜悯而不是厌恶."rather than+ to do"
所以意义表“宁愿”+do sth; 表“是...,而不是...:”++doing 或 to do.</p>
<p><strong>注意：rather than
后接不定式时，不定式可以带to，也可以不带to</strong>, 如：</p>
<p>I decided to write rather than (to) telephone.
我决定写信而不打电话。</p>
<p><strong>但rather than位于句首时，则只能接不带to
的不定式</strong>。</p>
<p>如：Rather than allow the vegetables to go bad, he sold them at half
price. 他唯恐蔬菜烂掉，把它们以半价卖掉了。</p>
<p>3）no... but rather... 不是......而是...... We don't discuss such
problems, but rather deal with them.</p>
<p>4）You rather than I are going to go camping.
是你而不是我要去要野营。 注意：rather than
连接两个名词或代词作主语时，<u><strong>谓语动词应与rather than
前面的名词或代词在人称和数上保持一致。</strong></u></p>
<p>5）连接两个分句 We should help him rather than he should help us.
是我们应该帮助他而不是他应该帮助我们。</p>
<p>6）连接两个动词，表示可观事实，而不是主观愿望宁愿怎么样。 He ran
rather than walked. 他是跑来的，而不是走来的。 注意：这里rather than
后用了walked，而没有用walk，表示客观事实，而不是主观愿望。如果换成walk，则作“宁愿……而不愿
……”解。</p>
<p>3. would rather (sooner)
后跟表示虚拟语气的<strong>宾语从句</strong>，谓语动词为过去式，表示对过去或者将来的虚拟。</p>
<p>Kids would rather <strong>play</strong> than study.</p>
<p>I <strong>would rather</strong> （that) you <strong>told</strong> me
the truth. (接宾语从句) 我宁愿你告诉我真相。</p>
<h3 id="不定式to在什么情况下可以省略">不定式to在什么情况下可以省略</h3>
<p><strong>1. 当and或or连接两个并列不定式时，第二个to常省。
</strong>摘录自：<a href="https://www.hjenglish.com/new/p1042599/"
class="uri">https://www.hjenglish.com/new/p1042599/</a></p>
<p>I plan <strong>to</strong> call him and discuss this question.</p>
<p>我计划给他打电话，讨论一下这个问题。</p>
<p>My friend in China asked me** to <strong>telephone or write</strong>
**to her in my free time.</p>
<p>我中国的朋友让我有空给她打电话或写信。</p>
<p><strong>2. 当两个并列to有对比意义，第二个to不能省。</strong></p>
<p>I haven’t decided <strong>to</strong> stay at home or
<strong>to</strong> travel to Beijing this holiday.</p>
<p>我还没决定假期是待在家里还是去北京旅行。</p>
<p><strong>To</strong> be, or not <strong>to</strong> be, that is the
question.</p>
<p>生存还是毁灭，这是一个值得思考的问题。（《哈姆雷特》）</p>
<p><strong>3. 当两个to之间无并列连词，to不可省。</strong></p>
<p>I came here not <strong>to</strong> help you, but <strong>to</strong>
fright you.</p>
<p>我来这不是为了帮你，而是为了吓唬你。</p>
<p><strong>4.
当三个或以上带to不定式构成排比，所有to不可省。</strong></p>
<p>Read not** to <strong>contradict or confute; nor </strong>to**
believe and take for granted; not** to <strong>find talk and discourse;
but </strong>to** weigh and consider.</p>
<p>读书时不可存心诘难作者，不可尽信书上所言，亦不可只为寻章摘句，而应推敲细思。（《论读书》）</p>
<p>因此，关于不定式，我们除了要分清带to不定式和不带to不定式，还要掌握带to不定式在哪些情况下要省略to这一符号，做到具体情况具体分析。</p>
<p><strong>5.
介词短语to作为并列的宾语，因为比较长，保留to可能会使得句子结构更加清晰？</strong></p>
<p>It <strong>applies</strong> equally <strong>to </strong>traditional
historians who view history as only the external and internal criticism
of sources, <strong>and to</strong> social science historians who
.....</p>
<h3
id="but后接的动词不定式to-do的三种形式何时能够省略to">but后接的动词不定式to
do的三种形式，何时能够省略to</h3>
<p>关于这个问题，大致可分三种情况。</p>
<p>第一种情况是but之后的动词不定式一般不带to，如：</p>
<p>(1)He <strong>did </strong>nothing but complain.
（连词but后省去了that he did）</p>
<p>(2)Under such circumstances he could not but fail.
（but之前省去了<strong>do</strong> anything）</p>
<p>(3)I cannot help but be sorry.
（but省去to是受了例(1)结构的影响所致）</p>
<p>(4)He could not choose but love her.
（也是受了例(1)的结构的影响，这种说法现在已不多见）</p>
<p>第二种情况是but之后的动词不定式一般须带to，如：</p>
<p>I have no choice but to accept the fact. （but后省去了the
choice）</p>
<p>There was no choice but to bear it and grin. （理由同上）</p>
<p>第三种情况是but后的动词不定式可带to亦可不带to，如：</p>
<p>There is nothing to do but (to) fight it out.
（部分地受到了例(1)结构的影响）</p>
<p>There was clearly nothing left to do but (to) flop down on the shabby
little couch and weep. （理由同上）</p>
<p>There remained nothing but (to) get into the water…（理由同上）</p>
<p><strong>except与but后的不定式何时可省to？</strong></p>
<p>有一读者问：有这样四个句子：</p>
<p><strong>(1)He seldom goes back home except to ask for money from his
parents.</strong></p>
<p><strong>(2)He did nothing there except watch TV for the whole
night.</strong></p>
<p><strong>(3)I had no choice but to stay in bed.</strong></p>
<p><strong>(4)Last night I did nothing but prepare my
lessons.</strong></p>
<p>我搞不懂except和but之后何时接to何时不接to。</p>
<p><strong>第一种情况：其前的谓语动词为do时(必须是实义动词do以及它的各种时态变形，而不是其他动词），可不用to，如例句(2)和(4)（用to也不为错）</strong></p>
<p><strong>第二种情况：其前的谓语动词不是do，则一般须用to，如例句(1)和(3)</strong></p>
<p><strong>第三种情况：其前为there is nothing to
do，则可用可不用to，如例句(5)：</strong></p>
<p><strong>(5)There is nothing to do but (to) fight it out.</strong></p>
<h3 id="as引导比较状语从句">as引导比较状语从句</h3>
<p><strong>as引导比较状语从句，其基本结构是as…as。前一个as是副词，后一个as是比较状语从句的连词</strong>。否定结构为not
so much … as …。例1：Small as it is, the ant is as much a creature as
are all other animals on the earth.
尽管蚂蚁很小，但是它同地球上的任何其他动物一样，也是一种动物。 例2：It
was not so much the many blows he received as (连词) the lack of
fighting spirit that led to his losing the game.
与其说是他受到了许多打击，还不如说是缺乏斗志使他输掉了比赛。</p>
<p>例3. You had <strong>as good/well </strong>go <strong>(do)</strong>
there on foot <strong>as</strong> wait <strong>(do) </strong>for the
bus, since the company is not far away.</p>
<h3 id="并列原因状语">并列原因状语</h3>
<p>The behavioral sciences have been slow to change <strong>partly
because</strong> the explanatory items often seem to be directly
observed** and partly because** other kinds of explainations have been
hard to find.</p>
<h3 id="并列表语从句">并列表语从句</h3>
<p>What is essential <strong>is not that</strong> his policy works,
<strong>but that </strong>the publick beliebe that it dose.</p>
<h3 id="并列定语">并列定语</h3>
<p>This seems mostly effetively done by supporting a certain amount of**
research ** not related to immediate goals but of possible consequence
in the future.</p>
<p>等等很多复合结构....</p>
<h2
id="meet-the-need-of也有meet-the-needs-of应该是看后面指代的是一件事还是多件事">14.
meet the need of，也有meet the needs
of，应该是看后面指代的是一件事还是多件事</h2>
<h2
id="a-equals-b-equal是及物动词后面不需加to-等价于-a-is-equal-to-b-用作形容词">15.
A equals B (equal是及物动词，后面不需加to） 等价于 A is equal to B
(用作形容词）</h2>
<h2 id="in-detail-是对的但是可以说-the-details-of-...">16. In detail
是对的，但是可以说 the details of ...</h2>
<blockquote>
<p>In detail</p>
</blockquote>
<p>Is correct. Assuming that the context looks like this.</p>
<blockquote>
<p>[After general description]</p>
<p>In detail, the algorithm will....</p>
</blockquote>
<p>But you could always say..</p>
<blockquote>
<p>Here, I describe the details of the algorithm.</p>
</blockquote>
<h2 id="becasue-和-because-of">17. becasue 和 because of</h2>
<p>具体可参考：<a
href="https://wenku.baidu.com/view/38c06463f011f18583d049649b6648d7c1c7089c.html"
class="uri">https://wenku.baidu.com/view/38c06463f011f18583d049649b6648d7c1c7089c.html</a></p>
<h3
id="because可以用来引导原因状语从句-或者-表语从句">because可以用来引导原因状语从句
或者 表语从句</h3>
<p>It is because you’re eating too much. 那是因为你吃得太多了.</p>
<p>汉语说“之所以……是因为……”，英语可以类似以下这样的句型（用that比用
because普通）。如：</p>
<p>The reason (why) I’m late is that [because] I missed the bus.
<strong><em>(引导表语从句</em></strong>）
我迟到的原因是因为我没有赶上公共汽车。</p>
<p><strong>传统语法认为这类句型不能用 because, 但在现代英语中用because
的情形已很普遍。</strong></p>
<h3
id="because有时可引导一个句子作主语此时通常采用just-because这样的形式并且主句谓语动词通常当然不是一定为mean">because有时可引导一个句子作主语，此时通常采用just
because这样的形式，并且主句谓语动词通常（当然不是一定）为mean。</h3>
<p>如： Just because you speak English doesn’t mean you can teach it.
你会说英语并不意味着你能教英语。</p>
<h3
id="because-of-用于构成复合介词because-of其后可接名词代词-动名词what-从句但不能是that从句或没有引导词的从句等">because
of 用于构成复合介词because of，其后可接名词、代词、 动名词、what
从句（<strong>但不能是that从句</strong>或没有引导词的从句）等。</h3>
<p>如：</p>
<p>He is here because of you (that). 他为你（那事）而来这里。
可以接代词that，但是不能接that从句。</p>
<p>We said nothing about it, because of his wife’s being there.
因为他妻子在那儿，我们对此只字未提</p>
<p>He left the company because of what the boss said at the meeting.
他离开了这家公司，是因为老板在会上讲的话</p>
<h3
id="because-of通常用来引导状语而不能用于引导表语引导表语时可用-due-to但是若主语是代词不是名词则它引出的短语也可用作表语"><strong>because
of通常用来引导状语，而不能用于引导表语（引导表语时可用 due
to）</strong>。<strong>但是，若主语是代词（不是名词），则它引出的短语也可用作表语。</strong></h3>
<p>误：His absence is because of the rain. 正：His absence is due to the
rain. 他因雨未来。</p>
<p>主语是代词，可以引导表语，如：</p>
<p>It is because of hard work. 那是因为辛苦工作的原因。 It will be
because of money. 那将都是因为钱的原因。</p>
<h2 id="容易混淆的可数和不可数名词以及既可以可数又可以不可数">18.
容易混淆的可数和不可数名词，以及既可以可数又可以不可数</h2>
<p><strong>不可数的抽象名词用来表示可数的人或物----抽象名词具体化（名词的数和词义发生变化！！！）</strong></p>
<p>例如：详情见： <a
href="http://blog.sina.com.cn/s/blog_53ca7b1b0102e0ac.html">http://blog.sina.com.cn/s/blog_53ca7b1b0102e0ac.html</a></p>
<p>1.<strong>Thought </strong> n. 1）[U] 思维;思考,考虑，深思;
没有复数形式。 • After serious thought, he decided to accept their
terms.经认真考虑,他决定接受他们的条件。 • After much thought he decided
not to buy the car.经过仔细考虑后他决定不买汽车了。</p>
<ol start="2" type="1">
<li><p>(常用在疑问、否定句中)意图，打算，倾向意图；目的：[U][(+of)] • He
had no thought of hurting her.他没想要伤害她。 • There was no thought of
coming home early.没想到提早回了家</p></li>
<li><p>[C] 想法，见解，观念；念头 (+of/about/on)]为可数名词有复数形式 •
Please write and let me have your thoughts on the
matter.请写信让我知道你对此事的看法。 • She's a quiet girl and doesn't
share her thoughts.她是个内向的女孩，不表露她的想法。</p></li>
<li><p>关心,注意，留意；注意；关怀；悬念（常与of, to连用）[C]
[U][(+for)] • The nurse was full of thought for the sick men.
那护士非常关怀病人。 • Her husband didn't give much thought to what she
said.她的丈夫不在乎她说什么 With no thought for his own safety, the old
man went off at a run to save the drowning
boy.老汉毫不顾虑自己的安全,奔去救那溺水的男孩。 拓展辨析</p></li>
</ol>
<p>Thinking n. [U] 没有复数形式。 1） thinking思想,思考，考虑 • I have
to do some thinking before making a
decision.我得先思考一下,然后才好作决定。 2）thinking [U]
意见，想法，观点；意见；见解；看法 • To my thinking, this is not a good
idea.我认为这不是个好主意 • independent thinking 独立思考 • wishful
thinking一厢情愿的想法 • in modern thinking按照现代的想法</p>
<p>2. <strong>attraction </strong>（[U]）吸引，吸引力；</p>
<p>（[C]）有吸引力的人或事物。 1）The idea of traveling to the moon has
little attraction for me. 到月球上旅行的想法对我没有什么吸引力。</p>
<p>2）The city's bright lights, theatres, and movies are great
attractions.城里明亮的灯、戏院、电影等有巨大的吸引力。 3) One of the
main attractions of the job is the high salary.
这份工作最吸引人的是薪水高。 3. <strong>comfort </strong> [U]
安慰，慰藉，宽恕；</p>
<p>[C]令人感到安慰的人或事物</p>
<p><strong>performance</strong></p>
<p>做表现，表演来说是可数的，但是作为性能来说是不可数的</p>
<p><strong>struture是否可数</strong></p>
<p>1.结构;构造;组织[U][C] We know a lot about the structure of genes
now. 如今我们对基因的结构有了较多的了解。 2.构造体;建筑物[C] We visited
the museum, a steel and glass structure.
我们参观了博物馆,它是一座钢和玻璃的建筑物。</p>
<h3
id="improvement看指的是整体的进步不可数还是因为某一个变动带来的具体的某个进步可数">improvement：看指的是整体的进步（不可数），还是因为某一个变动带来的具体的某个进步（可数）。</h3>
<p>1 [ uncountable ] the act of making sth better; the process of sth
becoming better 2 [ countable ] a change in sth that makes it better;
sth that is better than it was before Oxford Advanced Learner’s
Dictionary, 8th edition</p>
<h3 id="technology-和-improvement-类似">technology 和 improvement
类似</h3>
<h3 id="data-这个单词是-datum-的复数形式">Data 这个单词是 datum
的复数形式，</h3>
<p>在学术写作中通常视其为一个可数名词的复数，因为这种观点承认了 datum
这个单词作为可数名词单数的存在，更加严谨，虽然在学术论文中我们并不用
datum 这个单词。所以 data 后面的 be 动词为 are 或者 were。</p>
<p>当然我们也可以在 data 前面加上量词，如 "a set
of"，让读者更加明确。</p>
<p>例如：</p>
<p>While <strong>data</strong> for the annually averaged solar share are
not available, it is reasonable to anticipate that this could approach
～17% with sufficient storage, which is a worthwhile target (Nathan et
al., 2018).</p>
<h3 id="学术写作中-work-一般为不可数名词使用">学术写作中 work
一般为不可数名词使用，</h3>
<p>但如果 work 表示作品或者具体成果可以在后面加 "s"，例如 published
works，这种表达方式也是少有遇见的。</p>
<p>例如：</p>
<p>Much <strong>work</strong> has been conducted over the past decade on
applying compressive sensing methods to medical X-ray CT as a way of
reducing patient radiation exposure (Jones &amp; Huthwaite, 2015).</p>
<h3 id="result"><strong>Result</strong></h3>
<p>学术写作中 result 一般为可数名词。</p>
<p>例如：</p>
<p>Despite the promising <strong>results</strong> reported from these
and similar studies, the mass production of graphene by CVD has
predominantly focused on the electronic device industry (Papageorgiou et
al., 2017).</p>
<h3 id="information"><strong>Information</strong></h3>
<p>学术写作中 information 一般视为不可数名词。</p>
<p>例如：</p>
<p>If all <strong>information</strong> is known, then any price changes
should be a "random walk," preventing any prediction on future values
(Cooper et al., 2017).</p>
<h3 id="evidence"><strong>Evidence</strong></h3>
<p>学术写作中 evidence 一般视为不可数名词。</p>
<p>例如：</p>
<p>By these means, <strong>evidence</strong> is a weaker definition of
truth than proof（Fossum et al., 2019）</p>
<p>Recent <strong>evidence</strong> indicates that NOXA can also act as
an activator BH3 protein (Bhola &amp; Letai, 2015)</p>
<h3 id="research"><strong>Research</strong></h3>
<p>Research
这个单词既可以做可数名词，也可以做不可数名词，但是做不可数名词的情形更多。</p>
<p>例如：</p>
<p>Much <strong>research</strong> has examined the construction of
nanoelectrodes and nanoelectrode assemblies, and although a number of
ingenious strategies have been devised (Bhon, 2009).</p>
<p>当我们使用 "researches"，通常表示 "different or separate groups of
research"，并且大家需要注意，从语言学的角度来说，"a research"
这种表述方式在语法上是错误的。</p>
<h3 id="development"><strong>Development</strong></h3>
<p>Development 做可数名词和不可数名词的情况都有。</p>
<p>当表示某个领域或者事物的“缓慢发展与进步”，使得在原本基础上更加先进、有竞争力，这种情况下视为不可数名词。</p>
<p>例如：</p>
<p>Ag-based nanowires were particularly effective sensor elements owing
to the <strong>development</strong> of chemically responsive interfacial
boundaries (Bhon, 2009).</p>
<p>如果表示产生了新的发现、成果，制造了新的产品，这种情形视为可数名词。</p>
<p>例如：</p>
<p>Recent <strong>developments</strong> in medical imaging driven by
both increasing computational power and the desire to reduce patient
X-ray exposure have led to the development of a number of limited view
CT methodologies (Jones &amp; Huthwaite, 2015).</p>
<h3 id="environment"><strong>Environment</strong></h3>
<p>Enviroment
在学术论文中一般为可数名词，可以指自然界存在的环境，也可以指影响人或者事物发生发展结果的条件。</p>
<p>例如：</p>
<p>This is the major transport process relevant to many exposure
<strong>environments</strong> and degradation mechanisms and so its
reduction is the key to enhancing durability (Wong et al., 2015).</p>
<h3
id="meet-the-need-of也有meet-the-needs-of应该是看后面指代的是一件事还是多件事-1">meet
the need of，也有meet the needs
of，应该是看后面指代的是一件事还是多件事</h3>
<h3
id="principle-作原则原理行为规范准则来说是可数的作为操守道义为人之道来讲不可数">Principle
作原则、原理，（行为）规范/准则来说是可数的，作为操守，道义，为人之道来讲不可数</h3>
<p>We must come back to first principle<strong>s</strong>.
(基本原则）</p>
<p>I live according to my** principless**. (行为规范）</p>
<p>It is a matter of <strong>principle</strong> with him to tell the
truth. (操守）</p>
<h3 id="water">water</h3>
<p>作为物质名字，水是不可数的</p>
<p>two glasses of water 本身水没有加s，本质上是在数杯子的数量！</p>
<p>但是作为“水域”的意思就是可数的，比如two waters，international
waters国际水域</p>
<h3 id="pleasure">pleasure</h3>
<p>It is a pleasure. 这里的抽象名词
pleasure其实是指（某件）愉快的事情</p>
<h2 id="定语从句中先行词的位置">19. 定语从句中先行词的位置</h2>
<h3
id="定语从句谓语动词要与先行词保存一致">定语从句谓语动词要与先行词保存一致!</h3>
<p>He likes movies that __are__(be) about scary monster</p>
<h3
id="定语从句并不一定总是紧接着先行词的可能被分隔">定语从句并不一定总是紧接着先行词的，可能被分隔</h3>
<p>先行词通常与定语从句是“手拉手”在一起的，但也可能被分隔两处。被分隔的情况有三种，即按照英语的“尾重原则（principle
of end weight），被定语、状语或谓语分隔。</p>
<p>常见的三种不是紧接着的情况见：<a
href="http://www.01ue.com/--tutorial-show-g-1-i-3-po-32.html#">http://www.01ue.com/--tutorial-show-g-1-i-3-po-32.html#</a></p>
<p>The <strong>mineral elements</strong> from the soil <strong>that are
usable by the plant must be dissolved in the soil solution before they
can be taken into the root. </strong>（被定语分隔）</p>
<p>Never leave <strong>that</strong> until tomorrow <strong>which you
can do today</strong>. （被状语分隔）</p>
<p><strong>Social science</strong> is that branch of intellectual
enquiry <strong>which </strong>seeks to study humans and their endeavors
in the same reasoned, orderly, systematic, and dispassioned manner that
natural scientists used for the study of natural phenomena.
（被谓语分隔）</p>
<h2 id="consit-of表示由...组成是动词短语不是介词短语">20. consit
of，表示由...组成，是动词短语，不是介词短语！</h2>
<p>It can not handle person poses which consist of only one
keypoint.</p>
<h2 id="形容词变副词规则尤其是什么时候需要去e加ly">21.
形容词变副词规则，尤其是什么时候需要去e加ly</h2>
<p>1. 在形容词词尾直接加-ly.如:real-really; helpful-helpfully;
careful-carefully; hopeful-hopefully; slow-slowly; quick-quickly;
quiet-quietly 2.
以辅音字母加y结尾的形容词要变y为i,然后再加-ly.如:busy-busily;
angry-angrily; easy-easily 3.
某些以辅音字母加不发音的字母e结尾和以-ue结尾的形容词要先去掉e,然后再加-y或-ly.如:terrible-terribly;
true-truly; gentle-gently. 这里说是某些情况则肯定有特例，如 recursive -
recursively 就么有去e</p>
<h2 id="辨析容易混淆的单词和短语">22. 辨析容易混淆的单词和短语</h2>
<h3 id="each和every">each和every</h3>
<p>https://zhuanlan.zhihu.com/p/143404460</p>
<p><font color="#dd00dd">表示两者之中用each；
表示三者以上用each和every都可用。every重共性，将所表述的对象看作是一个整体（大特征)，each重个性，突出每个人的个性；在表示一个动作发生的频率时，我们一般只使用every而不用each，比如：every
morning、every Saturday。</font></p>
<p>请各位同学判断下面这句话中的every用得对不对呢？</p>
<blockquote>
<p><strong>Every</strong> teacher teaches things differently. （×）</p>
</blockquote>
<p>当然是不对的，因为这句话表达的是“每个老师的教学方法都不相同”，其中更注重的是个体间的不同，所以上句中应该使用each。</p>
<p>我们只能用each来表示“两个事物中的每一个”，如果使用every则会给别人造成此类事物在数量上“很多”的误解。所以我们说：</p>
<blockquote>
<p>She wore an anklet on <strong>each</strong> ankle.
<strong>Each</strong> of my parents gave me a present.</p>
</blockquote>
<p>因为每个人的“脚踝”和“父母”的数量都只有两个，<u>所以我们并不能将上面两句话中的each简单替换为every。</u></p>
<blockquote>
<p>I take the subway to work <strong>every</strong> morning. The bus
runs <strong>every</strong> 5 minutes.</p>
</blockquote>
<p>在上面示例中，表示的是“我每天早上乘地铁上班”和“公交车每5分钟发一趟”，都是表示某个动作或行为的频率，所以使用的是every。</p>
<p>也可以说，在表示“时间”的词的前面我们一般只使用every。</p>
<h3 id="clue和cue的区别">clue和cue的区别</h3>
<p>https://wikidiff.com/clue/cue</p>
<p>两个词都有线索的意思,前着指的是,法律及犯罪的线索,比如在犯罪现场发现的叫线索就叫clue,</p>
<p>而cue则指的是某某人给你的一种暗示(线索)，可能是一种信号或动作，比如给你猜个谜语的时候会给些提示(打个动物啦,或给你打个手势啦)这就用cue</p>
<h3 id="warp和wrap的区别">warp和wrap的区别</h3>
<p>warp:弯曲，使变形； wrap:包装</p>
<h3 id="extent和extend的区别">extent和extend的区别</h3>
<p>extent : 大小，程度，范围； extend: 延长，扩大，延期</p>
<h4 id="widely和wildly注意不要写错">widely和wildly注意不要写错</h4>
<p>wildly: 野生地、疯狂地</p>
<p>widely: 广泛地 this approach is <strong>widely</strong> used.</p>
<h3 id="result-in-和-lead-to-的区别">result in 和 lead to 的区别：</h3>
<p>lead to 表示造成某种情形或结果的逐步变化过程。如： The storm led to
serious floods. 暴雨造成了严重的洪水。 result in
表示意想不到的结果。如： Smoking too much will result in sickness.
吸咽过量会导致疾病。</p>
<h3 id="in-three-steps用介词in而不是at">in three
steps，用介词in而不是at</h3>
<p>We perform the task in three steps.</p>
<h3 id="need-for而不是need-of">need for，而不是need of</h3>
<p>need作名词表示需要常于介词for 搭配, the need for money.</p>
<p>类似的后面用for还有以下</p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111125838.png" /></p>
<h3 id="suffer与suffer-from的区别">suffer与suffer from的区别</h3>
<p>suffer 经受，使遭受（坏事，不愉快的事），其宾语一般是loss(损失),
pain（疼痛）, punishment（惩罚）, defeat（失败）, wrong, hardship,
torture, grief, injustice, disappointment等。</p>
<p>I will not suffer such conduct. 我不能容忍这种行为。</p>
<p>They suffered huge losses in the financial crisis.
他们在经济危机时遭受了巨大损失。</p>
<p>suffer from 因……而痛苦 1. suffer
from+疾病名词（或者他人闲言碎语、劳累、记忆力减退等），表示患病、为……受苦。</p>
<p>She suffers from headache. 她患头痛病。</p>
<p>I'm suffering from a lack of time this
week.我为这周时间不够用而苦。</p>
<p>2. suffer from+自然灾害 suffer from drought 遭受旱灾；suffer from
floods 遭受水灾</p>
<h3 id="vitrual-和-virtually">vitrual 和 virtually</h3>
<p><strong>虚拟的，实质上的，事实上地，几乎</strong>，<strong>而不是虚假的意思</strong>！</p>
<p>Virtual means so nearly that any difference is unimportant 。</p>
<p>virtual "虚拟的" 的解释是特指电脑方面的. 比如说 virtual imagery
(虚拟图像), virtual computing (模拟计算), virtual tour (虚拟游览),
而virtual 的另一个意思是 "基本上", 类似于 almost.</p>
<p><strong>Virtually</strong> all cooking was done over coal-fired
ranges. ** 事实上**所有的烹饪都是在烧煤的炉灶上完成的。</p>
<h3 id="most-和-mostly">most 和 mostly</h3>
<p>A.
<strong>most作副词时，是much的最高级形式，常和多音节的形容词或副词连用，构成最高级，表示“最”</strong>。most还可以修饰动词表示“最”，其位置较为灵活，放动词前后均可。此外，若most修饰可数名词单数且带形容词的短语时，可用“a
most...”结构，表示“非常”。</p>
<p>B. <strong>mostly
主要用来修饰be动词或介词短语等</strong>，<strong>表示“大部分地”、“大多数地”、“主要地”</strong>。</p>
<p>例句：</p>
<p>（1） It's the most important question. 这是最重要的问题。</p>
<p>（2） I like winter most. 我最喜欢冬天。</p>
<p>（3） Mary is a <strong>most </strong>beautiful girl.
玛丽是个<strong>非常</strong>漂亮的女孩。</p>
<p>（4） The guests are mostly her friends. 客人大多数是她的朋友。</p>
<p>（5） I am mostly out on Sunday. 星期日我多半不在家。</p>
<p>（6） He uses his car mostly for going to work.
他的汽车主要是上班用。</p>
<ol start="7" type="1">
<li>I am <strong>most grateful</strong> for your slefless donation.
<strong>十分感谢</strong>.....</li>
</ol>
<h3 id="wishhopemay的用法区别">wish，hope，may的用法区别</h3>
<p><a href="https://wenku.baidu.com/view/e45f79f3b0717fd5360cdcf1.html"
class="uri">https://wenku.baidu.com/view/e45f79f3b0717fd5360cdcf1.html</a></p>
<p>may you + do原型</p>
<p>wish sb + 名词 或者 wish sb to do</p>
<p>hope that接从句（从句用陈述语气），但是用作短语固定形式是 hope sb to
do</p>
<p>We hope to see you again. =We hope we can see you
again.我希望再次见到你</p>
<p>例：Wish you success (名次). May you succeed (动词原型). Hope you
will/can succeed.</p>
<h3 id="the-state-of-the-art-in-和-state-of-the-art">the state of the
art in 和 state-of-the-art</h3>
<p>前者表示“...艺术/技艺的现状", 后者意为“顶级的，尖端的”</p>
<h3 id="up-to-date-和-up-to-date">up-to-date 和 up to date</h3>
<p>其实这是连字符使用的规则，加上连字符构成形容词，后面需要跟被修饰的名词，若只是充当谓语后的成分则不加连字符。</p>
<p>[误] These figures are up-to-date.
理由：「up-to-date」并未在名词之前。 改正法：删除连字符，变成These
figures are up to date.</p>
<h3
id="first-...-second-...third-....-和-firstly-...-secondly-...thirdly-....-and-finally....">First,
... Second, ...Third, .... 和 Firstly, ... Secondly, ...Thirdly, ....
and finally,....</h3>
<p>前者用于列举说明各种并列的事实或者原因；后者表达的是“首先，····；其次，····；再次····；最后···"，<strong>有次序上的区别时应该用后者！</strong></p>
<p>当然，其中secondly可以替换成besides, in addition, what's
more等，finally也可以替换成 at last, last but not least.</p>
<h3
id="demand-of-和-demand-for-前者是动词性的后面接受体名词而后者是名词性的">demand
of 和 demand for, 前者是动词性的，后面接受体名词，而后者是名词性的</h3>
<p><strong>(1)demand of 要求/期待...</strong></p>
<p>The court will be adjourned for an hour according to the
<strong>demand of</strong> the defense.
根据被告及其辩护律师的要求，法庭将休庭一小时。</p>
<p>The factory is in great <strong>demand o</strong>f steel to keep up
production. 那个工厂需要大量钢铁以维持生产。</p>
<p>The main demand of the Indians is for the return of
one-and-a-half-million acres of forest to their communities.</p>
<p>印第安人的主要要求是将150万英亩的森林归还给他们族群。</p>
<p>(2)<strong>demadn for对 ... 的要求； 对 ... 的需求</strong></p>
<p>A fall in <strong>demand for</strong> oil tankers has put jobs in
jeopardy.</p>
<p>油轮需求量的下降使很多工作职位受到威胁。</p>
<p>These developments have created a great demand for home computers.
这些发展促使家用电脑的需求量增大。</p>
<p>Demand for oil has nosedived. 对汽油的需求已骤减。</p>
<h3 id="relieve和alleviate-有什么区别">relieve和alleviate
有什么区别</h3>
<p><strong>1. Alleviate 指短暂的减轻压抑而没有解决其根源</strong>：“No
arguments shall be wanting on my part that can alleviate so severe a
misfortune” “对我来说要减轻如此严重的不幸,不需要任何争论”. <strong>2.
Relieve 指缓解或使造成不快或压抑的某事变得可以忍受</strong>：“that
misery which he strives in vain to relieve” “他徒劳地奋力去减轻苦难”
“The counselor relieved her fears” “顾问消除了她的恐惧”
3.Allay意味着至少是暂时从造成负担或痛苦的事物中解脱出来：“This music
crept by me upon the waters,/Allaying both their fury and my
passion/With its sweet
air”.“在水面音乐爬上我的心头/平息了他们的愤怒和我的热情/用它甜美的声音”.</p>
<h3 id="based-on-与-on-the-basis-of用法不同">based on 与 on the basis
of用法不同</h3>
<p><a
href="https://wallaceediting.cn/blog/progress/grammar-usage/%E5%A6%82%E4%BD%95%E4%BE%9D%E7%85%A7%E6%9C%9F%E5%88%8A%E8%A7%84%E5%AE%9A%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8based-on%E4%B8%8Eon-the-basis-of.html">原文</a></p>
<p>based on不能替换according to，而 on the basis of能替换according
to。</p>
<p><strong>词组 based on 和 on the basis of
经常被替换使用，然而，这是不恰当的</strong>。因为based on
是分词，可被用来修饰名词、代名词和名词词组；而需修饰动词时，只能用介词词组on
the basis of 。Based on
具有动词或形容词的功用，做形容词时，可修饰前置的名词或代名词，例如： ˙
This conclusion is based on four years of experience. (base
on这里当动词用) ˙ Conclusions based on experience may still require
testing. (based on这里修饰最邻近的名词）</p>
<p>若要修饰动词，请使用词组on the basis of。 例句1：病句：Based on the
first four years of results, we discarded the original hypothesis.
（这里逻辑错误，based on修饰主句中的主语we） 修正句：On the basis of our
results, we discarded the original hypothesis.</p>
<p>例句2：病句：The administration sent a document on the basis of your
suggestion. 修正句：The administration sent a document based on your
suggestion.</p>
<p>在例句2的修正句中，based on 被用来修饰前置名词
document，显示文件内容是基于your suggestion。而在原句中，on the basis
of修饰动词send，表示根据your
suggestion，文件已寄出。简单来说，当要表达according
to（根据）的意思时，可以使用on the basis of，而当表达has a foundation
in/has resulted from（基于／由于）的意思时，则应使用 based on。</p>
<h3 id="approach-method-algorithm-way-means的区别">approach, method,
algorithm, way, means的区别</h3>
<p>An approach is a way of dealing with something or somebody. A method
is the process used or the steps taken to deal with an issue or a
person.</p>
<p>approach是学习或研究问题的方法，指解决某个具体问题所需要的各种步骤的统称，它一定是针对某个具体问题的解决方法。method指做某件工作的固定的套路，表示颇为复杂的一套方法，而且强调以效率和准确性为目的，是一种固定而且无需变化的方法。在计算机科学中，method是一类algorithm的总称。</p>
<ul>
<li><p>APPROACH - somebody’s way of solving the issue/problem</p></li>
<li><p>METHOD — a well established procedures of solving
issue/problem</p></li>
<li><p>TECHNIQUE—one of ways of solving issue/problem within the same
method.</p></li>
</ul>
<p>摘录自：https://kknews.cc/education/425g4j3.html</p>
<p>“Way” 泛指
“一个人的行事方法、思维方式，以及事物的外形样式”，<strong>它强调在很多选择、可能性中的一个</strong>。举一个例子，每个人学英语的方式都不大相同，而下面这句话中的说话人认为：
“学习英语最好的方式是和母语人士对话。”</p>
<p>e.g. I think the best way to learn English is through talking to
native speakers.</p>
<p>The way she talks is very polite.
这里Way强调一个人的行为方式。句子的意思是： “她说话的方式很有礼貌。”</p>
<p>“Method” 指
<strong>既定的、系统化的步骤、程序</strong>。比如：研究方法 research
method；cooking method 烹饪步骤；或者教学方法 teaching method 等等。</p>
<p>e.g. According to Confucius, the best teaching method is to make sure
instructions are suitable for the student’s abilities.</p>
<p>e.g. What <strong>method</strong> will you employ? That depends on
your <strong>approach</strong>.</p>
<p>“Means” 和刚刚讲过的两个单词 “way” 和 “method” 不同的是，“means” 强调
“为了达到<strong>特定的目标而使用的工具、方式或者手段</strong>”。</p>
<p>The company has survived the recession by means of layoffs.
这家公司通过裁员成功地躲避了这次经济衰退所带来的影响。</p>
<p>Which is your favourite means of transport? Trains, buses or
aeroplanes? 你最喜欢的交通方式是哪一种</p>
<h3 id="abilitycapability-和-capacity-的区别">ability、capability 和
capacity 的区别</h3>
<p>Ability、capability 和 capacity
就许多用法而言是近义词。这三个词常用于描述一个人完成某个动作的能力。例如，一个人可能拥有一周读两本的
ability、capability 或 capacity。但是
capacity——其与其它动词共有的含义通过比喻得到了进一步扩展——还有其它动词没有的特殊用法。Capacity
往往与体积、容量和数量相关。例如，“The vehicle’s fuel capacity is 120
gallons,”，在这句话中，capacity
指对车辆机油装载能力的一种度量，不能与剩下的两个词互换使用。“The
vehicle’s fuel ability”听起来不正确。</p>
<p>ability 和 capacity
之间的另一个常见不同点在于，对于人类和动物而言，capacities 是天生的，而
abilities 是学到的。例如，一个孩子可能天生就具有成为厨师的
capacity，但是做菜的 ability 必须通过学习获得。</p>
<p>同时，Capability 通常表示 ability 的极限。例如，你说你有写好文章的
ability，我可能会问你有没有在明天之前写出10页文章的
capability。此外，capabilities 往往是一种非此即彼的命题，而ability
往往是程度上的问题。例如，我可能会说，我虽然有写作的
ability，但是我没有写小说的
capability。不过，跟这些词的其它区别一样，这些不同比较模糊，除了一般固定的用法之外，这些词都是可以互换使用的。</p>
<h3
id="ignoreneglect和overlook的区别">ignore,neglect和overlook的区别</h3>
<p><strong>ignore v.忽视，不理睬，指有意识地拒绝。</strong></p>
<p>She saw him coming but she ignored
him.她看见他走过来，但是装作没看见。</p>
<p><strong>neglect
v.忽视，忽略，疏忽，指无意识地忽视或忘记。</strong></p>
<p>He neglected to make repairs in his house.他忘记了修理房子。</p>
<p><strong>overlook
v.忽略，疏漏，指有意识地遗漏，也指无意识地忽略。</strong></p>
<p>The mother overlooked her little boy's bad
behavior.那位母亲忽视了她的小儿子的不良行为</p>
<h3 id="inter和intra的区别">inter和intra的区别</h3>
<h4 id="intra-表示在内内部">intra-表示“在内，内部”</h4>
<p>intraparty党内的抄(intra+party党)
intracollegiate大学内的(intra+collegiate大学的，学院的)
intrapersonal个人内心的(intra+personal个人的)
intranational国内的(intra+national国家的)</p>
<h4 id="inter-表示在之间相互">inter-表示“在…之间，相互”</h4>
<p>international国际的(inter+national国家的)
interpose置于，介入(inter+pose放→放在二者之间)
intersect横断(inter+sect切割 →在中间切→zhidao横断)
intervene干涉(inter+vene走→走在二者之间→干涉)
interaction相互影响(inter+action行动→相互行动→影响)
interchangeable可互换的(inter+changeable可改变的)
interlude(活动间的)休息(inter+lude玩→在中间玩→活动间休息)
interrelate相互关连(inter+relate关连)</p>
<h3
id="axiomprincipletheoremlawrule-在数学物理中的含义如何区分">axiom，principle、theorem、law、rule
在数学、物理中的含义如何区分？</h3>
<p><a
href="https://www.zhihu.com/question/389899464/answer/1176208152">参考1</a></p>
<p><a
href="https://www.zhihu.com/question/20222198/answer/14386797">参考2</a></p>
<p><a
href="https://www.zhihu.com/question/389899464/answer/1174115855">参考3</a></p>
<p>数学中的<strong>公理axiom</strong>，与物理中的<strong>定律
principle</strong>同属客观规律，<strong>但无法用实验证明只能被假定成立</strong>，<strong>只能在一定范围内归纳</strong>。<strong>原理和公理一样，无法被证明，</strong>也因此科学的发展出现大的突破总是以公理/原理被证伪的形式。但注意，<strong>数学上没有Principle，只有Axiom。</strong>数学和物理在最开始的起点处，一个称之为"原理"一个称之为"公理"的东西，本质上似乎是相同的，只不过"原理"往往要比公理更复杂，原理(Principle)更注重推导的结果是否丰富，而公理(Axiom)仅仅为逻辑演绎提供起点。</p>
<p><strong>定理(Theorem)</strong>则是通过逻辑演绎，由原理或者公理推导出来的用途比较广的，重要性很强的推论，一般的推论称为Corollary，其实和定理本身属性相同，只是重要性(应用范围)差很多，一般只用于作为重要定理的证明中间过程存在着。物理里凡是叫做
theorem 的东西，总是跟数学拖不了干系。不来点数学推导，似乎就很难被称作
theorem。</p>
<p><strong>注意：</strong>命名这事儿，还是要结合当时的时代背景，在当时看来无解的principle或者实验规律law，可能经过物理学和数学地进一步的发展，找到更加底层的principle或者law，进而被推演出啦，升级为theorem，<strong>但是很多时候名称依然沿用而被保留了下来。</strong></p>
<p><strong>定律(Law)主要是一开始一些直接从经验中总结出来的东西，一开始的角色其实和原理差不多，只不过形式更为具象，原理更为抽象。同时作为原理一般能够推导出多条定律，比如最小作用量原理，既可以给出牛顿定律，也可以给出相对论下的运动方程，还可以给出麦克斯韦方程组</strong>。相对来说，<strong>如果完全是靠经验总结而非逻辑归纳得到的命题，应该就只能叫做定律了</strong>。</p>
<p><strong>规则(Rule)则是一些用于计算或者简化计算的常规套路</strong>，它们本身并没有多少意义，只是在漫长的理论推导过程中由于经常被用到，<strong>人们更愿意记住它们而不是每次重复之前做过的同样的步骤</strong>，相当于围棋中的"定式"。</p>
<h3 id="deficiency和defect的区别">deficiency和defect的区别</h3>
<p><strong>在表示人的缺点，设计制造方面的“缺点、缺陷”，两者应当没有区别。</strong>
正如，中文的“缺点" 与”缺陷“ 有什么区别呢？ 但 defect = deficiency =
fault = shortcoming 缺点、缺陷。 <strong>如果表示 “缺少、不足、缺乏” =
shortfall/ shortage/ lack 时，只可用 deficiency.</strong></p>
<h3
id="表示连续的单词的区别consistentconsecutivecontinuouscontiguousadjacentseccessive等">表示连续的单词的区别（consistent，consecutive，continuous，contiguous，adjacent，seccessive等）</h3>
<p>参考这里：https://www.xindejob.com/article/read?id=32</p>
<p><strong>consecutive</strong>是指<strong>空间</strong>（位置/排列）上的<strong><em>连续</em></strong>和<strong><em>贯通</em></strong>，它强调的是<strong><em>连</em></strong>，与之相对的是间断、断裂。<strong>one
after another in order</strong>，强调有秩序的一个接着一个。</p>
<p>e.g. Unemployment rates have fallen for the fifth consecutive month.
失业率已经连续五个月不断下降。</p>
<p><strong>continuous</strong>是指<strong>时间/过程上</strong>的<strong><em>继续</em></strong>和<strong><em>承接</em></strong>，强调中间的<strong><em>内容或实体</em></strong>不发生中断或缺失（但是在时间/过程上可以暂时中止/停顿，后续承接着往下发展）</p>
<p>e.g. There has been continuous rainfall in Northern England for 10
days.</p>
<p>注意contiguous是指地域上的相连、相邻、交界，是<strong>毗邻</strong>的意思。比如广西广东在地理上没有任何空隙的相邻。adjacent也是毗邻的，不要求拼凑性地将边界进行贴合，可以有空隙。</p>
<p>其他对比解释请参考链接文章。</p>
<h3 id="rather-than和other-than">rather than和other than</h3>
<p>http://www.kewo.com/article/13557</p>
<p>　rather than
　　①表示“是...而不是...；代替”时，侧重于指客观上的差别。如：</p>
<p>　　I call her hair chestnut rather than brown.</p>
<p>　　我宁愿说她的头发是栗色的，而不愿说她的头发是棕色的。（rather
than前后可接形容词）</p>
<p>　　②表示“与其...倒不如（宁可）...”时，侧重句子主语或说话人主观上的选择。如：</p>
<p>　　The person who should be praised is you rather than me.（rather
than前后可接代词）</p>
<p>　　该受到表扬的人是你，而不是我。</p>
<p>other than</p>
<p>　　①<strong>表示否定意义，即“不同于，非"，相当于different
from</strong>。如：</p>
<p>　　The truth is quite other than what you think.</p>
<p>　　事实真相和你们想的完全不一样。（other than后可接从句）</p>
<p>　　②表示排除意义，<strong>即“除了”，相当于except（常用于否定结构中）</strong>。如：</p>
<p>　　1.There’s nobody here other than me.</p>
<p>　　除了我这里没别人。（other than后可接代词）</p>
<p>　　2.You can’t get there other than by swimming.</p>
<p>　　你只能靠游泳游到那边去。（other than后可接介词短语）</p>
<p>　　③还<strong>有“除...以外，还...”的意思，相当于besides</strong>。如：</p>
<p>　　English is now spoken in many countries other than England.</p>
<p>　　除了英国外，其他许多国家都讲英语。（other than后可接名词）</p>
<p>​ 3.no (加名词) other than 正是，恰恰是</p>
<p>　　He is no other than the professional talent we are looking for.
他正是我们要物色的专业人才。</p>
<h4 id="rest和remaining的区别">rest和remaining的区别</h4>
<p>Rest of the things is used when we do not know the quantity of
things. On the other hand, Remaining is used when the quantity of things
is known to us</p>
<p>Lets assume that in case one I do not know how many pages a book
contain.</p>
<blockquote>
<ol type="1">
<li>I've read twenty pages today, I'll read the <em>rest of the
pages</em>tomorrow.</li>
</ol>
</blockquote>
<p>And in case two we know that a book contains <em>fifty</em>
pages.</p>
<blockquote>
<ol start="2" type="1">
<li>I've read twenty pages today, I'll read the <em>remaining thirty
pages</em> tomorrow.</li>
</ol>
</blockquote>
<p>Above was just an example, you can substitute the <em>pages</em> with
whatever you wish.</p>
<h4 id="continually-和-continuously-的区别">continually 和 continuously
的区别</h4>
<p>continually 强调频繁地，是有间段的继续 continuously
强调连续地，是一直继续地意思.</p>
<h2 id="英文标题大小写">23. 英文标题大小写</h2>
<p><strong>不定冠词a 在标题中是小写</strong></p>
<p>更加具体的标题大小写规则如下： <a
href="https://www.zhihu.com/question/50427700/answer/121428231">知乎回答</a></p>
<p><strong>Capitalize the first and the last words of the title and all
other words (including words following hyphens in compound words) except
articles, coordinating conjunctions (and, or, but, nor, for), short
prepositions, and the <em>to</em>infinitives:</strong> e.g. My First
Visit to the Palace Museum</p>
<p>The People Without a Country</p>
<p>Rules to Abide By (<strong>By是最后一个单词，所以大写</strong>）</p>
<p>Dickens and <em>David Copperfield</em></p>
<p>What Can the Artist Do in the World of Today?</p>
<p>What Reform Means to China</p>
<p>The Myth of <strong>a </strong>"Negro Literature"</p>
<p>The <strong>English-Speaking </strong>People in Quebec
<strong>（符合词连字符之后的单词开头也要大写，除了介词等那些例外）</strong></p>
<p>The Top-down and Bottom-up Approaches (这里的复合连词后是介词吗？）
Excerpt From: <em>A Handbook of Writing </em></p>
<p><strong>"Begin the first word and all other important words in the
title with a capital letter. The articles, prepositions, and
conjunctions are usually unimportant." </strong> Excerpt From: <em>The
Writing of English</em></p>
<h2
id="on-the-contrary-by-contrast-或-in-contrast-towith-on-the-other-hand的区别">24.
on the contrary, by contrast 或 in contrast (to/with), on the other
hand的区别</h2>
<p>在写文章的时候，如果你已经预设了立场，要表达与之前的说法相反或强调之前的说法是错误的，可以用
on the contrary。
若要做客观的观点对比，不加入个人对错想法，只是单纯比较时就用 by contrast
或 in contrast (to/with)。 而 on the other hand
则是说明一件事情的两面，或是不同的看法，虽有部分人士认为此片语不够正式，但在学术语料库出现的频率可是这三个里面最高的。</p>
<h2 id="because-for-as-since表示因为时的区别">25. because ,for ,as
,since表示“因为”时的区别</h2>
<p><a
href="http://www2.chinaedu.com/101resource004/wenjianku/200542/101ktb/ynjd/ynaewe0d/ynaewe0d.htm">摘录自</a></p>
<p>because, as, for,
since这几个词都是表示“原因”的连词，语气由强至弱依次为：because→since→as→for;其中because,
since, as均为从属连词，引导原因状语从句；而for
是并列连词，引导并列句。</p>
<p><strong>1.
because表示直接原因，它所指的原因通常是听话人所不知道的，其语气最强。常用来回答why的提问，一般放于主句之后，也可以单独存在。</strong>例如：</p>
<p>(1)I stayed at home because it rained. 因为下雨我呆在家里。</p>
<p>(2)Because Lingling was ill, she didn”t come to school.
玲玲因病，没有上学。</p>
<p>(3)Why is she absent? 她为什么缺席？</p>
<p>--Because she is sick. 因为她病了。</p>
<p><strong>此外，在强调句型中，只能用because</strong>。例如：</p>
<p>(4)It was because I missed the early bus that I was late for school.
我上学迟到是因为我没有赶上早班汽车。</p>
<p><strong>2.
since侧重主句，从句表示显然的或已为人所知的理由，常译为“因为”、“既然”，语气比because稍弱，通常置于句首，表示一种含有勉强语气的原因</strong>。例如：</p>
<p>(1)Since he asks you, you”ll tell him why.
他既然问你，那就告诉他为什么吧。</p>
<p>(2)Since everyone is here, let”s start.
既然大家都到齐了，我们就出发吧！</p>
<p>(3)Since I understood very little Japanese, I couldn”t follow the
conversation. 我日语懂得不多，因而听不懂对话。</p>
<p><strong>3.
as是常用词，它表示的“原因”是双方已知的事实或显而易见的原因</strong>，或者理由不是很重要，含义与since相同，但语气更弱，没有since正式，<strong>常译为“由于，鉴于”。从句说明原因，主句说明结果，主从并重</strong>。例如：</p>
<p>(1)We all like her as she is kind. 我们都喜欢她，因为她善良。</p>
<p>(2)As I had a cold, I was absent from school.
因为我感冒了，所以没去上课。</p>
<p>(3)As Xiaowang was not ready, we went without him.
由于小王没有准备好，我们只好不带他去了。</p>
<p>4.
for用作连词时，与because相似，但它所表示的原因往往提供上文未交待过的情况。<strong>for不表示直接原因</strong>，表明附加或推断的理由，因此for被看作等立连词，它所引导的分句只能放在句子后部(或单独成为一个句子)，并且前后两个分句间的逻辑关系不一定是因果关系，其间用逗号隔开，且for不可置于句首，for的这一用法常用在书面语中，较正式。例如：</p>
<p>(1)The days are short, for it is now December.
白天短了，因为现在已是十二月份。</p>
<p>(2)It must have rained, for the ground is wet.
(从“地面潮湿”作出“下过雨”的推测，但地湿并不一定是下雨所致,
for不可以换为because。)</p>
<p>(3)The ground is wet because it has rained.
(“下雨”是“地上潮湿”的直接原因。)</p>
<p>前后两个分句间有一定的因果关系时(有时很难区分是直接原因，还是推测性原因)，for与because可以互换使用。例如：</p>
<p>(4)I could not go, for / because I was ill.
我没能去，是因为我病了。</p>
<p>(5)He felt no fear, for / because he was a brave boy.
他没有害怕，因为他是个勇敢的男孩</p>
<h2 id="限定性定语从句和非限定性定语从句的区别-易错">26.0.
限定性定语从句和非限定性定语从句的区别 (** 易错！)</h2>
<p>比较下面的两个句子：</p>
<p>I have a sister who is a doctor.
我有一个医生的姐姐。(<strong>姐姐不止一个，必须用限定性的定语从句锁定范围</strong>)</p>
<p>I have a sister, who is a doctor.
我有一个姐姐，她是当医生的。(<strong>只有一个姐姐，只是补充说明</strong>)</p>
<p><strong>先行词不同</strong></p>
<p>限定性定语从句的先行词只能是名词或代词，而非限定性定语从句的先行词则可以是名词或代词，也可以是短语或句子；另外，<strong><u>当先行词为专有名词或其他具有独一无二性的普通名词时，通常要用非限制性定语从句</u></strong>，而不用限制性定语从句。如：</p>
<p>Peter drove too fast, which was dangerous.
彼得开车很快，这是很危险的。(which指drive too fast)</p>
<p>He changed his mind, which made me very angry.
他改变了主意，这使我很生气。(which指整个主句)</p>
<p>Mr. Smith, who is our boss, will leave for Japan next week.
我们的老板史密斯先生下周要去日本。(先行词为专有名词，要用非限制性定语从句修饰)</p>
<p>Her father, who has a lot of money, wishes her to study abroad.
她父亲很有钱，希望她出国学习。(<strong>先行词为表独一无二意义的普通名词，要用非限制性定语从句修饰</strong>)</p>
<p><strong>关系词不同</strong></p>
<p>关系词that和why可用于限制性定语从句中，通常不用于非限制性定语从句；另外，在限制性定语从句中，关系代词有时可以省略，而在非限制性定语从句中关系词一律不省略。</p>
<p><strong>我们可以通过省略定语从句中的引导词和<em>be</em>动词，来使整个句子更加精简和地道，但这样改写应该就不能算作定语从句了</strong></p>
<p>例如：</p>
<p>I caught the boy <em>who was in front of
me</em>.（我抓住了我前面的那个男孩。）</p>
<p>省略引导词（<em>who</em>）和<em>be</em>动词（<em>was</em>）后，可以得到：I
caught the boy <em>in front of me</em>.
句子的语义与原句相比没有变化，但名词the boy变为由介词短语“<em>in front
of me</em>”修饰，<strong>复杂句变成了简单句</strong>。</p>
<p>再举一个例子：</p>
<p>The store, <em>which was</em> <em>located in the town</em>, sold fish
and meat.（那个商店，位于城镇内，售卖鱼和肉。）</p>
<p>从句省略后可以得到：The store, <em>located in the town</em>, sold
fish and meat.</p>
<h2 id="定语从句中的关系代词">26. 定语从句中的关系代词</h2>
<h3 id="that-的用法以及什么情况下只能用that不能用which">that
的用法，以及什么情况下只能用that不能用which</h3>
<p>https://zhuanlan.zhihu.com/p/30427878</p>
<p>分析这个问题前，我们需要先了解一下<em>that</em>和<em>which</em>之间的不同：</p>
<blockquote>
<p><em>that</em>的词义是<strong>“那个”</strong>，暗指单个对象。</p>
<p><em>which</em>的词义是<strong>“那一个”</strong>，暗指多个对象中的一个。</p>
</blockquote>
<p><u><strong>有一个通用的规则：如果有“独一无二”的意思用that，否则就用which，who等有从多个中选一个的意思。</strong></u></p>
<p>学到这里，你是否能够解释下面两个例子中的引导词为什么只能用that，而不能用which了呢？</p>
<blockquote>
<p>What was the first <strong><em>that</em></strong> came to your
mind？</p>
<p>He is the only person <strong><em>that</em></strong> I have ever
met.</p>
</blockquote>
<p>没错。就是因为“the only person”、“the
first”所表示的事物只有一个或是对这个事物的指向已经非常明显了，并没有<strong>“从多个中选一个”</strong>的暗示在里面，所以引导词只能用<em>that</em>。</p>
<h3
id="另一个详解资源说明定语从句语法">另一个详解资源说明定语从句语法</h3>
<p><a
href="https://blog.csdn.net/iteye_9508/article/details/81796219">摘录自</a></p>
<p><strong>1）不用that的情况</strong> a) 在引导非限定性定语从句时。 (错)
The tree, that is four hundred years old, is very famous here. b)
介词后不能用。 We depend on the land from which we get our food. We
depend on the land that/which we get our food from.</p>
<p><strong>2) 只能用that作为定语从句的关系代词的情况</strong> a) 在there
be 句型中，只用that，不用which。 b) 在不定代词，如：anything, nothing,
the one, all, much, few, any,
little等这些不定代词作先行词时，只用that，不用which。 c) 先行词有the
only, the very修饰时，只用that。 d)
先行词为序数词、数词、形容词最高级时，只用that。. e)
先行词既有人，又有物时。 举例： All that is needed is a supply of oil.
所需的只是供油问题。 Finally, the thief handed everything that he had
stolen to the police. 那贼最终把偷的全部东西交给了警察。</p>
<h3 id="as引导定语从句">as引导定语从句</h3>
<p>as引导定语从句时，既可以引导限定性定语从句，又可以引导非限定性定语从句。区分as引导定语从句和其它从句的关键特征是：as引导定语从句时在从句中做成分，通常做主语或宾语。</p>
<h4 id="as引导限定性定语从句">1、as引导限定性定语从句</h4>
<p>** 如从句所修饰的名词前有such、the
same、as出现，后面的定语从句将由as引导**，形成such...as，the
same...as，as…as这样的固定结构，译为“和……一样”。 例1：I never heard such
stories as he told. 我从未听过他所讲的那样的故事。</p>
<p>例2：He’ll repeat such questions as are discussed in the book.
他将重复书中讨论过的问题。</p>
<p>例3：They made the same mistake as others would have made on such an
occasion. 他们犯了和其他人在这种场合下会犯的同样错误。</p>
<h4 id="as引导非限定性定语从句">2、as引导非限定性定语从句</h4>
<p><strong>as引导非限定性定语从句，往往指代一整句话</strong>，通常表示“正如”的意思。as引导的非限制性定语从句位置相对比较灵活，可以位于先行词之前、之后或中间。
例：As is known to everybody, the moon travels round the earth.
众所周知，月亮绕着地球转。</p>
<h3
id="butasthan作关系代词引导定语从句">but，as，than作关系代词引导定语从句！</h3>
<p>摘录自：</p>
<p><a href="https://www.hjenglish.com/new/p981200/"
class="uri">https://www.hjenglish.com/new/p981200/</a></p>
<p><a href="https://wenku.baidu.com/view/14850f6727d3240c8447eff6.html"
class="uri">https://wenku.baidu.com/view/14850f6727d3240c8447eff6.html</a></p>
<p><strong>As、but和than通常都是以介词、连词等身份被大家所熟知，而它们居然可以摇身一变成为关系代词，在定语从句中做主语和宾语。</strong></p>
<p><strong>but可被看作关系代词，引导定语从句，在句中作主语，在意义上相当于
who not或that
not，即用在否定词或具有否定意义的词后，构成双重否定。</strong></p>
<p>There is <strong>no </strong>mother <strong>but</strong> loves her
own children．（=There is <strong>no</strong> mother** that/who does
not** love her own children．）没有不爱自己孩子的母亲。</p>
<p>He is as brave a man as ever lived.
他是世界上最勇敢的人。(as作主语）</p>
<p>Don’t read such books as are not worth
reading．不要读那些不值得读的书。（as作主语）</p>
<p>Don't read such poems as you can't understnad.
不要看你（们）看不懂的诗词。 （as作宾语）</p>
<p>He is not such a man as he used to be. 他已经不是过去的那个样子了。
（as作表语）</p>
<p><strong>than既可指人，也可指物，可作关系代词来引导定语从句。than前通常有表比较的词。</strong></p>
<p>例如：Fewer friends than we had expected came to our evening
party．来参加晚会的朋友比我们预料的还要少。</p>
<p>He got more money than was wanted．他得到了更多的钱。</p>
<p>This enable the delivery of more information with greater speed to**
more** locations <strong>than</strong> has ever been possible.</p>
<h2 id="同位语和定语从句的区别">27. 同位语和定语从句的区别</h2>
<h3
id="同位语可分为限制性同位语和非限制性同位语-见原文">同位语可分为<strong>限制性同位语</strong>和<strong>非限制性同位语</strong>
见<a href="https://zhuanlan.zhihu.com/p/52542110" target="_blank" rel="noopener">原文</a></h3>
<p><em>Tom's uncle</em> <strong>Jim</strong> is a teacher.
Tom的叔叔Jim是老师**。此处Jim没有用逗号隔开，为uncle的限定性同位语，说明Tom的uncle不只一个。</p>
<p><em>Tom's uncle</em> <strong>，Jim</strong> ，is a teacher.
Tom的叔叔，Jim,是老师。</p>
<p>此处Jim用逗号隔开，为uncle的非限定性同位语，说明Tom的uncle只有一个</p>
<p>同位语不会影响被说明对象的数量</p>
<p><em>The policemen</em> <strong>each</strong> have a gun.
每个警察都有一把枪。</p>
<p><em>The policemen</em> <strong>each</strong> has a gun. (错误）</p>
<p>此外还有其他词类可以充当同位语，比如介词短语作同位语Put it
<em>here</em>, <strong>on the table</strong>.把它放这，这个桌子上。</p>
<p>摘录自：<a
href="https://wenku.baidu.com/view/71317919227916888486d7ed.html"
class="uri">https://wenku.baidu.com/view/71317919227916888486d7ed.html</a></p>
<p>同位语从句和定语从句从表面来看十分相似，但实质上是截然不同的两种从句：</p>
<h3
id="同位语从句所修饰的词是有限的一些抽象名词而定语从句就没有这种限制">（1）同位语从句所修饰的词是有限的一些抽象名词，而定语从句就没有这种限制。</h3>
<p>同位语（即
<strong>句子</strong>作为同位语）从句经常用于下列有限的几个词后：
hope（希望），idea（想法）， news（消息）， order（命令），
fact（事实），
question（问题），reason（理由），belief（相信），doubt（怀疑），evidence（根据），conclusion（结论），truth（真理），result（结果）等。</p>
<p>例如：The <em>idea</em> <strong>that I can be a millionaire</strong>
is impossible. 我会成为百万富翁这个想法是不可能的。</p>
<h3 id="句法功能不同">（2）句法功能不同。</h3>
<p>同位语从句）同它所修饰的名词在内容上是等同关系，在句中的语法作用处于同等地位。而定语从句说明前一名词的性质、特征，对先行词进行修饰、限制，是先行词不可缺少的定语。</p>
<h3
id="引导词that在同位语从句中不作句子成分而在定语从句中必作句子成分是从句所修饰的词的替代词"><strong>（3）引导词that在同位语从句中不作句子成分；而在定语从句中必作句子成分，是从句所修饰的词的替代词。</strong></h3>
<p>试比较： The fact that the Chinese people invented the compass is
known to all．中国人发明指南针这个事实是众所周知的。（同位语从句） The
fact that we talked about is very
important．我们谈论的这个事实很重要。（定语从句）</p>
<h3 id="如何区别同位语从句和定语从句">如何区别同位语从句和定语从句</h3>
<p>同位语从句和定语从句在形式上基本相同，都是跟在名词或代词之后，且又常由that引导。但它们的句法功能却是不同的，我们可以从三个方面来加以区别，<a
href="https://www.hjenglish.com/new/p981200/">具体说明见上述链接文章</a></p>
<h2 id="一些长句中短语或者词组的语序问题">28.
一些长句中短语或者词组的语序问题</h2>
<h3 id="whether-or-not">（1）whether or not</h3>
<p>例句：we just wish to determine <strong>whether or not</strong> the
words you hear are understandable.** **</p>
<p><strong>whether后所接的从句末尾可以加or not,
也可以不加</strong>。</p>
<p>We use the world, church to refer to all religious insititutions,
whetehr they are Christrian, Islanminc, Buddhis, and so on.</p>
<p>whether</p>
<p>conj (连词）. 是否；不论</p>
<p>pron (代词）. 两个中的哪一个</p>
<h3
id="按照英语的尾重原则principle-of-end-weight结构复杂的从句或成分置于其他成分后面">（2）
按照英语的“尾重原则（principle of end
weight）”，结构复杂的从句或成分置于其他成分后面</h3>
<p>This is an abstract concept whcih** makes** (谓语) ** possible
(<strong>宾补</strong>)** <strong>immense amounts of concrete and
research and understanding</strong>
(宾语太长了放后面）<strong>.</strong></p>
<p><strong>Thos</strong> force to exercise their smiling muscles reacted
more enthusiastically to funny cartoons <strong>than did those</strong>
whose mouth are contracted in a frown, suggesting that ....
（<u>than从句中的主语太长，做了倒装</u>，did代替主句中的整个谓语，并且对比的主语没有歧义，所以此处也可以省略did)</p>
<h2 id="名词修饰名词名词作定语单复数形式">29.
名词修饰名词（名词作定语）单复数形式</h2>
<p><strong>若后面被修饰的名词使用了复数，则注意前面的修饰名词一般用单数，特殊情况用复数！！</strong></p>
<p>如：This kindergarten only employs <strong>women (woman的复数)
teachers</strong>.</p>
<p>转载自：<a
href="http://ask.yygrammar.com/app_q-25901.html">http://ask.yygrammar.com/app_q-25901.html</a></p>
<h3
id="名词作定语时一般用单数形式但在个别情况下也有用复数的"><strong>名词作定语时，一般用单数形式，但在个别情况下也有用复数的。</strong></h3>
<p>goods train货车，sports meeting运动会，machines hall展览机器, games
console游戏机, doctors certificate of advice 医生证明， operations
manager 运营经理</p>
<p>不过，有少数的复合名词，前一个词单、复数的形式都有，而各有不同的指涉，比方说：number
game 是给小朋友玩的"数字游戏"，而numbers game
指的是"结果全靠机率的一件事"、time table 是"时间表"，而times table
是"九九乘法表"（这裡“times”其实是介词：“four times three is
twelve”）、art education 是"美术教育"，arts education
是"（各种）艺术教育"。不确定时就用单数，通常不会错。</p>
<p><strong>注意：被修饰的名词变复数时，一般情况下，作定语用的名词不需要变为复数形式，</strong></p>
<h3
id="但由man或woman作定语修饰的名词变成复数时两部分皆要变为复数形式"><strong>但由man或woman作定语修饰的名词变成复数时，两部分皆要变为复数形式。</strong></h3>
<p>man doctor—men doctors 男医生 woman singer—women singers 女歌手</p>
<p>有的作定语用的名词有与之相应的同根形容词。</p>
<p>一般情况下，名词作定语侧重说明被修饰的名词的内容或性质（内在）；</p>
<h3
id="同根形容词作定语则常常描写被修饰的名词的特征"><strong>同根形容词作定语则常常描写被修饰的名词的特征。</strong></h3>
<p>"gold watch"指手表含有金的性质；</p>
<p>而"golden watch"则表示手表是金色的特征，不一定含有金。</p>
<p>stone house 石头造的房子 stony heart 铁石般的心肠</p>
<p>peace conference 和平会议 peaceful construction 和平建设</p>
<h3
id="名词作定语与名词所有格作定语有时是有区别的"><strong>名词作定语与<u>名词所有格</u>作定语有时是有区别的</strong></h3>
<p><font color="#dddd00">一般来说，名词作定语通常说明被修饰的词的性质，而名词所有格作定语则强调对被修饰的词的所有（权）关系或表示逻辑上的谓语关系。</font></p>
<p>在“the Party members（党员）”中，名词定语表示members的性质；</p>
<p>在“the Party's
calls（党的号召）”中，Party具有动作发出者的作用，calls虽然是
名词，却具有动作的含义 .</p>
<p>a student teacher 实习教师</p>
<p>a student's teacher 一位学生的老师</p>
<h3
id="名词所有格表示共有的或者各自的所属用法不一样">名词所有格表示共有的或者各自的所属用法不一样</h3>
<p>Tom and Jerry's room.
Tom和Jerry<strong>共有的</strong>房间。如果是共有的，只需要在最后一个人后面加所有格's</p>
<p>Tom's and Jerry's rooms.
Tom和Jerry各自的房间。不是共有的就需要每个人后面加所有格's</p>
<h3
id="也可以使用of构成名词所有格只不过这时候其实更是在表示修饰性质而不是所属">也可以使用of构成名词所有格，只不过这时候其实更是在表示修饰、性质，而不是所属</h3>
<p>摘录自 英语兔🐰</p>
<p>the paper's qualit &lt;----&gt; the quality of the paper</p>
<p>today's news &lt;----&gt; the news of today</p>
<p>a meter's length &lt;----&gt; the length of a meter</p>
<p>比较下面两个句子含义的区别：a photo of Michael's
（Michael所拥有的照片中的一张）；a photo of Michael
(一张Michaeal出镜在其中的照片)</p>
<h2 id="倒装">30. 倒装</h2>
<p>摘录自：<a
href="https://wenku.baidu.com/view/fd9ac363a88271fe910ef12d2af90242a895abcd.html"
class="uri">https://wenku.baidu.com/view/fd9ac363a88271fe910ef12d2af90242a895abcd.html</a></p>
<p><strong>特别注意部分倒装，是前一个从句主谓倒装还是后一个从句主谓倒装。</strong></p>
<p>部分倒装：仅仅把助动词或情态动词提前；如 Hardly did I know that.
助动词didi提前，而谓语动词know不变位置。</p>
<p>完全倒装：把谓语动词全部提前。如 Out rushed his mother. 直接谓语动词
rushed 提前</p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111125013.png" /></p>
<p><a
href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171303.png">备份图片地址</a></p>
<p>在这里补充知识点</p>
<p><strong>介词短语是不能做主语的。有时介词短语在前看似做主语，其实可能是完全倒装。</strong></p>
<p>例如：</p>
<p>In the next photo are my grandparents.</p>
<p>这个句子是完全倒装。完全倒装就是句子的主语和谓语的位置互换。当表示地点的副词或介词短语置于句首，而且句子的主语为名词时，句子用完全倒装。
你的这句话原本是My grandparents are in the next photo.而in the next
photo置前，且主语my grandparents 是名词，用了完全倒装。 如the boy stood
under the tree变成 under the tree stood the boy. 再如The man went
out.相当于Out went the man. out 是表地点的副词。 但是如果是 He went out
只能变成 out he went.
<strong>因为句子主语是代词，就没有倒装</strong>。</p>
<p><strong>虚拟条件句的省略倒装 </strong> 虚拟条件句的从句部分含有were，
should， 或had时， <strong>可省略if，再把were， should或had
移到从句的句首，实行倒装</strong>。</p>
<p>例如：</p>
<p>Were they here now， they could help us. =If they were here now，
they could help us. 他们现在在的话，就会帮助我们了。</p>
<p>Had you come earlier， you would have met him. =If you had come
earlier， you would have met him. 你来得早一点，就碰到他了。</p>
<p>Should it rain， the crops would be saved. = Were it to rain，the
crops would be saved.假如下雨，庄稼就有救了。
注意：在虚拟语气的从句中，动词'be'的过去时态一律用"were"，不用was，</p>
<h2 id="分词作状语">31. 分词作状语</h2>
<p>摘录自：<a
href="https://wenku.baidu.com/view/1e88a91052d380eb62946de0.html"
class="uri">https://wenku.baidu.com/view/1e88a91052d380eb62946de0.html</a></p>
<p>现在分词与过去分词作状语的区别。
现在分词做状语与过去分词做状语的最主要区别在于两者与所修饰的主语的主动与被动关系的区别</p>
<p>分词作状语（关键找逻辑主语）</p>
<h3
id="a放在句首的分词往往看作-时间状语-以及-原因状语"><strong>a)放在句首的分词往往看作
时间状语 以及 原因状语</strong></h3>
<p>1. Looking (when I looked) at the picture, I couldn't help missing my
middle school days. （时间状语）</p>
<p>2.Seriously injured, Allen was rushed to the hospital. （原因状语）
=As he was seriously injured, Allen was rushed to the hospital.</p>
<p>The wolf (<strong>独立主格</strong>，是invite发出者，所以用主动ing)
inviting him, the rabbit decided to go to the party.</p>
<p>With the work finished, the rabbit went home.</p>
<h3
id="b放在句中或句末常常看作为伴随状态并列句"><strong>b)放在句中或句末常常看作为伴随状态(并列句)
</strong></h3>
<p>The girl was left alone in the room，weeping（crying ）bitterly.</p>
<p>she watched all the gifts , greatly amazed.(=she watched all the
gifts, and was greatly amazed.)</p>
<p><strong>伴随状语出现的条件是由一个主语发出两个动作或同一个生语处于两种状态，或同一个主语发出一个动作时又伴随有某一种状态。伴随状语的逻辑主语一般情况下必须是全句的生语，伴随状语与谓语动词所表示的动作或状态是同时发生的。一般而已，伴随状语中的动词具有“延续性”。而结果状语中的动词一般是非延续性的。</strong>
The dog entered the room, following his master(这条狗跟着主人进了屋)。
The master entered the room,followed by his
dog(主人进了屋，后面跟着他的狗)。视为“伴随状语”或“方式状语”都可以。</p>
<h3
id="c-但注意特殊generallyfrankly-speaking...-taken-as-a-whole总的来讲不考虑逻辑主语看作为独立成分"><strong>c)
但注意特殊：Generally/frankly speaking... / taken as a
whole(总的来讲）不考虑逻辑主语，看作为独立成分。</strong></h3>
<h3 id="d-结果状语">d) 结果状语</h3>
<p>以下三句中的分词短语应看作“结果状语”，而不是“伴随状语”： The fire
lasted nearly two days, leaving nothing valuable.
大火持续了将近两天，几乎没剩下什么值钱的东西。 My grandpa fell off the
bike, breaking his right arm and leg.
我爷爷从自行车上掉下来，摔断了右胳膊和右腿。 It has rained for over ten
days, causing the river to rise. 下了十多天雨，致使河水上涨。</p>
<h2 id="情态动词">31. 情态动词</h2>
<h3 id="shall-和-will-的区别-考研英语必背500句p170第150句">shall 和 will
的区别 《考研英语必背500句》P170，第150句</h3>
<p><a href="https://wenku.baidu.com/view/558d13b7551810a6f52486e5.html"
class="uri">https://wenku.baidu.com/view/558d13b7551810a6f52486e5.html</a></p>
<h3 id="will-和-would-的区别">will 和 would 的区别</h3>
<p><a href="https://zhidao.baidu.com/question/747096438925750852.html"
class="uri">https://zhidao.baidu.com/question/747096438925750852.html</a></p>
<h2 id="however-nevertheless-how-ever-的区别">32. however, nevertheless,
how ever 的区别</h2>
<h3 id="however-和-nevertheless">however 和 nevertheless</h3>
<p><a href="http://m.gaosan.com/gaokao/253523.html"
class="uri">http://m.gaosan.com/gaokao/253523.html</a></p>
<p>however 作连接副词用时, 是 然而 的意思 承上启下 要用逗号隔开
nevertheless 作连接副词用时, 是“然而”的意思 与however 用法一样 如 there
was no news, nevertheless, she went on hoping .消息杳然 尽管如此
她继续盼望着. 区别在于 however 所指关系比较 松弛 ,
nevertheless所指关系比较 紧凑,
<strong>nevertheless指尽管作出完全让步，也不会发生任何影响。</strong></p>
<p>She was very tired, nevertheless she kept on working.
她虽然很疲倦，可仍在继续工作。</p>
<p>He's stupid, but I like him nevertheless.
他是很笨，然而我喜欢他。</p>
<p><strong>更简单的处理方式是：You can always replace "nevertheless"
with "however," but you cannot always replace "however" with
"nevertheless." however的使用场景比nevertheless更多。</strong></p>
<p>详情见，很详细：<a
href="https://wenku.baidu.com/view/155a13287375a417866f8f77.html"
class="uri">https://wenku.baidu.com/view/155a13287375a417866f8f77.html</a></p>
<h3 id="however-和-how-ever">however 和 how ever</h3>
<p>however是副词或者是连接副词，意为“不论如何，但是”，<strong>而how
ever中的ever是起强调作用，意为“究竟怎样”</strong></p>
<p>How ever did you manage to get the car started?
你究竟如何把车发动起来的？</p>
<h2 id="there-be-n-doingto-dodone-句型的辨析">33. There be + n +
doing/to do/done 句型的辨析</h2>
<p>there be +名词+ doing 或 there be + 名词+ done 及 there be +名词+ to
do 这三个句型都是there be 句型，there be
当“有”讲，确是倒装句。句中的名词是主语，所以谓语动词be与其后的名词在人称和数上保持一致（即英语的主谓一致原则），<strong>而
doing，done 还有to do 在there be
句型中都是定语，修饰其前面的名词！</strong> 所以呀，there be
后不可能直接加doing ,
只能是加了名词后，才可能根据语义需要加出doing,done或者to do 结构。</p>
<p>1、解释：There be + 主语 + to do …通常表示动作尚未发生…如：</p>
<p>There are a lot of flowers to be watered.
那里还有很多花没被浇水。</p>
<p>There was nobody to look after the child. 没有人照顾这孩子。</p>
<p>There was a large crowd to send him off. 有一大群人要来给他送行。</p>
<p>注：当其中的宾语与其后的不定式为被动关系时，可用主动表被动，也可用被动式：</p>
<p>如：There is much work to do. = There is much work to be done.
有许多工作要做。</p>
<p><strong>There is much homework to do</strong>.= There is much
homework to be done.有许多作业要做。</p>
<p>2、There be + 主语 + doing …通常表示动作正在发生：</p>
<p>There are many boys playing basketball.
那里有许多男孩正在打篮球。</p>
<p><strong>There is a boy standing under the
tree</strong>.大树下有个男孩。</p>
<h2 id="一些可以提升逼格的高级词汇">34. 一些可以提升逼格的高级词汇</h2>
<h3 id="said-adj上述的用来替换mentioned-above">said:
adj，上述的，用来替换mentioned above</h3>
<p>e.g. Clicking on the "LaTeXiT" button on the bottom right will
produce <strong>said </strong>formula in the viewer</p>
<h3 id="concretely-adv具体地用来替换频繁使用to-be-specific">concretely:
adv，具体地，用来替换频繁使用to be specific</h3>
<p>e.g. (More) Concretely, it also means that women are more likely to
have</p>
<h3 id="off-the-shelf-adjadv现成的无需作重大修改地">off-the-shelf:
adj、adv，现成的，无需作重大修改地</h3>
<p>e.g. It's pretty clear that buying an off-the-shelf device is a lot
cheaper than building a custom box.</p>
<p>很显然，买现成的设备要比构建定制机器便宜得多</p>
<h3 id="cope-with-应对处理">cope with 应对、处理</h3>
<p>RLOF develops an adaptive window size strategy to cope with the
generalized aperture problem.</p>
<h3 id="deficiency-缺乏缺陷">deficiency 缺乏、缺陷</h3>
<p>most of these still have various deficiencies <strong>as regards
（至于）</strong> their elicitation paradigms.</p>
<h3 id="thus-far-到目前为止-heretofore-迄今为止-到目前为止">thus far
到目前为止 ； heretofore 迄今为止, 到目前为止</h3>
<p>Thus far there has been no comprehensive survey.</p>
<h3 id="akin-to-类似于">Akin to 类似于</h3>
<p>Akin to macro-expressions, micro-expressions also constitute a kind
of private facial information.</p>
<h3 id="to-name-only-a-few-举几个例子">to name only a few
举几个例子</h3>
<p>Conflict Theory, Independence Theory, Unity Theory and Dialogue
Theory, <strong>to name only a few</strong>, but they all have this or
that <strong>deficiency or defect
（不足或缺陷，参考deficiency和defect的区别 ）</strong>.</p>
<h3 id="intertwine-vt-vi-缠绕缠绕交织在一起">intertwine vt, vi
缠绕、缠绕、交织在一起</h3>
<p>Facial expressions in the real world <strong>are intertwined
with</strong> various factors, such as subject identity and AUs.</p>
<h3 id="feasible-adj.-可行的">feasible: adj. 可行的</h3>
<h3 id="there-exists-存在有可用来替换there-be句型">there exist(s)
存在，有。可用来替换there be句型</h3>
<p>There <strong>exist</strong> some new problems such as being
dishonest.</p>
<p>There exist 还是There
exists，取决于后面主语的单复数。这实际是个完全倒装语序，exist是谓语动词，后面的名词是主语。There
起引导作用，本身无意义。</p>
<p>其实，英语中这种用法没有什么奇怪的。不单是exist，其他一些静态动词，如stand,
sit, lie, live 都可以这样用。例如：There lives a big family in a
village.</p>
<h3 id="as-regards-关于">as regards 关于</h3>
<p>as regards,in regards to 和with regards to都是 有关于 的意思. As
regards sb. /sth “关于或(至于)某人/某事” 这个的 regards 作为动词存在. I
have information as regards his past。</p>
<p><strong>传统语法把这里的as视为跟which一样指代整个句子的关系代词</strong>,所以把as换成which也说得通.语法书上一般说as从句要放在句首.现代的看法是as引导的是个省略了主语的比较句.。</p>
<p>as的用法复杂主要在于其词性和词义多且缺乏对应关系,但是通过仔细分析归纳不难发现:
表示“当…时/随着”、“因为”或“尽管”这三个词义的as必然是连词。</p>
<h3 id="de-facto-事实上的拉丁语感觉可以做ajdadv">de facto
事实上的，拉丁语。感觉可以做ajd、adv</h3>
<p>de facto standard 事实标准 ; 约定俗成的标准 ; 标准</p>
<p>The general took de facto control of the country.
这位将军实际上控制了整个国家。</p>
<h3 id="stand-for-代表">stand for 代表</h3>
<p>可以用这个词代替represent， is short for</p>
<p>e.g. Superscripts E, M, H of AP stand for easy, medium and hard.</p>
<h3 id="an-array-of-一批大量">an array of 一批，大量</h3>
<p>e.g. Addressing the intricacies of this question opens up an array of
possibilities for high-fidelity character animation and mo- tion
synthesis</p>
<h4 id="assert-宣称断言维护坚持">assert 宣称、断言、维护、坚持</h4>
<p>e.g. A number of contributions are asserted in the Introduction
Section.</p>
<h2 id="连字符的使用复合词">35. 连字符的使用，复合词</h2>
<p>https://cn.chem-station.com/化学杂记/化学与英语/2017/03/hyphen（-）连字符的使用方法.html</p>
<p>英语中带「-」的复合词该怎么理解？有什么规律吗？ - JenniferCook的回答
- 知乎 https://www.zhihu.com/question/38688753/answer/816294490</p>
<p>总的来说，复合词组合形式有很多种，甚至不加连字符的只是分开的多个单词的情况，没有特别靠谱的统一的规则，还是多查询词典。同时连字符在<strong>避免复合词歧义</strong>以及<strong>连接前缀和词干</strong>（通常是因为前缀以元音结尾，而词干以元音开头。例如
co-editor, pre-eminent）都有重要的作用。</p>
<h2 id="due-to-because-of与owing-to的用法区别">36. due to, because
of与owing to的用法区别</h2>
<p>转载自：http://www.yywords.com/Article/201007/1602.html</p>
<p>三者均可表示“由于”、“因为”，<strong>按照传统语法：due to
主要引导表语，而 because of, owing to
两者都主要引导状语</strong>。如：</p>
<p>His illness is due to bad food. 他生病是由于吃了不好的食物。</p>
<p>The accident was due to careless driving.
这次车祸起因于驾驶疏忽。</p>
<p>He can’t come <a href="http://www.yywords.com/" target="_blank" rel="noopener">because of</a> the
bad weather. 他不能来是因为天气不好。</p>
<p>Owing to the rain, the match was cancelled. 比赛因雨被取消了。</p>
<p><strong>但是在现代英语中，due to 也可用来引导状语，而 owing to
也可以用来引导表语</strong>。如：</p>
<p>由于交通拥挤他迟到了。</p>
<p>正：He was late due to [owing to, because of] the very heavy
traffic.</p>
<p>正：Due to [Owing to, Because of] the very heavy traffic, he was
late.</p>
<p>because of 通常只用来引导状语，若引导表语，主语通常应为代词。如：</p>
<p>It is all because of what you said. 那完全是因为你说的话。</p>
<h2 id="英语中数学单位的单复数形式">英语中数学单位的单复数形式</h2>
<p>作者：Vicky
链接：https://www.zhihu.com/question/393503086/answer/1217842790</p>
<p>除了1之外的所有数值，其后的度量单位都按照语法规律变复数，分数和后面直接增加了名词的情况下除外，不多说理论，举几个例子吧，更容易一些：</p>
<ul>
<li>3.5 kilograms</li>
<li>0.2 kilograms（ 小数同样用复数）</li>
<li>0 degrees (celsius)</li>
<li>-1 degrees (celsius)</li>
</ul>
<p>2 分数略有不同</p>
<ul>
<li><strong>1/2 metre 虽然不是1，但可以理解成 省略了of a， half (of a)
metre，数值上与其同等的0.5，却写成0.5 metres</strong></li>
<li><strong>1½ metres 一又二分之一大于1，所以复数</strong></li>
</ul>
<p>3
如果其后直接加了名词则用单数，例句摘自剑桥英语字典。感觉下面是因为变成了复合形容词</p>
<ul>
<li><em>a 15-metre <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/yacht">yacht</a></em>
（非正式语境中可以省略hyphen -）</li>
<li><em>The women's 200 metre <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/event">event</a>
will be <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/follow">followed</a>
by the men's 100 metres.</em></li>
</ul>
<p>4
如果不是直接加名词，则还是遵守第一条和第二条规律，例句摘自剑桥英语字典</p>
<ul>
<li><em>The <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/boat">boat</a>
is ten metres in <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/length">length</a>.</em>
(中间隔了介词)</li>
<li><em>I <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/bought">bought</a>
three kilograms of <a
href="https://link.zhihu.com/?target=https%3A//dictionary.cambridge.org/dictionary/english/plum">plums</a>.</em>
(中间隔了介词)</li>
</ul>
<p>5 单位符号（unit symbol）永远不变复数。比如kg，m，cm</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>易错点</tag>
      </tags>
  </entry>
  <entry>
    <title>Apex相关（同时涉及egg-info的说明）</title>
    <url>/2020/02/12/Apex%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<h2
id="what-is-the-difference-between-fusedadam-optimizer-in-nvidia-amp-package-with-the-adam-optimizer-in-pytorch">What
is the difference between FusedAdam optimizer in Nvidia AMP package with
the Adam optimizer in Pytorch?</h2>
<p><a
href="https://discuss.pytorch.org/t/fusedadam-optimizer-in-nvidia-amp-package/47544">摘录自</a></p>
<p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries
out optimizer.step() by looping over parameters, and launching a series
of kernels for each parameter. This can require hundreds of small
launches that are mostly bound by CPU-side Python looping and kernel
launch overhead, resulting in poor device utilization. Currently, the
FusedAdam implementation in Apex flattens the parameters for the
optimization step, then carries out the optimization step itself via a
fused kernel that combines all the Adam operations. In this way, the
loop over parameters as well as the internal series of Adam operations
for each parameter are fused such that optimizer.step() requires only a
few kernel launches.</p>
<p>The current implementation (in Apex master) is brittle and only works
with Amp opt_level O2. I’ve got a WIP branch to make it work for any
opt_level (<a href="https://github.com/NVIDIA/apex/pull/351"
class="uri">https://github.com/NVIDIA/apex/pull/351</a>). I recommend
waiting until this is merged then trying it.</p>
<h2 id="how-to-use-tensor-cores">How to use Tensor Cores</h2>
<p><a href="https://github.com/NVIDIA/apex/issues/221" target="_blank" rel="noopener">摘录自</a></p>
<p><strong>Convolutions:</strong> For cudnn versions 7.2 and ealier,
<span class="citation" data-cites="vaibhav0195">@vaibhav0195</span> is
correct: input channels, output channels, and batch size should be
multiples of 8 to use tensor cores. However, this requirement is lifted
for cudnn versions 7.3 and later. For cudnn 7.3 and later, you don't
need to worry about making your channels/batch size multiples of 8 to
enable Tensor Core use.</p>
<p><strong>GEMMs (fully connected layers):</strong> For matrix A x
matrix B, where A has size [I, J] and B has size [J, K], I, J, and K
must be multiples of 8 to use Tensor Cores. This requirement exists for
all cublas and cudnn versions. This means that for bare fully connected
layers, the batch size, input features, and output features must be
multiples of 8, and for RNNs, you usually (but not always, it can be
architecture-dependent depending on what you use for encoder/decoder)
need to have batch size, hidden size, embedding size, and dictionary
size as multiples of 8.</p>
<p><strong>It may also help to set
torch.backends.cudnn.benchmark=True</strong> at the top of your script,
which enables pytorch‘s autotuner. Each time pytorch encounters a new
set of convolution parameters, it will test all available cudnn
algorithms to find the fastest one, then cache that choice to reuse
whenever it encounters the same set of convolution parameters again. The
first iteration of your network will be slower as pytorch tests all the
cudnn algorithms for each convolution, but the second iteration and
later iterations will likely be faster.</p>
<h2 id="fp16半精度带来的精度误差">FP16半精度带来的精度误差</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111211505.jpeg"
alt="在这里插入图片描述" />
<figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<h2 id="install-nvidia-apex">Install Nvidia Apex</h2>
<p>若第一次安装需要把项目从github克隆到本地</p>
<h3 id="clean-the-old-install-before-rebuilding">Clean the old install
before rebuilding:</h3>
<blockquote>
<p>pip uninstall apex cd apex_repo_dir rm -rf build (if present) rm -rf
apex.egg-info (if present)</p>
</blockquote>
<h3 id="install-package">Install package：</h3>
<p>注：<strong>--no-cache-dir功能:
不使用缓存在pip目录下的cache中的文件</strong></p>
<blockquote>
<p>pip install -v --no-cache-dir --global-option="--cpp_ext"
--global-option="--cuda_ext" ./ 或者 python setup.py install --cuda_ext
--cpp_ext</p>
</blockquote>
<h3
id="扩展1.-pip-install---editable-.-vs-python-setup.py-develop">扩展1.
<code>pip install --editable .</code> vs
<code>python setup.py develop</code></h3>
<p><a
href="https://stackoverflow.com/questions/30306099/pip-install-editable-vs-python-setup-py-develop">转载自</a>
Try to avoid calling setup.py directly, it will not properly tell pip
that you've installed your package.</p>
<p><strong>With pip install -e</strong>:</p>
<p>其中 -e 选项全称是--editable. For local projects, the
“SomeProject.egg-info” directory is created relative to the project path
(<strong>相对于此项目目录的路径</strong>). This is one advantage over
just using setup.py develop, which creates the “egg-info” directly
relative the current working directory
(<strong>相对于当前工作环境目录的路径</strong>).</p>
<h3 id="扩展2.-弄懂一个命令-pip3-install---editable-.traintest">扩展2.
弄懂一个命令 <code>pip3 install --editable '.[train,test]'</code></h3>
<p><a
href="https://github.com/vita-epfl/openpifpaf/blob/21baabf9c6bbd0bea3e8e465a726abfa8dbeeccf/setup.py#L76">例子在这里</a>
When you then did <code>pip install --editable .</code>, the command
installs the Python package in the current directory (signified by the
dot .) with the optional dependencies needed for training and testing
('[train,test]').
上面的安装命令中，-e选项全称是--editable，也就是可编辑的意思，以可继续开发的模式进行安装，<font color="#dd00dd">
'.' 表示当前目录，也就是setup.py存在的 那个目录，此时pip
install将会把包安装在当前文件目录下，而不是安装到所使用的python环境中的-site-packages。</font>
[train,test]
只是我们举的一个例字，是可选参数，在setup.py中可以找到这两个选项（也可能叫其他名字或者根本就没有）之下包含了哪些第三方包。</p>
<h3 id="扩展3.-关于egg-info">扩展3. 关于egg-info</h3>
<p>注意⚠️：选则本地安装<code>pip install .</code>成功安装完成后，apex.egg-info文件夹可以只处于当前项目文件夹下而不是安装在系统环境中，只需要在当前使用的python虚拟环境-site-packages中一个指向该egg-info文件的超链接即可(这个是在本地安装自动的行为，不需要我们关心操作)，这样就能找到使用Apex包时所需的apex.egg-info文件夹里的信息。</p>
<h3
id="ps-如果遇到cuda版本不兼容的问题解决办法见升级pytoch-1.3后-cuda10.1不匹配版本的警告已经消失">PS:
如果遇到Cuda版本不兼容的问题，解决办法见（升级pytoch 1.3后
cuda10.1不匹配版本的警告已经消失）：</h3>
<p><a
href="https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952">https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952</a></p>
<p>如果没有出现其他error，可以直接使用上面链接的建议，删除版本检查抛出的报错。见以下讨论：</p>
<p><a href="https://github.com/NVIDIA/apex/issues/350"
class="uri">https://github.com/NVIDIA/apex/issues/350</a></p>
<p><a href="https://github.com/NVIDIA/apex/pull/323"
class="uri">https://github.com/NVIDIA/apex/pull/323</a></p>
<h3 id="apex的使用">Apex的使用</h3>
<h4 id="命令行启动训练">命令行启动训练</h4>
<p>---也是如何Pycharm运行时添加命令行参数的例子</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=4 train_distributed.py</span><br></pre></td></tr></table></figure>
<h4
id="不使用命令行运行而是使用pycharm启动同步夸卡训练的配置">不使用命令行运行，而是使用Pycharm启动同步夸卡训练的配置</h4>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111126656.png"
alt="在这里插入图片描述" />
<figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>python</tag>
        <tag>环境配置</tag>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch中的Batch Normalization layer踩坑</title>
    <url>/2020/02/03/Pytorch%E4%B8%AD%E7%9A%84Batch-Normalization-layer%E8%B8%A9%E5%9D%91/</url>
    <content><![CDATA[<h2 id="注意momentum的定义">1. 注意momentum的定义</h2>
<p>Pytorch中的BN层的动量平滑和常见的动量法计算方式是相反的，默认的momentum=0.1
<span class="math display">\[
\hat{x}_{\text { new }}=(1-\text { momentum }) \times \hat{x}+\text {
momemtum } \times x_{t}
\]</span> BN层里的表达式为： <span class="math display">\[
y=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}} *
\gamma+\beta
\]</span>
其中<em>γ</em>和<em>β</em>是可以学习的参数。在Pytorch中，BN层的类的参数有：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASS torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>每个参数具体含义参见文档，需要注意的是，affine定义了BN层的参数<em>γ</em>和<em>β</em>是否是可学习的(不可学习默认是常数1和0).</p>
<h2 id="注意bn层中含有统计数据数值即均值和方差">2.
注意BN层中含有统计数据数值，即均值和方差</h2>
<p><strong>track_running_stats</strong> – a boolean value that when set
to <code>True</code>, this module tracks the running mean and variance,
and when set to <code>False</code>, this module does not track such
statistics and always uses batch statistics in both training and eval
modes. Default: <code>True</code></p>
<p>在训练过程中model.train()，train过程的BN的统计数值—均值和方差是<strong>通过当前batch数据估计的</strong>。</p>
<p>并且测试时，model.eval()后，若track_running_stats=True，模型此刻所使用的统计数据是Running
status
中的，即通过指数衰减规则，积累到当前的数值。否则依然使用基于当前batch数据的估计值。</p>
<h2
id="bn层的统计数据更新是在每一次训练阶段model.train后的forward方法中自动实现的而不是在梯度计算与反向传播中更新optim.step中完成">3.
BN层的统计数据更新是在每一次训练阶段model.train()后的forward()方法中自动实现的，<strong>而不是</strong>在梯度计算与反向传播中更新optim.step()中完成</h2>
<h2 id="冻结bn及其统计数据">4. 冻结BN及其统计数据</h2>
<p>从上面的分析可以看出来，正确的冻结BN的方式是在模型训练时，把BN单独挑出来，重新设置其状态为eval
(在model.train()之后覆盖training状态）.</p>
<p>解决方案：<a
href="https://discuss.pytorch.org/t/freeze-batchnorm-layer-lead-to-nan/8385">转载自</a></p>
<blockquote>
<p>You should use apply instead of searching its children, while
named_children() doesn’t iteratively search submodules.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_bn_eval</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">      m.eval()</span><br><span class="line"></span><br><span class="line">model.apply(set_bn_eval)</span><br></pre></td></tr></table></figure>
<p>或者，重写module中的train()方法：<a
href="https://discuss.pytorch.org/t/how-to-train-with-frozen-batchnorm/12106/8">转载自</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, mode=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Override the default train() to freeze the BN parameters</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(MyNet, self).train(mode)</span><br><span class="line">        <span class="keyword">if</span> self.freeze_bn:</span><br><span class="line">            print(<span class="string">"Freezing Mean/Var of BatchNorm2D."</span>)</span><br><span class="line">            <span class="keyword">if</span> self.freeze_bn_affine:</span><br><span class="line">                print(<span class="string">"Freezing Weight/Bias of BatchNorm2D."</span>)</span><br><span class="line">        <span class="keyword">if</span> self.freeze_bn:</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> self.backbone.modules():</span><br><span class="line">                <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                    m.eval()</span><br><span class="line">                    <span class="keyword">if</span> self.freeze_bn_affine:</span><br><span class="line">                        m.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">                        m.bias.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2
id="fixfrozen-batch-norm-when-training-may-lead-to-runtimeerror-expected-scalar-type-half-but-found-float">5.
Fix/frozen Batch Norm when training may lead to RuntimeError: expected
scalar type Half but found Float</h2>
<p>解决办法：<a
href="https://github.com/NVIDIA/apex/issues/122">转载自</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> apex.fp16_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fix_bn</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.eval()</span><br><span class="line"></span><br><span class="line">model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.cuda()</span><br><span class="line">model = network_to_half(model)</span><br><span class="line">model.train()</span><br><span class="line">model.apply(fix_bn) <span class="comment"># fix batchnorm</span></span><br><span class="line">input = Variable(torch.FloatTensor(<span class="number">8</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>).cuda().half())</span><br><span class="line">output = model(input)</span><br><span class="line">output_mean = torch.mean(output)</span><br><span class="line">output_mean.backward()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Please do</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def fix_bn(m):</span><br><span class="line"> classname &#x3D; m.__class__.__name__</span><br><span class="line"> if classname.find(&#39;BatchNorm&#39;) !&#x3D; -1:</span><br><span class="line">     m.eval().half()</span><br></pre></td></tr></table></figure>
<p>Reason for this is, for regular training it is better
(performance-wise) <font color="#dd0000">to <strong>use cudnn batch
norm, which requires its weights to be in fp32</strong>, thus batch norm
modules are not converted to half in <code>network_to_half</code>.
However, cudnn does not support batchnorm backward in the eval
mode</font> , which is what you are doing, and to use pytorch
implementation for this, weights have to be of the same type as
inputs.</p>
</blockquote>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>L1，L2正则化的理解</title>
    <url>/2020/01/28/L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>摘录自：</p>
<p>https://zhuanlan.zhihu.com/p/35356992</p>
<p>https://zhuanlan.zhihu.com/p/29360425</p>
<a id="more"></a>
<h2 id="正则化理解之结构最小化">正则化理解之结构最小化</h2>
<h3
id="首先给出一个例子解释l1的作用可以使得模型获得稀疏解">首先给出一个例子解释<strong>L1的作用可以使得模型获得稀疏解</strong></h3>
<p><a
href="https://blog.csdn.net/xiaojiajia007/article/details/90611838">点击查看：L1正则能够使得模型的解稀疏</a>
<img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111123216.jpeg"
alt="在这里插入图片描述" /></p>
<p><strong>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。</strong></p>
<p>给loss function加上正则化项，能使得新得到的优化目标函数h =
f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。</p>
<p><strong>L1正则化和L2正则化：</strong></p>
<p>L1正则化就是在loss
function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss
function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。</p>
<h3 id="作图说明">作图说明</h3>
<p><a
href="https://blog.csdn.net/liangdong2014/article/details/79517638">摘录自</a></p>
<p><img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124537.png"
alt="在这里插入图片描述" />
<strong>从等高线和取值空间的交点可以看到L1更容易倾向一个权重偏大一个权重为0。L2更容易倾向权重都较小。</strong></p>
<p>而通过求导数可以看出，对于两种正则带来的梯度更新：</p>
<ul>
<li>L1减少的是一个常量，L2减少的是权重的固定比例</li>
<li>孰快孰慢取决于权重本身的大小，权重刚大时可能L2快，较小时L1快</li>
<li>L1使权重稀疏，L2使权重平滑，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0</li>
</ul>
<hr />
<h2
id="正则化理解之最大后验概率估计map">正则化理解之最大后验概率估计（MAP）</h2>
<h3 id="在最大似然估计中1">在最大似然估计中<a href="#fn1"
class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></h3>
<p>设<span class="math inline">\(X\)</span> 、 <span
class="math inline">\(y\)</span> 为训练样本和相应的标签, <span
class="math inline">\(X=\left(x_{1}, x_{2}, \ldots, x_{n}
\right)\)</span> 是一组抽样数据，满足独立同分布假设（i.i.d），假设权重
<span class="math inline">\(w\)</span>
是未知的参数，从而求得对数<strong>似然函数</strong>，然后找到使得MLP在当前观测到的数据集条件下达到最大值时对应的权重<span
class="math inline">\(w\)</span>: <span class="math display">\[
\operatorname{MLP}=l(w)=\log [P(y | X ; w)]=\log \left[\prod_{i}
P\left(y^{i} | x^{i} ; w\right)\right]
\]</span> 通过假设 <span class="math inline">\(y^{i}\)</span>
的不同概率分布，即可得到不同的模型（即变成已知模型形式，拟合模型参数的问题，<font color="#000066">w的写法是前面加分号，表示它是某一固定参数值，而不是概率条件！</font>）。例如若假设
<span class="math inline">\(y^{i} \sim N\left(w^{T} x^{i},
\sigma^{2}\right)\)</span> 的高斯分布（<span
class="math inline">\(x^{i}\)</span>也是一系列随机向量，随机向量的每个分量都对<span
class="math inline">\(y^{i}\)</span>有影响，若随机向量的维度很大，可以认为<span
class="math inline">\(y^{i}\)</span>服从正态分布，而一般正态分布可以转化成标准正态分布求解），则有：
<span class="math display">\[
l(w)=\log \left[\prod_{i} \frac{1}{\sqrt{2 \pi} \sigma}
e^{-\frac{\left(y^{i}-w^{T} x^{i}\right)^{2}}{2
\sigma^{2}}}\right]=-\frac{1}{2 \sigma^{2}} \sum_{i}\left(y^{i}-w^{T}
x^{i}\right)^{2}+C
\]</span> 式子中<span
class="math inline">\(C\)</span>是常数项，常数项和系数项不影响求最大值，因而可令<span
class="math inline">\(J(w ; X,
y)=-l(w)\)</span>即可得到线性回归的代价函数。这里我们可以看到，假设<span
class="math inline">\(y^{i}\)</span>
<font color=purple face=bold>服从正态分布的极大似然估计方法和均方误差最小化求解线性回归的结果是一样的！</font></p>
<h3 id="在最大后验概率估计中">在最大后验概率估计中</h3>
<p>将权重<span
class="math inline">\(w\)</span>看作随机变量，也具有某种分布，从而有：
<span class="math display">\[
P(w|X,y)=\frac { P(w,X,y) }{ P(X,y) } =\frac { P(X,y|w)P(w) }{ P(X,y) }
=\frac { P(X)P(y|w,X)P(w) }{ P(X,y) } \propto P(y|X,w)P(w)
\]</span> 上面式子中<span
class="math inline">\(P(X,y)\)</span>等对于特定问题已经是固定值了，与<span
class="math inline">\(w\)</span>无关，所以求的<span
class="math inline">\(P(w|X,y)\)</span>正比于<span
class="math inline">\(P(y|X,w)P(w)\)</span>。
那我们利用最大后验概率估计求参数 <span class="math inline">\(w\)</span>
的时候，找到使得MAP在当前观测到的数据集条件下达到最大值时对应的权重<span
class="math inline">\(w\)</span>。同样取对数有<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>： <span class="math display">\[
\operatorname{MAP}=\log P(y | X, w) P(w)=\log P(y | X, w)+\log P(w)
\]</span> 可以看出后验概率函数为在似然函数的基础上增加了一项 <span
class="math inline">\(\log P(w)\)</span>。<span
class="math inline">\(P(w)\)</span>的意义是对权重系数<span
class="math inline">\(w\)</span>的概率分布的先验假设,在收集到训练样本{<span
class="math inline">\(X,y\)</span>}后，则可根据w在{<span
class="math inline">\(X,y\)</span>}下的后验概率对<span
class="math inline">\(w\)</span>进行修正，从而可以对<span
class="math inline">\(w\)</span>更好地估计。</p>
<blockquote>
<p><strong>这里补充一下周志华老师的西瓜书149页的知识：</strong></p>
<p>概率学派认为参数虽然未知，但确实是客观存在的固定值，而贝叶斯学派则认为参数是未观察到的随机变量，其本身也有分布。因此可以先假定参数服从某个先验分布（没有观测到任何当前的数据前的先验知识），然后基于当前的观测值来计算参数的后验分布。</p>
</blockquote>
<h3
id="若假设-w_j-的先验分布为0均值的高斯分布即-w_j-sim-nleft0-sigma2right">若假设
<span class="math inline">\(w_{j}\)</span>
的先验分布为0均值的高斯分布，即 <span class="math inline">\(w_{j} \sim
N\left(0, \sigma^{2}\right)\)</span>，</h3>
<p>则有： <span class="math display">\[
\log P(w)=\log \prod_{j} P\left(w_{j}\right)=\log
\prod_{j}\left[\frac{1}{\sqrt{2 \pi} \sigma}
e^{-\frac{\left(w_{j}\right)^{2}}{2 \sigma^{2}}}\right]=-\frac{1}{2
\sigma^{2}} \sum_{j} w_{j}^{2}+C^{\prime}
\]</span> <img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124147.jpeg"
alt="高斯分布" /> 可以看到，在高斯分布下<span
class="math inline">\(\log{P(w)}\)</span>的效果等价于在代价函数中增加<span
class="math inline">\(L_{2}\)</span>正则项，也就是说<strong>在MAP中使用一个高斯分布的先验等价于在MLE中采用L2的正则</strong>。从上图可以看出<span
class="math inline">\(w\)</span>值取到0附近的概率特别大。也就是说我们提前先假设了<span
class="math inline">\(w\)</span>的解更容易取到0的附近。</p>
<h3
id="若假设w_j服从均值为0参数为a的拉普拉斯分布即pleftw_jrightfrac12-a-efrac-leftw_jrighta">若假设<span
class="math inline">\(w_{j}\)</span>服从均值为0、参数为a的拉普拉斯分布，即：<span
class="math inline">\(P\left(w_{j}\right)=\frac{1}{2 a}
e^{\frac{-\left|w_{j}\right|}{a}}\)</span></h3>
<p>则有： <span class="math display">\[
\log P(w)=\log \prod_{j} \frac{1}{2 a}
e^{\frac{-\left|w_{j}\right|}{a}}=-\frac{1}{a}
\sum_{j}\left|w_{j}\right|+C^{\prime}
\]</span> <img
src="https://cdn.jsdelivr.net/gh/hellojialee/PictureBed@master/img2bolg/202309111124443.jpeg"
alt="拉普拉斯分布" /> 可以看到，在拉普拉斯分布下<span
class="math inline">\(\log{P(w)}\)</span>的效果等价于在代价函数中增加<span
class="math inline">\(L_{1}\)</span>正则项。从下图可以看出<span
class="math inline">\(w\)</span>值取到0的概率特别大。也就是说我们提前先假设了<span
class="math inline">\(w\)</span>的解更容易取到0。</p>
<h3
id="我们得到对于l_1l_2正则化的一种最大后验角度理解">我们得到对于<span
class="math inline">\(L_{1}\)</span>、<span
class="math inline">\(L_{2}\)</span>正则化的一种最大后验角度理解</h3>
<ul>
<li><span class="math inline">\(L_{1}\)</span>正则化可通过假设权重<span
class="math inline">\(w\)</span>
的先验分布为拉普拉斯分布，由最大后验概率估计导出</li>
<li><span class="math inline">\(L_{2}\)</span>正则化可通过假设权重<span
class="math inline">\(w\)</span>
的先验分布为高斯分布，由最大后验概率估计导出</li>
</ul>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="math inline">\(\arg \max
\sum_{i=1}^{n} \log P\left(x_{i} ;
\theta\right)\)</span>。对于离散的用分布律P，对于连续型的变量在数学中已知模型表达式则用概率密度函数f<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>辨析：MLP（Maximum A
Posteriori）是最大后验概率估计（贝叶斯学派）； MAP（Maximum Likelihood
Estimation）是最大似然估计（概率学派）。<a
href="https://zhuanlan.zhihu.com/p/32480810">请参考</a><a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>这里应该是省略的写法，比如<span
class="math inline">\(P(w|X,y)\)</span>，里面三个都是随机变量，实际上对于一个具体的训练样本完整的写法应该是<span
class="math inline">\(P(w=a|X=x1,Y=y1)\)</span><a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>一个 Pytorch 训练实践 (分布式训练 + 半精度_混合精度训练)</title>
    <url>/2020/01/28/%E4%B8%80%E4%B8%AA%20Pytorch%20%E8%AE%AD%E7%BB%83%E5%AE%9E%E8%B7%B5%20%EF%BC%88%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%20+%20%E5%8D%8A%E7%B2%BE%E5%BA%A6_%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%89/</url>
    <content><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/96408719"
title="Permalink to 一个 Pytorch 训练实践 （分布式训练 + 半精度/混合精度训练）">Source</a></p>
<h3 id="内容速览">内容速览</h3>
<ol type="1">
<li>'train.py': single training process on one GPU only.</li>
<li>'train_parallel.py': signle training process on multiple GPUs using
<strong>Dataparallel</strong> (包括不同GPU之间的负载均衡).</li>
<li>'train_distributed.py' (<strong>recommended</strong>): multiple
training processes on multiple GPUs using <strong>Nvidia Apex</strong>
&amp; <strong>Distributed Training:</strong></li>
</ol>
<a id="more"></a>
<p><code>python -m torch.distributed.launch --nproc_per_node=4 train_distributed.py</code></p>
<h3 id="项目完整代码地址">项目（完整代码）地址</h3>
<p>网页地址：</p>
<p><a
href="https://link.zhihu.com/?target=https%253A//github.com/jialee93/Improved-Body-Parts">https://github.com/jialee93/Improved-Body-Parts​github.com</a></p>
<p><strong>Please cite <a
href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">this
paper</a> kindly in your publications if the corresponding projects
helps your research.</strong></p>
<p><a
href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">https://arxiv.org/abs/1911.10529​arxiv.org</a></p>
<pre><code>inproceedings{li2019simple,
    title={Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation},
    author={Jia Li and Wen Su and Zengfu Wang},
    booktitle = {arXiv preprint arXiv:1911.10529},
    year={2019}
}</code></pre>
<h3 id="项目声明">项目声明</h3>
<p>最近开源了一个项目的代码，因为平时有很多其他事情需要处理，业余时间自己又喜欢享受生活，科研对我来说简直彻底成为了副业，羞愧。</p>
<p>之前一直使用TensorFlow以及Keras，但是一直觉得它们没有继承Python丝滑的特性，所以最近果断转移到Pytorch。因此在做这份工作时，我以新入门的用户身份开始，把整个训练和测试流程给走一遍，踩到并且努力解决全过程的坑。这里也是给自己实验过程的踩坑做个总结，并且希望通过这个分享能够提高自己工作的关注度，以及希望自己的经验能够为他人所用。当然了，如果我的分享能够帮助到大家（<strong>主要针对Pytorch新手或者普通玩家，高级玩家请忽略本菜鸟，轻拍，谢谢</strong>），也欢迎引用我的对应的项目论文。</p>
<h3
id="多gpu分布式训练混合精度加速训练">多GPU分布式训练+混合精度加速训练</h3>
<p>对应上面开源链接的<em>train_distributed.py</em>脚本。相信最直接的代码以及注释就是最好的说明。该脚本同时包含了最常用的代码模版，包括例如
多进程的训练数据准备，模型权重的保存与载入，冻结部分网络层的权重，变相增加batch
size，使用Nvidia官方的Apex包通过半精度或混合精度进行模型压缩和加速等等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> apex.optimizers <span class="keyword">as</span> apex_optim</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> config.config <span class="keyword">import</span> GetConfig, COCOSourceConfig, TrainingOpt</span><br><span class="line"><span class="keyword">from</span> data.mydataset <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> models.posenet <span class="keyword">import</span> Network</span><br><span class="line"><span class="keyword">from</span> models.loss_model <span class="keyword">import</span> MultiTaskLoss</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> apex.optimizers <span class="keyword">as</span> apex_optim</span><br><span class="line">    <span class="keyword">from</span> apex.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line">    <span class="keyword">from</span> apex.fp16_utils <span class="keyword">import</span> *</span><br><span class="line">    <span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br><span class="line">    <span class="keyword">from</span> apex.multi_tensor_apply <span class="keyword">import</span> multi_tensor_applier</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">raise</span> ImportError(<span class="string">"Please install apex from https://www.github.com/nvidia/apex to run this example."</span>)</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'PoseNet Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--resume'</span>, <span class="string">'-r'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>, help=<span class="string">'resume from checkpoint'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--freeze'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    help=<span class="string">'freeze the pre-trained layers before output layers'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--warmup'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>, help=<span class="string">'using warm-up learning rate'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--checkpoint_path'</span>, <span class="string">'-p'</span>, default=<span class="string">'link2checkpoints_distributed'</span>, help=<span class="string">'save path'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--max_grad_norm'</span>, default=<span class="number">10</span>, type=float,</span><br><span class="line">                    help=(<span class="string">"If the norm of the gradient vector exceeds this, "</span></span><br><span class="line">                          <span class="string">"re-normalize it to have the norm equal to max_grad_norm"</span>))</span><br><span class="line"><span class="comment"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied automatically by torch.distributed.launch.</span></span><br><span class="line">parser.add_argument(<span class="string">"--local_rank"</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">parser.add_argument(<span class="string">'--opt-level'</span>, type=str, default=<span class="string">'O1'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--sync_bn'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>,</span><br><span class="line">                    help=<span class="string">'enabling apex sync BN.'</span>)  <span class="comment"># 无触发为false， -s 触发为true</span></span><br><span class="line">parser.add_argument(<span class="string">'--keep-batchnorm-fp32'</span>, type=str, default=<span class="literal">None</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--loss-scale'</span>, type=str, default=<span class="literal">None</span>)  <span class="comment"># '1.0'</span></span><br><span class="line">parser.add_argument(<span class="string">'--print-freq'</span>, <span class="string">'-f'</span>, default=<span class="number">10</span>, type=int, metavar=<span class="string">'N'</span>, help=<span class="string">'print frequency (default: 10)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="comment"># ###################################  Setup for some configurations ###########################################</span></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span>  <span class="comment"># 如果我们每次训练的输入数据的size不变，那么开启这个就会加快我们的训练速度</span></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">checkpoint_path = args.checkpoint_path</span><br><span class="line">opt = TrainingOpt()</span><br><span class="line">config = GetConfig(opt.config_name)</span><br><span class="line">soureconfig = COCOSourceConfig(opt.hdf5_train_data)  <span class="comment"># # 对于分布式训练，total_batch size = batch_size*world_size</span></span><br><span class="line">train_data = MyDataset(config, soureconfig, shuffle=<span class="literal">False</span>, augment=<span class="literal">True</span>)  <span class="comment"># shuffle in data loader</span></span><br><span class="line"></span><br><span class="line">soureconfig_val = COCOSourceConfig(opt.hdf5_val_data)</span><br><span class="line">val_data = MyDataset(config, soureconfig_val, shuffle=<span class="literal">False</span>, augment=<span class="literal">False</span>)  <span class="comment"># shuffle in data loader</span></span><br><span class="line"></span><br><span class="line">best_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">start_epoch = <span class="number">0</span>  <span class="comment"># 从0开始或者从上一个epoch开始</span></span><br><span class="line"></span><br><span class="line">args.distributed = <span class="literal">False</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'WORLD_SIZE'</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">    args.distributed = int(os.environ[<span class="string">'WORLD_SIZE'</span>]) &gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">args.gpu = <span class="number">0</span></span><br><span class="line">args.world_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,</span></span><br><span class="line"><span class="comment"># the 'WORLD_SIZE' environment variable will also be set automatically.</span></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    args.gpu = args.local_rank</span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    <span class="comment"># Initializes the distributed backend which will take care of synchronizing nodes/GPUs</span></span><br><span class="line">    torch.distributed.init_process_group(backend=<span class="string">'nccl'</span>, init_method=<span class="string">'env://'</span>)</span><br><span class="line">    args.world_size = torch.distributed.get_world_size()  <span class="comment"># 获取分布式训练的进程数</span></span><br><span class="line">    print(<span class="string">"World Size is :"</span>, args.world_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.backends.cudnn.enabled, <span class="string">"Amp requires cudnn backend to be enabled."</span></span><br><span class="line"></span><br><span class="line">model = Network(opt, config, dist=<span class="literal">True</span>, bn=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.sync_bn:  <span class="comment"># 用累计loss来达到sync bn 是不是更好，更改bn的momentum大小</span></span><br><span class="line">    <span class="comment">#  This should be done before model = DDP(model, delay_allreduce=True),</span></span><br><span class="line">    <span class="comment">#  because DDP needs to see the finalized model parameters</span></span><br><span class="line">    <span class="comment"># We rely on torch distributed for synchronization between processes. Only DDP support the apex sync_bn now.</span></span><br><span class="line">    <span class="keyword">import</span> apex</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Using apex synced BN."</span>)</span><br><span class="line">    model = apex.parallel.convert_syncbn_model(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># It should be called before constructing optimizer if the module will live on GPU while being optimized.</span></span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">        print(<span class="string">'Parameters of network: Autograd'</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="comment"># ######################################## Froze some layers to fine-turn the model  ########################</span></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="keyword">if</span> args.freeze:</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():  <span class="comment"># 带有参数名的模型的各个层包含的参数遍历</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'out'</span> <span class="keyword">or</span> <span class="string">'merge'</span> <span class="keyword">or</span> <span class="string">'before_regress'</span> <span class="keyword">in</span> name:  <span class="comment"># 判断参数名字符串中是否包含某些关键字</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># #############################################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Actual working batch size on multi-GPUs is 4 times bigger than that on one GPU</span></span><br><span class="line"><span class="comment"># fixme: add up momentum if the batch grows?</span></span><br><span class="line"><span class="comment"># fixme: change weight_decay?</span></span><br><span class="line"><span class="comment">#    nesterov = True</span></span><br><span class="line"><span class="comment"># optimizer = apex_optim.FusedSGD(filter(lambda p: p.requires_grad, model.parameters()),</span></span><br><span class="line"><span class="comment">#                                 lr=opt.learning_rate * args.world_size, momentum=0.9, weight_decay=5e-4)</span></span><br><span class="line">optimizer = optim.SGD(filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()),</span><br><span class="line">                      lr=opt.learning_rate * args.world_size, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"><span class="comment"># optimizer = apex_optim.FusedAdam(model.parameters(), lr=opt.learning_rate * args.world_size, weight_decay=1e-4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置学习率下降策略, extract the "bare"  Pytorch optimizer before Apex wrapping.</span></span><br><span class="line"><span class="comment"># scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.4, last_epoch=-1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Amp.  Amp accepts either values or strings for the optional override arguments,</span></span><br><span class="line"><span class="comment"># for convenient interoperation with argparse.</span></span><br><span class="line"><span class="comment"># For distributed training, wrap the model with apex.parallel.DistributedDataParallel.</span></span><br><span class="line"><span class="comment"># This must be done AFTER the call to amp.initialize.</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer,</span><br><span class="line">                                  opt_level=args.opt_level,</span><br><span class="line">                                  keep_batchnorm_fp32=args.keep_batchnorm_fp32,</span><br><span class="line">                                  loss_scale=args.loss_scale)  <span class="comment"># Dynamic loss scaling is used by default.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    <span class="comment"># By default, apex.parallel.DistributedDataParallel overlaps communication with computation in the backward pass.</span></span><br><span class="line">    <span class="comment"># model = DDP(model)</span></span><br><span class="line">    <span class="comment"># delay_allreduce delays all communication to the end of the backward pass.</span></span><br><span class="line">    <span class="comment"># DDP模块同时也计算整体的平均梯度, 这样我们就不需要在训练步骤计算平均梯度。</span></span><br><span class="line">    model = DDP(model, delay_allreduce=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ###################################  Resume from checkpoint ###########################################</span></span><br><span class="line"><span class="keyword">if</span> args.resume:</span><br><span class="line">    <span class="comment"># Use a local scope to avoid dangling references</span></span><br><span class="line">    <span class="comment"># dangling references: a variable that refers to an object that was deleted prematurely</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">resume</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(opt.ckpt_path):</span><br><span class="line">            print(<span class="string">'Resuming from checkpoint ...... '</span>)</span><br><span class="line">            checkpoint = torch.load(opt.ckpt_path,</span><br><span class="line">                                    map_location=torch.device(<span class="string">'cpu'</span>))  <span class="comment"># map to cpu to save the gpu memory</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># #################################################</span></span><br><span class="line">            <span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">            new_state_dict = OrderedDict()</span><br><span class="line">            <span class="comment"># # #################################################</span></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> checkpoint[<span class="string">'weights'</span>].items():</span><br><span class="line">                <span class="comment"># Exclude the regression layer by commenting the following code when we change the output dims!</span></span><br><span class="line">                <span class="comment"># if 'out' or 'merge' or 'before_regress'in k:</span></span><br><span class="line">                <span class="comment">#     continue</span></span><br><span class="line">                name = <span class="string">'module.'</span> + k  <span class="comment"># add prefix 'module.'</span></span><br><span class="line">                new_state_dict[name] = v</span><br><span class="line">            model.load_state_dict(new_state_dict, strict=<span class="literal">False</span>)  <span class="comment"># , strict=False</span></span><br><span class="line">            <span class="comment"># # #################################################</span></span><br><span class="line">            <span class="comment"># model.load_state_dict(checkpoint['weights'])  # 加入他人训练的模型，可能需要忽略部分层，则strict=False</span></span><br><span class="line">            print(<span class="string">'Network weights have been resumed from checkpoint...'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># amp.load_state_dict(checkpoint['amp'])</span></span><br><span class="line">            <span class="comment"># print('AMP loss_scalers and unskipped steps have been resumed from checkpoint...')</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ############## We must convert the resumed state data of optimizer to gpu  ##############</span></span><br><span class="line">            <span class="comment"># """It is because the previous training was done on gpu, so when saving the optimizer.state_dict, the stored</span></span><br><span class="line">            <span class="comment">#  states(tensors) are of cuda version. During resuming, when we load the saved optimizer, load_state_dict()</span></span><br><span class="line">            <span class="comment">#  loads this cuda version to cpu. But in this project, we use map_location to map the state tensors to cpu.</span></span><br><span class="line">            <span class="comment">#  In the training process, we need cuda version of state tensors, so we have to convert them to gpu."""</span></span><br><span class="line">            optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_weight'</span>])</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> optimizer.state.values():</span><br><span class="line">                <span class="keyword">for</span> k, v <span class="keyword">in</span> state.items():</span><br><span class="line">                    <span class="keyword">if</span> torch.is_tensor(v):</span><br><span class="line">                        state[k] = v.cuda()</span><br><span class="line">            print(<span class="string">'Optimizer has been resumed from checkpoint...'</span>)</span><br><span class="line">            <span class="keyword">global</span> best_loss, start_epoch  <span class="comment"># global declaration. otherwise best_loss and start_epoch can not be changed</span></span><br><span class="line">            best_loss = checkpoint[<span class="string">'train_loss'</span>]</span><br><span class="line">            print(<span class="string">'******************** Best loss resumed is :'</span>, best_loss, <span class="string">'  ************************'</span>)</span><br><span class="line">            start_epoch = checkpoint[<span class="string">'epoch'</span>] + <span class="number">1</span></span><br><span class="line">            print(<span class="string">"========&gt; Resume and start training from Epoch &#123;&#125; "</span>.format(start_epoch))</span><br><span class="line">            <span class="keyword">del</span> checkpoint</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"========&gt; No checkpoint found at '&#123;&#125;'"</span>.format(opt.ckpt_path))</span><br><span class="line"></span><br><span class="line">    resume()</span><br><span class="line"></span><br><span class="line">train_sampler = <span class="literal">None</span></span><br><span class="line">val_sampler = <span class="literal">None</span></span><br><span class="line"><span class="comment"># Restricts data loading to a subset of the dataset exclusive to the current process</span></span><br><span class="line"><span class="comment"># Create DistributedSampler to handle distributing the dataset across nodes when training 创建分布式采样器来控制训练中节点间的数据分发</span></span><br><span class="line"><span class="comment"># This can only be called after distributed.init_process_group is called 这个只能在 distributed.init_process_group 被调用后调用</span></span><br><span class="line"><span class="comment"># 这个对象控制进入分布式环境的数据集以确保模型不是对同一个子数据集训练，以达到训练目标。</span></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)</span><br><span class="line">    val_sampler = torch.utils.data.distributed.DistributedSampler(val_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器，在训练和验证步骤中喂数据</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">                                           num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>, sampler=train_sampler, drop_last=<span class="literal">True</span>)</span><br><span class="line">val_loader = torch.utils.data.DataLoader(val_data, batch_size=opt.batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                         num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>, sampler=val_sampler, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">        print(<span class="string">'Parameters of network: Autograd'</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #  Update the learning rate for start_epoch times</span></span><br><span class="line"><span class="comment"># for i in range(start_epoch):</span></span><br><span class="line"><span class="comment">#     scheduler.step()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    print(<span class="string">'\n ############################# Train phase, Epoch: &#123;&#125; #############################'</span>.format(epoch))</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># DistributedSampler 中记录目前的 epoch 数， 因为采样器是根据 epoch 来决定如何打乱分配数据进各个进程</span></span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        train_sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># scheduler.step()  use 'adjust learning rate' instead</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># adjust_learning_rate_cyclic(optimizer, epoch, start_epoch)  # start_epoch</span></span><br><span class="line">    print(<span class="string">'\nLearning rate at this epoch is: %0.9f\n'</span> % optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>])  <span class="comment"># scheduler.get_lr()[0]</span></span><br><span class="line"></span><br><span class="line">    batch_time = AverageMeter()</span><br><span class="line">    losses = AverageMeter()</span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, target_tuple <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment"># # ##############  Use schedule step or fun of 'adjust learning rate' #####################</span></span><br><span class="line">        adjust_learning_rate(optimizer, epoch, batch_idx, len(train_loader), use_warmup=args.warmup)</span><br><span class="line">        <span class="comment"># print('\nLearning rate at this epoch is: %0.9f\n' % optimizer.param_groups[0]['lr'])  # scheduler.get_lr()[0]</span></span><br><span class="line">        <span class="comment"># # ##########################################################</span></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            <span class="comment">#  这允许异步 GPU 复制数据也就是说计算和数据传输可以同时进.</span></span><br><span class="line">            target_tuple = [target_tensor.cuda(non_blocking=<span class="literal">True</span>) <span class="keyword">for</span> target_tensor <span class="keyword">in</span> target_tuple]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]</span></span><br><span class="line">        images, mask_misses, heatmaps = target_tuple  <span class="comment"># , offsets, mask_offsets</span></span><br><span class="line">        <span class="comment"># images = Variable(images)</span></span><br><span class="line">        <span class="comment"># loc_targets = Variable(loc_targets)</span></span><br><span class="line">        <span class="comment"># conf_targets = Variable(conf_targets)</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># zero the gradient buff</span></span><br><span class="line">        loss = model(target_tuple)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> loss.item() &gt; <span class="number">2e5</span>:  <span class="comment"># try to rescue the gradient explosion</span></span><br><span class="line">            print(<span class="string">"\nOh My God ! \nLoss is abnormal, drop this batch !"</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">            scaled_loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)  # fixme: 可能是这个的问题吗？</span></span><br><span class="line">        optimizer.step()  <span class="comment"># TODO：可以使用累加的loss变相增大batch size，但对于bn层需要减少默认的momentum</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train_loss += loss.item()  # 累加的loss !</span></span><br><span class="line">        <span class="comment"># 使用loss += loss.detach()来获取不需要梯度回传的部分。</span></span><br><span class="line">        <span class="comment"># 或者使用loss.item()直接获得所对应的python数据类型，但是仅仅限于only one element tensors can be converted to Python scalars</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Every print_freq iterations, check the loss, accuracy, and speed.</span></span><br><span class="line">            <span class="comment"># For best performance, it doesn't make sense to print these metrics every</span></span><br><span class="line">            <span class="comment"># iteration, since they incur an allreduce and some host&lt;-&gt;device syncs.</span></span><br><span class="line">            <span class="comment"># print 会触发allreduce，而这个操作比较费时</span></span><br><span class="line">            <span class="keyword">if</span> args.distributed:</span><br><span class="line">                <span class="comment"># We manually reduce and average the metrics across processes. In-place reduce tensor.</span></span><br><span class="line">                reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                reduced_loss = loss.data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># to_python_float incurs a host&lt;-&gt;device sync</span></span><br><span class="line">            losses.update(to_python_float(reduced_loss), images.size(<span class="number">0</span>))  <span class="comment"># update needs average and number</span></span><br><span class="line">            torch.cuda.synchronize()  <span class="comment"># 因为所有GPU操作是异步的，应等待当前设备上所有流中的所有核心完成，测试的时间才正确</span></span><br><span class="line">            batch_time.update((time.time() - end) / args.print_freq)</span><br><span class="line">            end = time.time()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">                print(<span class="string">'==================&gt; Epoch: [&#123;0&#125;][&#123;1&#125;/&#123;2&#125;]\t'</span></span><br><span class="line">                      <span class="string">'Time &#123;batch_time.val:.3f&#125; (&#123;batch_time.avg:.3f&#125;)\t'</span></span><br><span class="line">                      <span class="string">'Speed &#123;3:.3f&#125; (&#123;4:.3f&#125;)\t'</span></span><br><span class="line">                      <span class="string">'Loss &#123;loss.val:.10f&#125; (&#123;loss.avg:.4f&#125;) &lt;================ \t'</span>.format(</span><br><span class="line">                    epoch, batch_idx, len(train_loader),</span><br><span class="line">                    args.world_size * opt.batch_size / batch_time.val,</span><br><span class="line">                    args.world_size * opt.batch_size / batch_time.avg,</span><br><span class="line">                    batch_time=batch_time,</span><br><span class="line">                    loss=losses))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> best_loss</span><br><span class="line">    <span class="comment"># DistributedSampler控制进入分布式环境的数据集以确保模型不是对同一个子数据集训练，以达到训练目标。</span></span><br><span class="line">    <span class="comment"># train_loss /= (len(train_loader))  # Each GPU process can only see 1/(world_size) training samples per epoch</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Write the log file each epoch.</span></span><br><span class="line">        os.makedirs(checkpoint_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        logger = open(os.path.join(<span class="string">'./'</span> + checkpoint_path, <span class="string">'log'</span>), <span class="string">'a+'</span>)</span><br><span class="line">        logger.write(<span class="string">'\nEpoch &#123;&#125;\ttrain_loss: &#123;&#125;'</span>.format(epoch, losses.avg))  <span class="comment"># validation时不要\n换行</span></span><br><span class="line">        logger.flush()</span><br><span class="line">        logger.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> losses.avg &lt; float(<span class="string">'inf'</span>):  <span class="comment"># &lt; best_loss</span></span><br><span class="line">            <span class="comment"># Update the best_loss if the average loss drops</span></span><br><span class="line">            best_loss = losses.avg</span><br><span class="line">            print(<span class="string">'\nSaving model checkpoint...\n'</span>)</span><br><span class="line">            state = &#123;</span><br><span class="line">                <span class="comment"># not posenet.state_dict(). then, we don't ge the "module" string to begin with</span></span><br><span class="line">                <span class="string">'weights'</span>: model.module.state_dict(),</span><br><span class="line">                <span class="string">'optimizer_weight'</span>: optimizer.state_dict(),</span><br><span class="line">                <span class="comment"># 'amp': amp.state_dict(),</span></span><br><span class="line">                <span class="string">'train_loss'</span>: losses.avg,</span><br><span class="line">                <span class="string">'epoch'</span>: epoch</span><br><span class="line">            &#125;</span><br><span class="line">            torch.save(state, <span class="string">'./'</span> + checkpoint_path + <span class="string">'/PoseNet_'</span> + str(epoch) + <span class="string">'_epoch.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    print(<span class="string">'\n ############################# Test phase, Epoch: &#123;&#125; #############################'</span>.format(epoch))</span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment"># DistributedSampler 中记录目前的 epoch 数， 因为采样器是根据 epoch 来决定如何打乱分配数据进各个进程</span></span><br><span class="line">    <span class="comment"># if args.distributed:</span></span><br><span class="line">    <span class="comment">#     val_sampler.set_epoch(epoch)  # 验证集太小，不够4个划分</span></span><br><span class="line">    batch_time = AverageMeter()</span><br><span class="line">    losses = AverageMeter()</span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, target_tuple <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">        <span class="comment"># images.requires_grad_()</span></span><br><span class="line">        <span class="comment"># loc_targets.requires_grad_()</span></span><br><span class="line">        <span class="comment"># conf_targets.requires_grad_()</span></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            <span class="comment">#  这允许异步 GPU 复制数据也就是说计算和数据传输可以同时进.</span></span><br><span class="line">            target_tuple = [target_tensor.cuda(non_blocking=<span class="literal">True</span>) <span class="keyword">for</span> target_tensor <span class="keyword">in</span> target_tuple]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]</span></span><br><span class="line">        images, mask_misses, heatmaps = target_tuple  <span class="comment"># , offsets, mask_offsets</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            _, loss = model(target_tuple)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.distributed:</span><br><span class="line">            <span class="comment"># We manually reduce and average the metrics across processes. In-place reduce tensor.</span></span><br><span class="line">            reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reduced_loss = loss.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_python_float incurs a host&lt;-&gt;device sync</span></span><br><span class="line">        losses.update(to_python_float(reduced_loss), images.size(<span class="number">0</span>))  <span class="comment"># update needs average and number</span></span><br><span class="line">        torch.cuda.synchronize()  <span class="comment"># 因为所有GPU操作是异步的，应等待当前设备上所有流中的所有核心完成，测试的时间才正确</span></span><br><span class="line">        batch_time.update((time.time() - end))</span><br><span class="line">        end = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">            print(<span class="string">'==================&gt;Test: [&#123;0&#125;/&#123;1&#125;]\t'</span></span><br><span class="line">                  <span class="string">'Time &#123;batch_time.val:.3f&#125; (&#123;batch_time.avg:.3f&#125;)\t'</span></span><br><span class="line">                  <span class="string">'Speed &#123;2:.3f&#125; (&#123;3:.3f&#125;)\t'</span></span><br><span class="line">                  <span class="string">'Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;)\t'</span>.format(</span><br><span class="line">                batch_idx, len(val_loader),</span><br><span class="line">                args.world_size * opt.batch_size / batch_time.val,</span><br><span class="line">                args.world_size * opt.batch_size / batch_time.avg,</span><br><span class="line">                batch_time=batch_time, loss=losses))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">        <span class="comment"># Write the log file each epoch.</span></span><br><span class="line">        os.makedirs(checkpoint_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        logger = open(os.path.join(<span class="string">'./'</span> + checkpoint_path, <span class="string">'log'</span>), <span class="string">'a+'</span>)</span><br><span class="line">        logger.write(<span class="string">'\tval_loss: &#123;&#125;'</span>.format(losses.avg))  <span class="comment"># validation时不要\n换行</span></span><br><span class="line">        logger.flush()</span><br><span class="line">        logger.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span><span class="params">(optimizer, epoch, step, len_epoch, use_warmup=False)</span>:</span></span><br><span class="line">    factor = epoch // <span class="number">15</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch &gt;= <span class="number">78</span>:</span><br><span class="line">        factor = (epoch - <span class="number">78</span>) // <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    lr = opt.learning_rate * args.world_size * (<span class="number">0.2</span> ** factor)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""Warmup"""</span></span><br><span class="line">    <span class="keyword">if</span> use_warmup:</span><br><span class="line">        <span class="keyword">if</span> epoch &lt; <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># print('=============&gt;  Using warm-up learning rate....')</span></span><br><span class="line">            lr = lr * float(<span class="number">1</span> + step + epoch * len_epoch) / (<span class="number">3.</span> * len_epoch)  <span class="comment"># len_epoch=len(train_loader)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># if(args.local_rank == 0):</span></span><br><span class="line">    <span class="comment">#     print("epoch = &#123;&#125;, step = &#123;&#125;, lr = &#123;&#125;".format(epoch, step, lr))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate_cyclic</span><span class="params">(optimizer, current_epoch, start_epoch, swa_freqent=<span class="number">5</span>, lr_max=<span class="number">4e-5</span>, lr_min=<span class="number">2e-5</span>)</span>:</span></span><br><span class="line">    epoch = current_epoch - start_epoch</span><br><span class="line"></span><br><span class="line">    lr = lr_max - (lr_max - lr_min) / (swa_freqent - <span class="number">1</span>) * (epoch - epoch // swa_freqent * swa_freqent)</span><br><span class="line">    lr = round(lr, <span class="number">8</span>)</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageMeter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Computes and stores the average and current value"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.val = <span class="number">0</span></span><br><span class="line">        self.avg = <span class="number">0</span></span><br><span class="line">        self.sum = <span class="number">0</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, val, n=<span class="number">1</span>)</span>:</span></span><br><span class="line">        self.val = val</span><br><span class="line">        self.sum += val * n</span><br><span class="line">        self.count += n</span><br><span class="line">        self.avg = self.sum / self.count</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Reduces the tensor data across all machines</span></span><br><span class="line">    <span class="comment"># If we print the tensor, we can get:</span></span><br><span class="line">    <span class="comment"># tensor(334.4330, device='cuda:1') *********************, here is cuda:  cuda:1</span></span><br><span class="line">    <span class="comment"># tensor(359.1895, device='cuda:3') *********************, here is cuda:  cuda:3</span></span><br><span class="line">    <span class="comment"># tensor(263.3543, device='cuda:2') *********************, here is cuda:  cuda:2</span></span><br><span class="line">    <span class="comment"># tensor(340.1970, device='cuda:0') *********************, here is cuda:  cuda:0</span></span><br><span class="line">    rt = tensor.clone()  <span class="comment"># The function operates in-place.</span></span><br><span class="line">    dist.all_reduce(rt, op=dist.reduce_op.SUM)</span><br><span class="line">    rt /= args.world_size</span><br><span class="line">    <span class="keyword">return</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, start_epoch + <span class="number">100</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test(epoch)</span><br></pre></td></tr></table></figure>
<p>后面有时间的话再继续编辑，希望自己的总结可以帮助Pytorch新手们。如果我的具体代码或者具体的工作任务能够为大家提供一些帮助，那当然更加欢迎引用我的工作啦：</p>
<p>“Simple Pose: Rethinking and Improving a Bottom-up Approach for
Multi-Person Pose Estimation”</p>
<p><a
href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">https://arxiv.org/abs/1911.10529​arxiv.org</a></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
</search>
