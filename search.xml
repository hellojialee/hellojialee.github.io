<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Pytorch实用指南</title>
    <url>/2020/05/28/Pytorch%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/84784982" target="_blank" rel="noopener">Source</a></p>
<h1 id="网络模型构建">网络模型构建</h1>
<h2 id="nn.sequential和nn.modulelist的区别">1. nn.Sequential和nn.ModuleList的区别</h2>
<p>简而言之就是，nn.Sequential类似于Keras中的贯序模型，它是Module的子类，在构建数个网络层之后会自动调用forward()方法，从而有网络模型生成。而nn.ModuleList仅仅类似于pytho中的list类型，只是将一系列层装入列表，并没有实现forward()方法，因此也不会有网络模型产生的副作用。两者使用的一个很好的例子如链接：<a href="https://www.cnblogs.com/hellcat/p/8477195.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/hellcat/p/8477195.html</a></p>
<p>另外需要注意的是<strong>，网络中需要训练的参数一定要被正确地注册，比如如果使用了普通list, dict等，之后一定要用nn.Sequential或者nn.ModuleList包装一下；甚至在定义网络时，网络的一个attribute是一个list, list里面是一个或者多个子网络Module类别，也依然需要用nn.ModuleList替换掉这个普通的list，这样才能将模型参数和子网络模型参数顺利被优化器识别</strong>。否则，运行时不会报错，但是没有被注册的参数将不会被训练！并且，只有被正确注册之后，我们用model.cuda()，这些参数才会被自动迁移到GPU上，否则只会停留在CPU上。</p>
<h2 id="nn.modulelist可以由多维下标索引但用嵌套的list初始化时需注意">2. nn.ModuleList可以由多维下标索引，但用嵌套的list初始化时需注意</h2>
<p>注意： 比如下面self.outs定义了具有二维索引的modulelist，需要注意的是，内层list也要加nn.ModuleList包装，这样内层list内部就是可迭代的Module subclass对象， <strong>否则内层就是普通的list，不满足输入参数的类型要求</strong>，pytorch不能正确识别它们是可训练的模型参数，会报错。</p>
<pre><code>class PoseNet(nn.Module):
    def __init__(self, nstack, inp_dim, oup_dim, bn=False, increase=128, **kwargs):
        &quot;&quot;&quot; Pack or initialize the trainable parameters of the network&quot;&quot;&quot;
        super(PoseNet, self).__init__()
        self.pre = nn.Sequential(
            Conv(3, 64, 7, 2, bn=bn),
            Conv(64, 128, bn=bn),
            nn.MaxPool2d(2, 2))

        self.outs = nn.ModuleList(
            [nn.ModuleList([Conv(inp_dim, oup_dim, 1, relu=False, bn=False) for j in range(4)]) for i in range(nstack)])</code></pre>
<h1 id="网络结构可视化">网络结构可视化</h1>
<h2 id="网络结构可视化-1">1. 网络结构可视化</h2>
<pre><code>def make_dot(var, params=None):
    &quot;&quot;&quot; Produces Graphviz representation of PyTorch autograd graph
    Blue nodes are the Variables that require grad, orange are Tensors
    saved for backward in torch.autograd.Function
    Args:
        var: output Variable
        params: dict of (name, Variable) to add names to node that
            require grad (TODO: make optional)
    &quot;&quot;&quot;
    if params is not None:
        assert isinstance(params.values()[0], Variable)
        param_map = {id(v): k for k, v in params.items()}

    node_attr = dict(style=&#39;filled&#39;,
                     shape=&#39;box&#39;,
                     align=&#39;left&#39;,
                     fontsize=&#39;12&#39;,
                     ranksep=&#39;0.1&#39;,
                     height=&#39;0.2&#39;)
    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=&quot;12,12&quot;))
    seen = set()

    def size_to_str(size):
        return &#39;(&#39; + (&#39;, &#39;).join([&#39;%d&#39; % v for v in size]) + &#39;)&#39;

    def add_nodes(var):
        if var not in seen:
            if torch.is_tensor(var):
                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=&#39;orange&#39;)
            elif hasattr(var, &#39;variable&#39;):
                u = var.variable
                name = param_map[id(u)] if params is not None else &#39;&#39;
                node_name = &#39;%s\n %s&#39; % (name, size_to_str(u.size()))
                dot.node(str(id(var)), node_name, fillcolor=&#39;lightblue&#39;)
            else:
                dot.node(str(id(var)), str(type(var).__name__))
            seen.add(var)
            if hasattr(var, &#39;next_functions&#39;):
                for u in var.next_functions:
                    if u[0] is not None:
                        dot.edge(str(id(u[0])), str(id(var)))
                        add_nodes(u[0])
            if hasattr(var, &#39;saved_tensors&#39;):
                for t in var.saved_tensors:
                    dot.edge(str(id(t)), str(id(var)))
                    add_nodes(t)

    add_nodes(var.grad_fn)
    return dot</code></pre>
<p>使用以上代码的例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># plot the model</span><br><span class="line"># net &#x3D; PoseNet(nstack&#x3D;4, inp_dim&#x3D;256, oup_dim&#x3D;68)</span><br><span class="line"># x &#x3D; Variable(torch.randn(1, 3, 512, 512))  # x的shape为(batch，channels，height，width)</span><br><span class="line"># y &#x3D; net(x)</span><br><span class="line"># g &#x3D; make_dot(y)</span><br><span class="line"># g.view()</span><br></pre></td></tr></table></figure>
<h2 id="类似于keras-打印网络每层输出的形状shape">2. 类似于keras, 打印网络每层输出的形状shape</h2>
<p>更新：推荐使用增强版工具 <a href="https://github.com/nmhkahn/torchsummaryX" target="_blank" rel="noopener"><strong>torchsummaryX</strong></a>，它可以同时给出输出shape，参数数目，以及乘加运算数目等</p>
<p>Improved visualization tool of <a href="https://github.com/sksq96/pytorch-summary" target="_blank" rel="noopener">torchsummary</a>. Here, it visualizes kernel size, output shape, # params, and Mult-Adds. Also the torchsummaryX can handle RNN, Recursive NN, or model with multiple inputs.</p>
<hr />
<p>使用模仿keras中的summary()函数，<strong>torchsummary</strong> <a href="https://www.jianshu.com/p/97c626d33924" target="_blank" rel="noopener">转载自</a></p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">device &#x3D; torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # PyTorch v0.4.0</span><br><span class="line">model &#x3D; Net().to(device)</span><br><span class="line"></span><br><span class="line">summary(model, (1, 28, 28))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">            Conv2d-1           [-1, 10, 24, 24]             260</span><br><span class="line">            Conv2d-2             [-1, 20, 8, 8]           5,020</span><br><span class="line">         Dropout2d-3             [-1, 20, 8, 8]               0</span><br><span class="line">            Linear-4                   [-1, 50]          16,050</span><br><span class="line">            Linear-5                   [-1, 10]             510</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 21,840</span><br><span class="line">Trainable params: 21,840</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.00</span><br><span class="line">Forward&#x2F;backward pass size (MB): 0.06</span><br><span class="line">Params size (MB): 0.08</span><br><span class="line">Estimated Total Size (MB): 0.15</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="pytorch中layer的输出shape的尺寸取整">3. pytorch中layer的输出shape的尺寸取整</h2>
<p>默认使用的是向下取整(floor)，如：</p>
<pre><code>self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)  # (batch_size, 512, 38, 38)

# (H + 2*p - d(ks - 1) - 1) / 2 + 1
# (38 + 12 - 6*(3 - 1) -1 ) / 2 + 1 = 19.5 向下取整 19
self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # (batch_size, 1024, 19, 19)</code></pre>
<p>Maxpooling层也是默认使用向下取整。如果想使用向上取整(ceil)，需要设置取整模式 ceil_mode=True， 默认是False</p>
<pre><code>nn.MaxPool2d(kernel_size=2, stride=2),  # (batch_size, 256, 37, 37), 想变成38*38可以使用　ceil_mode=True</code></pre>
<h2 id="超级给力的网络结构可视化工具netron-和-hiddenlayer">4. 超级给力的网络结构可视化工具：Netron 和 hiddenlayer</h2>
<p>前者是一款在浏览器中使用的可视化工具，可以使用pip安装，然后在命令行中输入netron或者netron -b [model file]。需要把模型转换onnx模型。</p>
<pre><code>import torch.onnx

net = Hourglass2(2, 32, 1, Residual)
dummy_input = Variable(torch.randn(1, 32, 128, 128))
torch.onnx.export(net, dummy_input, &quot;model.onnx&quot;)</code></pre>
<p>后者是在jupyter notebook内使用的，例子如下：</p>
<p>Netron: <a href="https://github.com/lutzroeder/netron" target="_blank" rel="noopener" class="uri">https://github.com/lutzroeder/netron</a></p>
<p>hiddenlayer: <a href="https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb" target="_blank" rel="noopener">https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb</a></p>
<h2 id="计算网络模型的参数量和浮点运算数">5. 计算网络模型的参数量和浮点运算数</h2>
<p>使用第三方库thop</p>
<pre><code>from thop import profile
from thop import clever_format

dummy_input = torch.randn(1, 256, 128, 128)
flops, params = profile(MyNetwork, inputs=(dummy_input,))
flops, params = clever_format([flops, params], &quot;%.3f&quot;)
print(flops, params)</code></pre>
<h1 id="tensor的操作">Tensor的操作</h1>
<h2 id="tensor.view和tensor.permute-permute变换">１. Tensor.view和Tensor.permute (permute:变换)</h2>
<p>torch中的view类似与numpy中的reshape，但不同的是前者会与变换后的tensor共享内存，而后者不共享不会影响原始数组。</p>
<p>torch中的permute类似与numpy中的transpose. <strong>注意：</strong>view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。</p>
<p>一个在SSD中的例子：</p>
<pre><code> y_loc = self.loc_layers[i](x)
            batch_size = y_loc.size(0)  # int
            # 此处y_loc的shape是(batch_size, anchor*4, Hi, Wi), pytorch的数据结构为(N, C, H, W)
            y_loc = y_loc.permute(0, 2, 3, 1).contiguous()
            # 此处y_loc的shape是(batch_size, Hi, Wi, anchor*4)
            # 要先把4放到最后，然后再改变shape 变成 ##### (batch_size, anchor_all_number, 4) ######,  anchor_all_number代表anchor的总数
            # permute可以对任意高维矩阵进行转置. 但没有 torch.permute() 这个调用方式， 只能 Tensor.permute()。
            # view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。
            y_loc = y_loc.view(batch_size, -1, 4)</code></pre>
<h2 id="若前面有一个tensor输入需要梯度则后面的输出也需要梯度">２. 若前面有一个tensor输入需要梯度，则后面的输出也需要梯度</h2>
<pre><code>x = torch.zeros((1), requires_grad=True)
# 若前面有一个输入需要梯度，则后面的输出也需要梯度。有的版本这里是默认值false
# 注：　Tensor变量的requires_grad的属性默认为False,若一个节点requires_grad被设置为True，那么所有依赖它的节点的requires_grad都为True。</code></pre>
<h2 id="tensor之间要是同一个数据类型dtype才能运算因此有时需要进行类型转换">３. Tensor之间要是同一个数据类型<strong>dtype</strong>才能运算，因此有时需要进行类型转换</h2>
<p>比如即便都是int类型，但是一个是int16，一个是int32也需要先转换然后才能进行运算。使用Tensor.<code>to(torch.float32)进行转换。</code></p>
<pre><code># 因为loc_loss是float32，而num_matched_box是int64，没办法直接除所以转换一下
# 这里是不会损失数据的，因为假如batch_size=32,每个图片8732个，就只有8732*32=279424
# num_matched_boxes最大的值不会超过float32的表示范围的
num_matched_boxes = num_matched_boxes.to(torch.float32)  # Tensor dtype and/or device 转换
loc_loss /= num_matched_boxes   # 除以的是正样本的数目</code></pre>
<h2 id="tensor的clone和copy_的区别">４. Tensor的clone和copy_的区别：</h2>
<p>copy_()不会追踪梯度，而clone会追踪并进行梯度的反向传播</p>
<p>Unlike copy_(), clone is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</p>
<h2 id="tensor初始化">５. Tensor初始化</h2>
<h3 id="a.-torch.tensor和torch.from_numpy效果不同">a. torch.tensor和torch.from_numpy()效果不同</h3>
<p>torch.tensor会重新拷贝原始数据，返回新的数据。如果不想拷贝，即内存相关联，对numpy array来说可以使用torch.from_numpy()。</p>
<p>可以直接用list数据进行初始化，并且对list中某一个元素是tuple还是list都无所谓，如：</p>
<p>x= [(1,2,3,4), [5,6,7,8]] # x[0]是tuple而x[1]是list torch.tensor(x) Out[20]: tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8]])</p>
<h2 id="data和detach的区别">６. data和detach()的区别</h2>
<p>推荐使用detach()，这样万一需要在反向传播时需要记录变量，可以报错指出，避免Tensor.data没有报错，但是计算错误的情况。</p>
<p><a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/38475183</a></p>
<blockquote>
<p><em>"However, .data can be unsafe in some cases. Any changes on x.data wouldn’t be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward."</em></p>
</blockquote>
<p><strong>Any in-place change on x.detach() will cause errors when x is needed in backward, so .detach() is a safer way for the exclusion of subgraphs from gradient computation. <a href="https://github.com/pytorch/pytorch/issues/6990" target="_blank" rel="noopener" class="uri">https://github.com/pytorch/pytorch/issues/6990</a></strong></p>
<h2 id="pytorch中损失函数对tensor操作的reducesize_average参数说明">7. pytorch中损失函数对tensor操作的reduce,size_average参数说明</h2>
<p>参考：<a href="https://blog.csdn.net/u013548568/article/details/81532605" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u013548568/article/details/81532605</a></p>
<p>以及 <a href="https://zhuanlan.zhihu.com/p/91485607" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>size_average是说是不是对一个batch里面的所有的数据求均值</p>
<hr />
<p><strong>Reduce </strong> <strong>size_average </strong> * 意义* True True 对batch里面的数据取均值loss.mean() True False 对batch里面的数据求和loss.sum() False – returns a loss per batch element instead, 这个时候忽略size_average参数</p>
<hr />
<p>reduction : 可选的参数有：‘none’ | ‘elementwise_mean’ | ‘sum’, 正如参数的字面意思</p>
<hr />
<p>假设输入和target的大小分别是NxCxWxH，那么一旦reduce设置为False，loss的大小为NxCxWxH，返回每一个元素的loss</p>
<p><strong>reduction代表了上面的reduce和size_average双重含义，这也是文档里为什么说reduce和size_average要被Deprecated 的原因</strong></p>
<p>例子：</p>
<pre><code>import torch
import torch.nn as nn

# ----------------------------------- MSE loss

# 生成网络输出 以及 目标输出
output = torch.ones(2, 2, requires_grad=True) * 0.5
target = torch.ones(2, 2)

# 设置三种不同参数的L1Loss
reduce_False = nn.MSELoss(size_average=True, reduce=False) # 等效于reduction=&#39;none&#39;
size_average_True = nn.MSELoss(size_average=True, reduce=True) # 等效于reduction=&#39;mean&#39;
size_average_False = nn.MSELoss(size_average=False, reduce=True) # 等效于reduction=&#39;sum&#39;

o_0 = reduce_False(output, target)
o_1 = size_average_True(output, target)
o_2 = size_average_False(output, target)

print(&#39;\nreduce=False, 输出同维度的loss:\n{}\n&#39;.format(o_0))
print(&#39;size_average=True，\t求平均:\t{}&#39;.format(o_1))
print(&#39;size_average=False，\t求和:\t{}&#39;.format(o_2))</code></pre>
<p>输出：</p>
<pre><code>reduce=False, 输出同维度的loss:
tensor([[0.2500, 0.2500],
        [0.2500, 0.2500]], grad_fn=&lt;MseLossBackward&gt;)

size_average=True，  求平均:    0.25

size_average=False， 求和: 1.0</code></pre>
<h2 id="将tensor以及model迁移至cuda上">8. 将tensor以及model迁移至cuda上</h2>
<p><strong>将数据迁移到cuda上必须reassign，tensor.cuda()不是in-place操作，而是返回一个新的在cuda上的tensor。而网络模型不需要reassign.</strong></p>
<h3 id="a.-迁移tensor">a. 迁移tensor</h3>
<p><strong>问题：</strong>Hi, this works, <code>a = torch.LongTensor(1).random_(0, 10).to("cuda")</code>. but this won’t work:</p>
<p><strong>回答：</strong></p>
<p>If you are pushing tensors to a device or host, <strong>you have to reassign them:</strong></p>
<pre><code>a = a.to(device=&#39;cuda&#39;)</code></pre>
<h3 id="b.-迁移模型">b. 迁移模型</h3>
<p><code>nn.Module</code>s push all parameters, buffers and submodules recursively and don’t need the assignment.</p>
<blockquote>
<p>model.cuda()</p>
</blockquote>
<h2 id="对feature-map-即也是tensor做尺寸上的缩放">9. 对feature map (即也是tensor)做尺寸上的缩放</h2>
<blockquote>
<p><code>torch.nn.functional.``interpolate</code>(<em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em>)</p>
</blockquote>
<p>默认的<em>align_corners=None就是和Opencv中的缩放规则保持一致，默认使用几何中心对齐，以此消除量化误差（或者说</em>计算出的灰度值也相对于源图像偏左偏上）<em>。</em></p>
<p>若做缩放，需要在缩放后图像 的位置上找到对应的 原始图像位置上 的像素值，有以下</p>
<p>SrcX=(dstX+0.5)* (srcWidth/dstWidth) -0.5 SrcY=(dstY+0.5) * (srcHeight/dstHeight)-0.5</p>
<p>具体参考我的另一篇博客：</p>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/100150726" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xiaojiajia007/article/details/100150726</a></p>
<h2 id="注册参数--模型的普通类成员变量和pytorch中自动注册的parameter或者buffer区别">10. 注册参数--模型的普通类成员变量和Pytorch中自动注册的Parameter或者buffer区别</h2>
<p><a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/89442276</a></p>
<p>同时参考 第一节#网络模型构建中nn.ModuleList</p>
<p>模型中需要保存下来的参数包括两种:</p>
<p>一种是反向传播需要被optimizer更新的，称之为 parameter 一种是反向传播不需要被optimizer更新，称之为 buffer，它只能在forward中被更新。</p>
<p>第一种参数我们可以通过 model.parameters() 返回；第二种参数我们可以通过 model.buffers() 返回。因为我们的模型保存的是 state_dict 返回的 OrderDict，所以这两种参数不仅要满足是否需要被更新的要求，还会被保存到OrderDict。而<strong>普通的类成员变量属性是无法自动保存到模型的 OrderDict中去的。</strong></p>
<p>模型进行设备移动时，模型中注册的参数(Parameter和buffer)会同时进行移动，比如使用model.cuda()之后注册的参数parameter和buffer会自动迁移到cuda上去，<strong>而普通成员变量不会自动设备移动</strong>。</p>
<h1 id="pytorch训练数据准备">pytorch训练数据准备</h1>
<h2 id="dataloader-类">1. DataLoader 类</h2>
<h3 id="参数说明-摘录自">参数说明 <a href="https://blog.csdn.net/weixin_42236288/article/details/80893882%C2%A0" target="_blank" rel="noopener">摘录自</a></h3>
<p>1. dataset：加载的数据集(Dataset对象) 2. batch_size：batch size 3. shuffle:：是否将数据打乱 4. sampler： 样本抽样，后续会详细介绍 5. num_workers：使用多进程加载的进程数，0代表不使用多进程 6. collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可 7. pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些 8. drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</p>
<h3 id="对于-pin_memory-的解释摘录自">对于 pin_memory 的解释：<a href="https://oldpan.me/archives/pytorch-to-use-multiple-gpus" target="_blank" rel="noopener">摘录自</a></h3>
<p><strong>pin_memory就是锁页内存</strong></p>
<blockquote>
<p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。 主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。显卡中的显存全部是锁页内存,当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>
</blockquote>
<h2 id="多进程读取hdf5文件支持的不好以及解决办法">2. 多进程读取HDF5文件支持的不好以及解决办法</h2>
<p>DataLoader中多进程高效处理hdf5文件：</p>
<p><a href="https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643" target="_blank" rel="noopener">摘录自</a></p>
<p><strong>My recommendations:</strong></p>
<blockquote>
<ul>
<li>Use HDF5 in version 1.10 (better multiprocessing handling),</li>
<li>Because an opened HDF5 file isn’t pickleable and to send Dataset to workers’ processes it needs to be serialised with pickle, you can’t open the HDF5 file in <code>__init__</code>. Open it in <code>__getitem__</code>and <strong>store as the singleton!</strong>. Do not open it each time as it introduces huge overhead.</li>
<li>Use <code>DataLoader</code> with <code>num_workers</code> &gt; 0 (reading from hdf5 (i.e. hard drive) is slow) and <code>batch_sampler</code> (random access to hdf5 (i.e. hard drive) is slow).</li>
</ul>
</blockquote>
<p><strong>Sample code:</strong></p>
<pre><code>class H5Dataset(torch.utils.data.Dataset):
    def __init__(self, path):
        self.file_path = path
        self.dataset = None
        with h5py.File(self.file_path, &#39;r&#39;) as file:
            self.dataset_len = len(file[&quot;dataset&quot;])

    def __getitem__(self, index):
        if self.dataset is None:
            self.dataset = h5py.File(self.file_path, &#39;r&#39;)[&quot;dataset&quot;]
        return self.dataset[index]

    def __len__(self):
        return self.dataset_len</code></pre>
<p><strong>如何安装HDF5 1.10以及对应的python hdf5的包呢？ 查看<a href="https://blog.csdn.net/xiaojiajia007/article/details/87873443" target="_blank" rel="noopener">我的另一个博客</a></strong></p>
<p><strong>使用命令行环境变量HDF5_DIR=/usr/local/hdf5 pip install h5py。具体如下：</strong></p>
<p>Then you should be fine. Install HDF5 1.10 from source into somewhere you want to. The .tar is here: https://www.hdfgroup.org/HDF5/release/obtainsrc5110.html Follow the install readme but basically you just need to give it a directory with: &gt; ./configure --prefix=/usr/local/h5py before you make.</p>
<p>Now install with you anaconda version of python. You may want to make a separate environment using conda but that's your call.</p>
<p>Remove the h5py you have with anaconda using &gt; conda uninstall h5py or &gt; pip uninstall h5py</p>
<p>Then use pip to reinstall h5py but pointing to the HDF5 library you made from source. From here: http://docs.h5py.org/en/latest/build.html <strong>&gt; HDF5_DIR=/usr/local/hdf5 pip install h5py</strong></p>
<p>Then you should be good. Open up a python terminal and test if you can use SWMR mode: &gt; import h5py &gt; f = h5py.File("./swmr.h5", 'a', libver='latest', swmr=True)</p>
<h2 id="多进程准备数据随机种子seed的问题">3. 多进程准备数据<strong>随机种子seed</strong>的问题</h2>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/87881231" target="_blank" rel="noopener">参见我另一个博客</a></p>
<h2 id="如何加速训练数据准备并载入gpu训练">4. 如何加速训练数据准备并载入GPU训练</h2>
<p>参考一个知乎博客，data_prefetcher： <a href="https://zhuanlan.zhihu.com/p/80695364" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/80695364</a></p>
<p>以及Pytorch论坛上的一个讨论： <a href="https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee" target="_blank" rel="noopener">https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19?u=jia_lee</a></p>
<h1 id="pytorch训练阶段">Pytorch训练阶段</h1>
<h2 id="stochastic-weight-averaging-in-pytorch">1. Stochastic Weight Averaging in PyTorch</h2>
<p>这是一种model weight average策略，类似于模型集成，常常用来刷指标，提高模型的泛化精度。详细说明请见我的单独博客：</p>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/90748115" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90748115</a></p>
<h2 id="通过梯度积累变相增大batch-size">2. 通过梯度积累变相增大batch size</h2>
<p><a href="https://www.zhihu.com/question/303070254/answer/573037166" target="_blank" rel="noopener">详情请见 PyTorch中在反向传播前为什么要手动将梯度清零？ - Pascal的回答 - 知乎</a> 但是需要注意的是，因为BN层的参数是在 forward()阶段更新的，这样积累梯度并没有增大BN layers的实际batch size。可以通过减少BN层的 momentum 值，让BN层动态更新统计参数时能够记住更长。</p>
<h1 id="pytorch-测试阶段">Pytorch 测试阶段</h1>
<h2 id="正确的测试预测时间计时代码">1. 正确的测试（预测）时间计时代码</h2>
<pre><code>torch.cuda.synchronize() # 等待当前设备上所有流中的所有核心完成
start = time.time() 
result = model(input) 
torch.cuda.synchronize() 
end = time.time()</code></pre>
<p>在pytorch里面，程序的执行都是异步的。如果没有torch.cuda.synchronize() ，测试的时间会很短，因为执行完end=time.time()程序就退出了，后台的cu也因为python的退出退出了，如果采用torch.cuda.synchronize() ，代码会同步cu的操作，等待gpu上的操作都完成了再继续成形end = time.time() 原文：https://blog.csdn.net/u013548568/article/details/81368019</p>
<h2 id="训练测试两个阶段需要注意设置不同状态-参考">2. 训练，测试两个阶段需要注意设置不同状态 <a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/10" target="_blank" rel="noopener">参考</a></h2>
<h3 id="a.-model.train和model.val">a. model.train()和model.val()</h3>
<p>比如BN和Dropout</p>
<p>During eval <code>Dropout</code> is deactivated and just passes its input. During the training the probability <code>p</code> is used to drop activations. Also, the activations are scaled with <code>1./p</code> as otherwise the expected values would differ between training and eval.</p>
<pre><code>drop = nn.Dropout()
x = torch.ones(1, 10)

# Train mode (default after construction)
drop.train()
print(drop(x))

# Eval mode
drop.eval()
print(drop(x))</code></pre>
<h3 id="b.-测试val时不光要设置model.eval-为了防止内存爆炸应该追加torch.no_grad">b. 测试（val)时不光要设置<code>model.eval()</code> ，为了防止内存爆炸，应该追加<code>torch.no_grad()</code></h3>
<ul>
<li><code>model.eval()</code> will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.</li>
<li><code>torch.no_grad():</code> impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script). 注意，<code>torch.no_grad()是</code>context manager。</li>
</ul>
<h2 id="dropout里需要设置训练标志位否则会踩坑">3. Dropout里需要设置训练标志位，否则会踩坑</h2>
<h3 id="使用f.dropout-nn.functional.dropout-的时候需要设置它的可选参数training-state">使用F.dropout ( nn.functional.dropout )的时候需要设置它的可选参数training state</h3>
<p>这个状态参数与模型整体的一致，否则就是out=out，没有效果，具体说明见链接 <a href="https://www.zhihu.com/question/67209417/answer/302434279" target="_blank" rel="noopener">查看</a></p>
<pre><code>Class DropoutFC(nn.Module):
   def __init__(self):
       super(DropoutFC, self).__init__()
       self.fc = nn.Linear(100,20)

   def forward(self, input):
       out = self.fc(input)
       out = F.dropout(out, p=0.5, training=self.training) # set dropout&#39;s training sate
       return out

Net = DropoutFC()
Net.train()

# train the Net
#作者：雷杰
#链接：https://www.zhihu.com/question/67209417/answer/302434279</code></pre>
<h3 id="或者直接使用nn.dropout即利用包装后的layer">或者直接使用nn.Dropout()，即利用包装后的layer</h3>
<p>nn.Dropout()实际上是对F.dropout的一个包装, 也将self.training传入了)</p>
<pre><code>Class DropoutFC(nn.Module):
  def __init__(self):
      super(DropoutFC, self).__init__()
      self.fc = nn.Linear(100,20)
      self.dropout = nn.Dropout(p=0.5)

  def forward(self, input):
      out = self.fc(input)
      out = self.dropout(out)
      return out
Net = DropoutFC()
Net.train()

# train the Net</code></pre>
<h2 id="多gpu模型权重的保存与加载">4. 多GPU模型权重的保存与加载</h2>
<p>Instead of deleting the “module.” string from all the state_dict keys, you can save your model with: <code>torch.save(model.module.state_dict(), path_to_file)</code> instead of <code>torch.save(model.state_dict(), path_to_file)</code> <strong><em>that way you don’t get the “module.” string to begin with…</em></strong></p>
<pre><code># original saved file with DataParallel
state_dict = torch.load(&#39;myfile.pth.tar&#39;)
# 把所有的张量加载到CPU中
# torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage)

# create new OrderedDict that does not contain `module.`
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
model.load_state_dict(new_state_dict)

############## 还有一个可用的封装更好的函数
# 加载模型，解决命名和维度不匹配问题,解决多个gpu并行
def load_state_keywise(model, model_path):
    model_dict = model.state_dict()
    pretrained_dict = torch.load(model_path, map_location=&#39;cpu&#39;)
    key = list(pretrained_dict.keys())[0]
    # 1. filter out unnecessary keys
    # 1.1 multi-GPU -&gt;CPU
    if (str(key).startswith(&#39;module.&#39;)):
        pretrained_dict = {k[7:]: v for k, v in pretrained_dict.items() if
                           k[7:] in model_dict and v.size() == model_dict[k[7:]].size()}
    else:
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           k in model_dict and v.size() == model_dict[k].size()}
    # 2. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)
    # 3. load the new state dict
    model.load_state_dict(model_dict)

 ################## 更简单直接的方式 ##################
# Instead of deleting the “module.” string from all the state_dict keys, you can save your model with:

torch.save(model.module.state_dict(), path_to_file)
# instead of

torch.save(model.state_dict(), path_to_file)

# that way you don’t get the “module.” string to begin with…</code></pre>
<h2 id="恢复保存的优化器状态optimizer-checkpoint-resume继续优化">5. 恢复保存的优化器状态(optimizer checkpoint resume)，继续优化</h2>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/88417329" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xiaojiajia007/article/details/88417329</a></p>
<h2 id="载入模型权重gpu内存被额外占用的bug解决">6. 载入模型权重GPU内存被额外占用的bug解决</h2>
<h3 id="分布式多进程中的这种情况的现象是对于同一个python进程pid号相同会两次占用固定大小的gpu内存">分布式/多进程中的这种情况的现象是，对于同一个python进程（pid号相同）会两次占用固定大小的gpu内存</h3>
<p>观察到的现象是python进程多于预期应有的进程数。比如我们单机多卡分布式训练，已经完成了网络模型的in-place参数设备转换，即network.cuda()，现在我们有4块GPU，我们在程序中的每一个进程分配一块GPU时本来应该只有4个进程，每个进程占用一定的GPU显存，但实际情况如所示：</p>
<pre><code>Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 1291 G /usr/lib/xorg/Xorg 153MiB |
| 0 2549 G fcitx-qimpanel 14MiB |
| 0 21740 G compiz 138MiB |
| 0 22840 C /home/jia/.virtualenvs/phoenix/bin/python 6097MiB | 
| 0 22841 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22842 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 22843 C /home/jia/.virtualenvs/phoenix/bin/python 859MiB | # 本不该出现
| 0 23207 G /opt/teamviewer/tv_bin/TeamViewer 24MiB |
| 0 23985 G .../Software/pycharm-2019.2.4/jbr/bin/java 12MiB |
| 1 22841 C /home/jia/.virtualenvs/phoenix/bin/python 6129MiB |
| 2 22842 C /home/jia/.virtualenvs/phoenix/bin/python 6227MiB |
| 3 22843 C /home/jia/.virtualenvs/phoenix/bin/python 6229MiB</code></pre>
<p>原因：在同一个cuda上之后不使用的内存将会被自动销毁并回收，但是对于不同GPU之间目前没有自动的内存管理机制??，如果某一个进程在cuda0上实例化的tensor x，在另一个使用cuda2的进程中使用了，但cuda2上的进程并没有对tensor x进行内存销毁回收，造成GPU内存的占用。</p>
<p>解决办法：在当前进程中销毁不在同一个cuda上的内存垃圾，或者载入权重时使用torch.load(model_path, <strong>map_location='cpu'</strong>)</p>
<h3 id="gpu预训练保存的权值可以直接载入到cpu下的网络模型network中并且载入之后network的参数会移到预训练权值所在的device上">GPU预训练保存的权值可以直接载入到CPU下的网络模型network中，并且载入之后network的参数会移到预训练权值所在的device上</h3>
<p>如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20200108114844390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" /></p>
<p>这个行为挺诡异，按照正常的设计逻辑，本来CPU的模型直接载入GPU预训练权值应该会因为device不同而报错（cpu, cuda0)但结果并没有，可以成功载入，并且载入之后CPU下的模型network的device也变成cuda0了。甚至我们可以仅仅载入某一layer的权值，那么这一layer的weight.data将变到cuda0上，而其没有载入更改的layer的weight.data仍然在cpu上！</p>
<p>解决办法同上一种情况，把GPU预训练权值map到cpu上之后再network.load_state_dict()。</p>
<h1 id="pytorch的内存优化和加速">Pytorch的内存优化和加速</h1>
<p><strong>有一个 pytorch提速指南： <a href="https://zhuanlan.zhihu.com/p/39752167" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/39752167</a></strong></p>
<p><strong>可以参考 <a href="https://blog.csdn.net/jacke121/article/details/81329679%C2%A0" target="_blank" rel="noopener">原文</a></strong></p>
<h2 id="使用inplace减少内存开辟从而压缩内存需求">1. 使用inplace减少内存开辟，从而压缩内存需求</h2>
<p>对于in-place operation的解读，见：<a href="https://blog.csdn.net/u012436149/article/details/80819523" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012436149/article/details/80819523</a></p>
<p>以及：<a href="https://blog.csdn.net/york1996/article/details/81835873" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/york1996/article/details/81835873</a></p>
<p>如，ReLu(inplace=True)</p>
<p>在官方问文档中由这一段话：</p>
<blockquote>
<p>如果你使用了in-place operation而没有报错的话，那么你可以确定你的梯度计算是正确的。<strong>因为Pytorch在内存占用和执行速度上做了很多算法优化，哪些需要保留梯度不能使用in-place覆盖就显得不那么显而易见了，不能单纯地用原始梯度反向传播过程来决定。</strong></p>
</blockquote>
<p>inplace只是可以节省存储tensor的内存，但是PYTORCH中的自动微分机制仍然能够追踪，对于内存来说inplace可能是同一个对象，但是对于autograd来说，依然是两个不同的对象。 一个例子：<a href="https://discuss.pytorch.org/t/why-relu-inplace-true-does-not-give-error-in-official-resnet-py-but-it-gives-error-in-my-code/21004/3" target="_blank" rel="noopener">resnet</a></p>
<blockquote>
<p><strong><code>inplace</code> means that it will not allocate new memory and change tensors inplace</strong>. <strong>But from the autograd point of view, you have two different tensors (even though they actually share the same memory)</strong>. One is the output of conv (or batchnorm for resnet) and one is the output of the relu.</p>
</blockquote>
<h2 id="torch.backends.cudnn.benchmark-true">2. torch.backends.cudnn.benchmark = True</h2>
<p>在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</p>
<h2 id="torch.cuda.empty_cache">3. torch.cuda.empty_cache()</h2>
<p>因为每次迭代都会引入点临时变量，会导致训练速度越来越慢，基本呈线性增长。开发人员还不清楚原因，但如果周期性的使用torch.cuda.empty_cache()的话就可以解决这个问题。</p>
<h2 id="使用checkpoint分阶段计算这样可以在显卡上放下更大的网络">4. 使用checkpoint分阶段计算，这样可以在显卡上放下更大的网络</h2>
<p>知乎回答的一个例子：https://www.zhihu.com/question/274635237/answer/574193034</p>
<h2 id="尝试nvidia-apex-16位浮点数扩展">5. 尝试Nvidia Apex 16位浮点数扩展</h2>
<p>温馨提示：我的另一篇博客<a href="https://mp.csdn.net/console/editor/html/84784982" target="_blank" rel="noopener">pip install, python setup.py, egg-info的说明--以Nvidia Apex安装为例</a></p>
<h3 id="clean-the-old-install-before-rebuilding">Clean the old install before rebuilding:</h3>
<blockquote>
<p>pip uninstall apex cd apex_repo_dir rm -rf build (if present) rm -rf apex.egg-info (if present)</p>
</blockquote>
<h3 id="install-package">Install package：</h3>
<blockquote>
<p>pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./</p>
<p># --no-cache-dir 清除安装缓存文件</p>
</blockquote>
<p>或者</p>
<blockquote>
<p>python setup.py install --cuda_ext --cpp_ext</p>
</blockquote>
<h3 id="ps-如果遇到cuda版本不兼容的问题解决办法见若pytorch升级到1.3cuda10.1则没有这个error了">ps: 如果遇到Cuda版本不兼容的问题，解决办法见：（若pytorch升级到1.3，cuda10.1则没有这个error了）</h3>
<p><a href="https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952" target="_blank" rel="noopener">https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952</a></p>
<p>如果没有出现其他error，可以直接使用上面链接的建议，删除版本检查抛出的报错。</p>
<p>讨论：</p>
<p><a href="https://github.com/NVIDIA/apex/issues/350" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/issues/350</a></p>
<p><a href="https://github.com/NVIDIA/apex/pull/323" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/pull/323</a></p>
<h2 id="pytorch内存泄露僵尸进程解决办法-原文链接">6. Pytorch内存泄露（僵尸进程）解决办法 <a href="https://blog.csdn.net/liuyifang0810680/article/details/79628394%C2%A0" target="_blank" rel="noopener">原文链接</a></h2>
<p>nvidia-smi 发现内存泄露问题，即没有进程时，内存被占用</p>
<blockquote>
<p>fuser -v /dev/nvidia* 发现僵尸进程</p>
<p>ps x |grep python|awk '{print $1}'|xargs kill 杀死所有僵尸进程</p>
</blockquote>
<p>命令解读：</p>
<p>ps x: show all process of current user</p>
<p>grep python: to get process that has python in command line</p>
<p>awk '{print $1}': to get the related process pidxargs kill`: to kill the process</p>
<p>note: make sure you don’t kill other processes! do ps x |grep python first.</p>
<h2 id="相关的进程和内存管理bash-cmd-命令行命令">7. 相关的进程和内存管理bash cmd (命令行命令）</h2>
<p>nvidia-smi -l xxx 监控GPU，动态刷新信息（默认5s刷新一次），按Ctrl+C停止，可指定刷新频率，以秒为单位；</p>
<p>watch -n 1 nvidia-smi <strong>实时监控GPU</strong>； watch -n 1 lscpu实时监控CPU，watch是周期性的执行下个程序 ps -elf进程查看， <strong>ps -elf | grep python 查看Python子进程</strong>，这个也是命令比较实用，能够用在监视其他基于python解释器运行的进程， kill -9 [PID]杀死进程PID。</p>
<blockquote>
<blockquote>
<p><strong>watch -n 5 -t -d=cumulative 'command'</strong></p>
</blockquote>
<p>watch是周期性的执行下个程序，并全屏显示执行结果</p>
<p>-n 每隔5秒周期执行一次</p>
<p>-t 开头的间隔时间和信息等不显示</p>
<p><strong>-d=cumulative 发生变动的地方高亮</strong></p>
</blockquote>
<h2 id="如何才能使用-tensor-core">8. 如何才能使用 Tensor Core</h2>
<p><strong>Convolutions</strong>: For cudnn versions 7.2 and ealier, <span class="citation" data-cites="vaibhav0195">@vaibhav0195</span> is correct: input channels, output channels, and batch size should be multiples of 8 to use tensor cores. However, this requirement is lifted for cudnn versions 7.3 and later. <strong>For cudnn 7.3 and later, you don't need to worry about making your channels/batch size multiples of 8 to enable Tensor Core use</strong>.</p>
<p><strong>GEMMs (fully connected layers)</strong>: For matrix A x matrix B, where A has size [I, J] and B has size [J, K], I, J, and K must be multiples of 8 to use Tensor Cores. This requirement exists for all cublas and cudnn versions. This means that for <strong>bare fully connected layers, the batch size, input features, and output features must be multiples of 8</strong>, and** for RNNs, you usually (but not always, it can be architecture-dependent depending on what you use for encoder/decoder) need to have batch size, hidden size, embedding size, and dictionary size as multiples of 8.**</p>
<h2 id="apex的fused-adam的特点是模型参数更新迭代得比pytorch中原生的adam快"><strong>9. Apex的Fused Adam的特点是模型参数更新迭代得比Pytorch中原生的Adam快</strong></h2>
<p>What is the difference between FusedAdam optimizer in Nvidia AMP package with the Adam optimizer in Pytorch?</p>
<p><a href="https://discuss.pytorch.org/t/fusedadam-optimizer-in-nvidia-amp-package/47544" target="_blank" rel="noopener">摘录自</a></p>
<blockquote>
<p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries out optimizer.step() by looping over parameters, and launching a series of kernels for each parameter. This can require hundreds of small launches that are mostly bound by CPU-side Python looping and kernel launch overhead, resulting in poor device utilization. Currently, the FusedAdam implementation in Apex flattens the parameters for the optimization step, then carries out the optimization step itself via a fused kernel that combines all the Adam operations. In this way, the loop over parameters as well as the internal series of Adam operations for each parameter are fused such that optimizer.step() requires only a few kernel launches.</p>
<p>The current implementation (in Apex master) is brittle and only works with Amp opt_level O2. I’ve got a WIP branch to make it work for any opt_level (<a href="https://github.com/NVIDIA/apex/pull/351" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/pull/351</a>). I recommend waiting until this is merged then trying it.</p>
</blockquote>
<h1 id="pytorch-使用陷阱易错点"><strong>Pytorch 使用陷阱，易错点</strong></h1>
<h2 id="tensor.expand-expand_as是共享内存的只是原始数据的一个视图-view并没有在扩展的axis上有新的数据复制牵一发动全身"><strong>1. Tensor.expand, expand_as是共享内存的，只是原始数据的一个视图 view，并没有在扩展的axis上有新的数据复制，牵一发动全身！</strong></h2>
<p><strong>为了避免对 expand() 后对某个channel操作会影响原始tensor的全部元素，需要使用clone()</strong></p>
<p>如果没有clone()，对mask_miss的某个通道赋值后，所有通道上的tensor都会变成1！</p>
<blockquote>
<p># Notice! expand does not allocate more memory but just make the tensor look as if you expanded it. # You should call .clone() on the resulting tensor if you plan on modifying it # https://discuss.pytorch.org/t/very-strange-behavior-change-one-element-of-a-tensor-will-influence-all-elements/41190</p>
</blockquote>
<pre><code>mask = mask_miss.expand_as(sxing).clone()            # type: torch.Tensor
mask[:, :, -2, :, :] = 1   # except for person mask channel</code></pre>
<h2 id="损失计算图因为pytorch的动态机制越来越大直到耗尽内存">2. 损失计算图因为Pytorch的动态机制越来越大，直到耗尽内存</h2>
<p>摘录自</p>
<p>常见的原因有</p>
<h3 id="在循环中使用全局变量当做累加器且累加梯度信息">在循环中使用全局变量当做累加器，且累加梯度信息</h3>
<p>举个例子，下面的代码中</p>
<pre><code>total_loss=0
for i in range(10000):
  optimizer.zero_grad()
  output=model(input)
  loss=criterion(output)
  loss.backward() # 计算的梯度自动叠加到各个权重的grad上，并且计算完成后销毁计算图！！！
  optimizer.step()
  total_loss+=loss
  #这里total_loss是跨越循环的变量，起着累加的作用，
  #loss变量是带有梯度的tensor，会保持历史梯度信息，在循环过程中会不断积累梯度信息到tota_loss，占用内存</code></pre>
<p>以上例子的修正方法是在循环中的最后一句修改为：</p>
<p>total_loss+=float(loss)</p>
<p>或者 total_loss += loss.item() # tensor.item()是取张量的python数值</p>
<p>利用类型变换解除梯度信息，这样，多次累加不会累加梯度信息。</p>
<h3 id="局部变量逗留导致内存泄露">局部变量逗留导致内存泄露</h3>
<p>局部变量通常在变量作用域之外会被Python自动销毁，在作用域之内，不需要的临时变量可以使用del x来销毁。</p>
<h3 id="list数据类型不断append增长了计算图大小">list数据类型，不断append增长了计算图大小</h3>
<h2 id="pytorch中的batch-normalization-layer踩坑">3. Pytorch中的Batch Normalization layer踩坑</h2>
<p>详情查看我的另一篇博客：<a href="https://blog.csdn.net/xiaojiajia007/article/details/90115174" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xiaojiajia007/article/details/90115174</a></p>
<h2 id="优化器的权值衰减weight_decay项导致的隐蔽bug即是的网络的权值不断减少至0">4. 优化器的权值衰减weight_decay项导致的隐蔽bug，即是的网络的权值不断减少至0</h2>
<p>摘录自：<a href="https://zhuanlan.zhihu.com/p/91485607" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/91485607</a></p>
<p>我们都知道weight_decay指的是权值衰减，（<strong>注意：权值衰减不等价于在原损失的基础上加上一个L2惩罚项！具体说明见下面那条笔记</strong>），使得模型趋向于选择更小的权重参数，起到正则化的效果。但是我经常会忽略掉这一项的存在，从而引发了意想不到的问题。</p>
<p>这次的坑是这样的，在训练一个ResNet50的时候，网络的高层部分layer4暂时没有用到，因此也并不会有梯度回传，于是我就放心地将ResNet50的所有参数都传递给Optimizer进行更新了，想着layer4应该能保持原来的权重不变才对。但是实际上，尽管layer4没有梯度回传，但是weight_decay的作用仍然存在，它使得layer4权值越来越小，趋向于0。后面需要用到layer4的时候，发现输出异常（接近于0），才注意到这个问题的存在。</p>
<p>虽然这样的情况可能不容易遇到，但是还是要谨慎：暂时不需要更新的权值，一定不要传递给Optimizer，避免不必要的麻烦。</p>
<h2 id="l2正则不等于权值衰减">5. L2正则不等于权值衰减</h2>
<p>权值衰减（Weight Decay）：在网络权值通过损失函数更新后，直接再减去权值本身的一个倍数，可以写成 W(t+1)’ = W(t+1)-W(t)；</p>
<p>而 L2正则（L2 Regulation）：在原有的算是函数基础上，添加了网络权值平方和*一个倍数，L' = L+1/2∑w^2，注意在参数更新，对L'求关于某个分量的导数时其他参数视作常数，导数为0。</p>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/104045066" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xiaojiajia007/article/details/104045066</a></p>
<p><img src="https://img-blog.csdnimg.cn/20200119205411821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" /></p>
<p>在Pytorch中，对于SGD优化器，两者是等效的，但是对于Adam优化器，两者作用有差别，对于Adam会有耦合的错误。</p>
<p>我看到有的开源项目中(<a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/utils/utils.py#L60" target="_blank" rel="noopener">链接</a>)，SGD使用weight decay，而Adam中没有使用weight decay。</p>
<p>具体分析见下面两个文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/40814046</a>，</p>
<p><a href="https://zhuanlan.zhihu.com/p/63982470" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/63982470</a></p>
<h2 id="torch.sqrt在0处的左导数没有定义会返回nan换用-torch.norm">6. torch.sqrt()在0处的左导数没有定义，会返回nan，换用 torch.norm()</h2>
<p>例如： # https://github.com/pytorch/pytorch/issues/2421 # norm = torch.sqrt((x1 - t1)**2 + (x2 - t2)**2)</p>
<p><code>norm = (torch.stack((x1, x2)) - torch.stack((t1, t2))).norm(dim=0)</code></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>python</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>英语写作</title>
    <url>/2020/05/15/%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E6%98%93%E9%94%99%E7%82%B9/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/77971927" target="_blank" rel="noopener" title="Permalink to 英语写作_论文,写作_xiaojiajia007的博客-CSDN博客">Source</a></p>
<h2 id="and-but作连词-什么时候加逗号">1. and, but作连词 什么时候加逗号</h2>
<p><strong>1). and连接两个以上的单词、词组或子句；连接两个相同的单词或短语,以强调某事物的程度、暗示某事继续发生或在一段时间内不断增加；连接两件相继发生的事件；连接两个从句,表示因果关系时不加.</strong></p>
<p>He lay on his back and looked up at the sky. 他仰卧着观看天空。</p>
<p>I'm going to write good jokes and become a good comedian.</p>
<p><strong>三个以上的词（短语）并列的场合，通常只在最后一个词（短语）前加and，这种情形，and之前加不加逗号都可以。如：</strong></p>
<p>I visited London, Paris<strong>(,)</strong> and Rome.我游览过伦敦、巴黎和罗马。《英汉多功能词典 》</p>
<p>In that room there were a chair, a table<strong>(,)</strong> and a bed. 在那房间里面有一张椅子、一张桌子和一张床。《21世纪英汉汉英双向词典》</p>
<p>▲<strong>但省略and之前的逗号的情况较多。如</strong>：</p>
<p>Joan was rich, beautiful and proud. 琼非常有钱, 漂亮且庄重。《简明英汉词典》</p>
<p>one woman, two men and three children 一个女人﹑ 两个男人及三个孩子 《牛津高阶英汉双解学习词典》</p>
<p>He was tall, dark and handsome. 他身材高大、皮肤黝黑、长相帅气。《朗文当代英语词典》</p>
<p><strong>2). 连接两句话,第二句话的意思是第一句的延伸时加逗号,</strong></p>
<p>如：You could only really tell the effects of the disease in the long term, and five years wasn't long enough.</p>
<h3 id="另外一个解释"><strong>另外一个解释：</strong></h3>
<p><strong>and, but 连接两个独立句子的时候,需要逗号,如果是两个词组或是短语或是各种从句不用逗号.</strong></p>
<p><strong>当有多个并列成分时，最后的并列连词and 或 or 之前一般是不加逗号的。不过近年来，越来越多的作者在表达时出现and 前加逗号的情况，特别是在美国。</strong></p>
<p><strong>另外，当书写长而复杂的句子，特别是有多个并列结构嵌套时，作者往往可在and前加逗号来帮助读者理清句子。</strong></p>
<p>例如：there will be television chat shows hosted by robots, <strong>and cars </strong>with polllution monitors that will disable them when they offend. 加上and之后，句子变得更加清晰了。</p>
<p>For hit to be re-elected, what is essential is not that his policy works<strong>, but that </strong>the public believe that it does. (并列表语从句）</p>
<h3 id="as-well-as-不等同于and.-a-as-well-as-b侧重在于a他们地位不平等另外谓语动词单复数由a决定">As well as 不等同于and. A as well as B侧重在于A,他们地位不平等。另外谓语动词单复数由A决定。</h3>
<p>具体可参考： <a href="https://www.jianshu.com/p/6e9e11a784c2" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/6e9e11a784c2</a></p>
<h3 id="关于but的其他用法">关于but的其他用法</h3>
<p>如 can/cannot but (只能/不得不）， all but （除了...之外都，几乎），but for （=without，如果没有，若不是）， but that （若非），but then （=on the other hand, 另一方面），aything but（绝不）， 见《考研英语必背500句》P58.</p>
<h3 id="and-but是否能够置于居首">And, But是否能够置于居首</h3>
<p>如果前后有关系的，可以大些并且置于句首。但是建议不要太多，适量换成moreover,additionally (in addition), however.</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS84MC8xMDEyYzM3Yzc1ZTcyMjlhYzdjY2ViOWUzY2RhYjNlNF8xNDQwdy5qcGc?x-oss-process=image/format,png" /></p>
<p><a href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171330.jpeg" target="_blank" rel="noopener">备份图片地址</a></p>
<h2 id="while-作为连词要不要加逗号"><strong>2. while 作为连词要不要加逗号</strong></h2>
<p>加逗号的一般表示转折,例如：some people believe soming is harmful to health, while others consider smoking is helpful for reducing working presure. 不加逗号一般表示“同时...” 例如：I worked for one hour outside while it was raining.</p>
<p>表示转折也可不用逗号的，如下面两句就摘自《牛津高阶英汉双解词典》的 while 词条：</p>
<p>I drink black coffee <strong>while</strong> he prefers it withcream.我爱喝清咖啡而他喜欢加奶油的。</p>
<p>English is understood all over the world<strong>while</strong>Turkish is spoken by only a few people outsideTurkey itself. 英语世界通行，但土耳其语离开本国就很少有人说了。</p>
<h2 id="such-as-for-example-and-so-on-etc-i.e.-et-al点击打开链接-具体看链接"><strong>3. such as, for example, and so on, etc, i.e., et al<a href="https://wenku.baidu.com/view/05a1e58f58f5f61fb7366696.html" target="_blank" rel="noopener">点击打开链接 （具体看链接）</a></strong></h2>
<h3 id="such-as-不论有举多少例并列成分的最后一个前面要加and.-需要注意的是如果结尾是etc.就不用加了因为etc.相当于and-so-on.">such as 不论有举多少例,并列成分的最后一个前面要加and. 需要注意的是,如果结尾是etc.就不用加了,因为etc.相当于and so on.</h3>
<p>such as beijing, shanghai, xiamen and xian (或者 xiamen, and xian)</p>
<p>Human pose estimation refers to the task of recognizing postures by localizing body keypoints (head, shoulders, elbows, wrists, knees, ankles, etc.) from images.</p>
<h3 id="因为such-as是对前面的复数名词部分起列举作用若全部列举出要改用namelythat-is-或者i.e意思为即">因为such as是对前面的复数名词部分起列举作用,若全部列举出,要改用namely，that is 或者i.e,意思为“即”。</h3>
<p>比如 He knows four languages, such as Chinese, English, Japanese and German,应将such as改成namely(或i. e. )及后面加逗号, 即He knows four languages, namely, Chinese, English, Japanese and German.</p>
<p>He knows four languages, that is, Chinese, English, Japanese and German.</p>
<h3 id="当使用-such-as-for-example-e.g.-表示泛泛地举几个例子时读者已理解后面接着的会是一些不完整的列举-因此不要再加上-and-so-on-或-etc.-等">当使用 such as ，for example (e.g.) 表示泛泛地举几个例子时,读者已理解后面接着的会是一些不完整的列举, 因此不要再加上 and so on 或 etc. 等!</h3>
<p>它们就已经包含“等等”,如果再加etc. 或and so on,就画蛇添足了。</p>
<p>Writing instructors focus on a number of complex skills that require extensive practice, e. g. , organization, clear expression, and logical thinking.</p>
<p><strong>for example用来举例说明某一论点或情况,一般只举同类人或物中的"一个"为例,作插入语,可位于句首、句中或句末。</strong></p>
<p>Cryptography operations, for example, decryption or signing, in a given period only involve the corresponding temporary secret key without further access to the helper.</p>
<p>A lot of people here, for example, Mr John, would rather have coffee. 这儿的许多人，例如约翰先生，宁愿喝咖啡</p>
<h3 id="like也常用来表示举例可与such-as互换但such-as用于举例可以分开使用此时不可与like互换">like也常用来表示举例，可与such as互换。但such as用于举例可以分开使用，此时不可与like互换。</h3>
<p>Some warm-blooded animals，like/such as the cat，the dog or the wolf，do not need to hibernate.</p>
<p>He has several <strong>such</strong> reference books <strong>as (分开了)</strong> dictionaries and handbooks. 他有几本像字典、手册之类的参考书。</p>
<h2 id="冠词"><strong>4. 冠词 </strong></h2>
<p><a href="http://blog.sina.com.cn/s/blog_5e0022550100ctz2.html" target="_blank" rel="noopener">一个比较全的说明</a></p>
<h3 id="通常情况下复数名词物质名词专有名词前是不加冠词的但如果是特指的话复数名词前就要加the">通常情况下复数名词，物质名词，专有名词前是不加冠词的。<strong>但如果是特指的话，复数名词前就要加the</strong>。</h3>
<p>例如：The books on the table are mine. The water in the glass is hot.</p>
<h3 id="具体的来说什么时候不需要加冠词"><strong>具体的来说，什么时候不需要加冠词，</strong></h3>
<p>作者：未某人 链接：https://www.zhihu.com/question/20321498/answer/22869588 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<ol type="1">
<li><strong>某事物和其他事物比较时，要加定冠词；自身与自身比较，不用定冠词。</strong></li>
</ol>
<p>以前老师都说形容词最高级前要加冠词 The lake is the deepest in the world.</p>
<p>这一句加了定冠词，因为其实后面的名词省略了，完整说法是the deepest (lake) in the world。再请看下面两个例句：</p>
<p>The lake is deepest at this point. 湖水在这一点最深</p>
<p>I feel happiest when my life is maddest.</p>
<p>以上两句最高级都没有定冠词，因为此处 deepest，happiest，maddest 都只是一般的形容词，你在后面没法再补全一个名词，没有名词被省略。既然只是形容词当然没必要加定冠词了。</p>
<p>2.<strong>部分名词作补语起到形容词作用</strong>，该名词词义不再是其所指对象本身，而是所指对象的特征，此时该名词相当于形容词，故<strong>无需冠词</strong>。请看例句</p>
<p>He is fool enough to marry her.</p>
<p>He is no fool.</p>
<p>I'm not philosopher enough to think out a solution.</p>
<p>以上 fool 并非表示笨蛋，而是笨这个特质。Philosopher 也不是哲学家，而是聪明这个特质</p>
<p>3.<strong>复数普通名词前有无 the 的区别：有 the 表示全部，没有 the 表示绝大部分（即有例外）</strong>。硬要解释的话，应该是有了 the 以后将限制的名词视为一个整体，整体中包括所有个体，没有 the 的话就只是单纯的名词复数，表示多个。我自己说出来都觉得很抽象不好理解，看例句吧~</p>
<p>Americans think that they can win the war. 很多美国人认为可以赢</p>
<p>The Americans are an active people. 美国人是活跃的民族（指所有美国人）</p>
<p>Students of our school are diligent. 我们学校学生大多很勤奋</p>
<p>The students of our school are diligent. 所有学生都勤奋</p>
<p>4.He came to the town. （他对于这座城是个陌生人）他来到城里</p>
<p>He came to town. （他和这座城市有关系）他来到城里</p>
<p>为什么这里没有 the 显得他对于这座城很熟悉，而有了 the 显得陌生？我个人的理解是没有 the 的时候，town 就像是一个专有名词，一说他去了城里，大家都知道他去的是哪座城；有了 the 说明需要特指一下是“这座城”，既然需要特指，说明这座城和他没有特定关系。</p>
<p>5.World War II 无冠词 the Second World War 有冠词</p>
<p>6.though/as 引导让步状语从句，名词前省冠词；但若有形容词修饰，不省略冠词</p>
<p>Child as/though he is, he knows much about the society.</p>
<p>A small child as he is, he can solve the problem.</p>
<p>7.<strong>(泛指的物质名称）</strong>，中文中说某某人是个当官的好材料这个当官的好材料英文就是 leader material，同理，踢球的料就是 football material，这些 XXX material 之<strong>前都不加冠词的。当然如果不是泛指，而是特指，那么还是要加 the 的。</strong></p>
<p>8.在新闻标题，标志语，广告词，商品标签等中，为了节约篇幅，会有意省略冠词。</p>
<p>9.<strong>大多数专有名词前不要加冠词：</strong></p>
<p><strong>专有地区名称不加：</strong> Do you know <strong>Nanjing Road</strong> in Shanghai？你知道上海的南京路吗？</p>
<p>海洋、山脉、河流前要加the，the Yangtze River</p>
<p>湖泊、山峰不加the, Mount Everst</p>
<p><strong>但是由某些由普通名构成的专有名词前要加定冠词the</strong> the People's Republic of China 中华人民共和国 the United States of America 美利坚合众国 the Ming Dynasty 明朝 the Great Wall长城 the Great Cultural Revolution文化大革命</p>
<p>10.<strong>用作称呼语或表示头衔的名词前不要加冠词：</strong> What are you reading，Boy？孩子，你在读什么？ He is head of the factory．他是工厂的厂长。</p>
<p>但如果还有修饰语，就要加the，如 the 32nd President/Chirman of ...</p>
<p>11.【<strong>泛指的】抽象名词前不加冠词</strong>： Life is always presenting new things to children．生活总是不断地呈现给孩子们新的东西。</p>
<p><strong>12. 一些特例</strong></p>
<p><strong>mankind(人类)</strong></p>
<p>人是一个不可数的集合名词，不用<a href="https://baike.baidu.com/item/%E5%A4%8D%E6%95%B0/13131232" target="_blank" rel="noopener">复数</a>形式，也不连用冠词。如：This is an invention that benefits mankind. 这是一项造福人类的发明。Mankind has its own problems. 人类有自己的问题。注：mankind 表示“mankind 人(类)”时，虽不可数，但有时却可以表示复数意义，尤其是当其<a href="https://baike.baidu.com/item/%E8%A1%A8%E8%AF%AD" target="_blank" rel="noopener">表语</a>是复数时。如：Mankind are intelligent animals. 人是理智的动物。</p>
<p><strong>13. 乐器前要加 the, 球类前不加the</strong></p>
<p>play the violin (乐器）</p>
<p>play football (球类）</p>
<p>14. <strong>大写缩略词前要不要定冠词the分为两种情况</strong></p>
<p>如果缩略词可以单独发音，不加the。比如NASA，NATO，FIFA，TOEFL，APEC；</p>
<p>如果不能单独发音，需加‘the’，比如 the NBA, the FBI, the HBO.</p>
<p><strong>15. 这里补充一个总结 （看不看无所谓，前面机会全包括了）</strong></p>
<p>转载自：英文论文写作有哪些需要注意的细节？ - 春时粟的回答 - 知乎 https://www.zhihu.com/question/46825717/answer/652234107</p>
<figure>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS92Mi01YjBkNmZlOWYxNzhlMzQ3ZDc4MjIzZDI3YTY1YjFkZF9yLmpwZw?x-oss-process=image/format,png" alt="preview" /><figcaption>preview</figcaption>
</figure>
<p><a href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171358.jpeg" target="_blank" rel="noopener">备份图片地址</a></p>
<h3 id="定冠词基本用法">定冠词基本用法:</h3>
<p>1）特指某人、某事 Wellington is the capital of New Zealand. 惠灵顿是新西兰的首都。 2）指世上独一无二的事物 We have friends all over the world .我们的朋友遍天下。 The moon goes around the earth .月亮绕着地球转。 The sun is rising in the east .太阳在东方冉冉升起。 3）重提前文中提到过的人或事物（即文中第二次出现的人或事物） He, suddenly, saw an isolated house at the foot of the mountain. And curiosity made him approaching the house. 他突然看到山脚下有一栋孤独的房子；好奇心驱使他向那栋房子走了过去。 4）说话人和听话人都熟悉的人或事物 Be sure to bring me the book when you come next time. 你下次来一定要将那本带给我。 5）用于序数词、形容词的最高级形式、和表示方位的名词前 Thanksgiving Day is on the 4th Thursday in November. 感恩节在每年十一月的第四个星期四。 Changjiang is the longest river in China. 长江是中国最长的河流。 Japan lies to the east of China .日本位于中国的东面。 He is one the most famous football stars in the world. 他是世界最著名的足球明星之一。</p>
<p>6）间或用于单数的可数名词前表示泛指 The compass was invented in China.指南针是中国发明的。 The horse is a useful animal .马是有用的动物。 The tiger is in danger of extinct .老虎有绝种的危险。 The monkey is a clever animal. 猴是一种聪明的动物。 注：这种"泛指"是从整个属类的意义上说，而不是"用一个人或物来说明整个属类的 特点"。也就是说，属前者情况时加用定冠词表示泛指，属后者情况时则加不定冠词表示泛指。</p>
<p>7）用于某些由普通名构成的专有名词前 the People's Republic of China 中华人民共和国 the United States of America 美利坚合众国 the Ming Dynasty 明朝 the Great Wall长城 the Great Cultural Revolution文化大革命</p>
<p>8）用于某些词组中。这种用法是约定束成的，我们只有遵从而无旁的选择。 in the morning ( afternoon , evening )上午（下午，晚上） go to the cinema 看电影 on the whole总体上 to the best of就……所及 the sane as 和……一样 out of the question不可能的 on the one hand一方面 on the other hand 另一方面 on the average一般说来 on the contrary相反地 in the least 一点，丝毫 in the long run从长远来看 in the event of 万一 in the final analysis归根结底</p>
<p>9）定冠词+形容词使形容词名词化 We always stand for the oppressed and the exploited. 我们永远支持受压迫、受剥削的人们。 The aged are well taken care of in the community. 在这个社区，老人得到了很好的照顾。 She was fond of writing about the unusual. 她喜欢写一些古怪的题材。 The school for the deaf and the blind is just newly built. 那所聋哑人学校是刚刚新建的。</p>
<h2 id="时态问题"><strong>5. 时态问题 </strong></h2>
<h3 id="美国人的文章里大多全用一般现在时">美国人的文章里，大多全用一般现在时。</h3>
<p><strong><a href="http://blog.csdn.net/lcj_cjfykx/article/details/27173883" target="_blank" rel="noopener">点击打开链接</a></strong></p>
<h3 id="if-引导的真实条件句中的时态"><strong>if 引导的真实条件句中的时态</strong></h3>
<p>除了if引导强调 【<strong>个人主观意愿】 </strong>和 【<strong>与事实不相符的假设】 </strong>的虚拟语气外，if引导的真实的条件句用于陈述语气，假设的情况可能发生。</p>
<p><a href="http://ishare.iask.sina.com.cn/f/34zCicivVbQ.html" target="_blank" rel="noopener" class="uri">http://ishare.iask.sina.com.cn/f/34zCicivVbQ.html</a></p>
<h3 id="if引导虚拟语气考研英语500句p6--17th和p22--78th">if引导虚拟语气：《考研英语500句》P6 -17th和P22 -78th</h3>
<p>分为对现在/将来的虚拟，过去的虚拟，不同的句式固定搭配would或者should，需要注意。</p>
<h3 id="主句和从句中的动作发生有先后关系时需要注意两者的时态">主句和从句中的动作发生有先后关系时，需要注意两者的时态</h3>
<p>比如：After he had finished his homework, he went to bed. 做完作业后，他上床睡觉了。</p>
<p>She felt humble just as she had (felt) whe she <strong>had fisrt taken</strong> a good look at herself. 此句中，as引导方式状语从句，when引导时间状语从句。看她自己发生在感觉之前，所以是过去的过去。</p>
<p>注意： before, after 引导的时间状语从句中，由于 before 和 after 本身已表达了动作的先后关系，若主、从句表示的动作紧密相连，则主、从句都用一般过去时。如： After he closed the door, he left the classroom.他关上了门，离开了教室。</p>
<h2 id="respectively-individually-each-separatelly">6. respectively, individually, each, separatelly</h2>
<p>each每个,各自地 adj, adv,</p>
<p>eg. They cut the cakein two andatehalf each</p>
<p>hese apples willgohalf apoundeach.</p>
<p>respectively各自地, 前面提到了一种顺序，后面的与前面的一一对应，用于句尾</p>
<p>individually个别地,个性的</p>
<p>separately分别地,分开地</p>
<h2 id="将句子的main-idea核心放在开头其他表示时间条件等放在后面"><strong>7. 将句子的main idea核心放在开头，其他表示时间，条件等放在后面</strong></h2>
<h2 id="英语写作检查软件">8. 英语写作检查软件</h2>
<p><a href="http://blog.csdn.net/chuminnan2010/article/details/21811117" target="_blank" rel="noopener">点击打开链接</a></p>
<h2 id="不应该否定词的缩写而应该使用完整形式">9. 不应该否定词的缩写，而应该使用完整形式</h2>
<p>如cannot (特殊的一个，中间没有空格）， is not, do not</p>
<h2 id="主谓一致"><strong>10. 主谓一致</strong></h2>
<h3 id="both...and...两者都...连接名词或代词作主语时谓语动词用复数形式不受就近原则的限制如">1. both...and...“两者都...”，连接名词或代词作主语时，<a href="https://www.baidu.com/s?wd=%E8%B0%93%E8%AF%AD%E5%8A%A8%E8%AF%8D&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBrj6dnA7BryckP1bzrycL0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnHmknjcYnWDv" target="_blank" rel="noopener">谓语动词</a>用复数形式，不受‘<a href="https://www.baidu.com/s?wd=%E5%B0%B1%E8%BF%91%E5%8E%9F%E5%88%99&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBrj6dnA7BryckP1bzrycL0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnHmknjcYnWDv" target="_blank" rel="noopener">就近原则</a>’的限制。如：</h3>
<p>Both you and I are students. Both Li Ming and Wei Hua are good at English.</p>
<h3 id="and-连接两个成分作主语时谓语要用复数形式"><code>2. and 连接两个成分作主语时，谓语要用复数形式。</code></h3>
<p><code>如，You and she are on duty today.</code></p>
<h3 id="主谓一致采用就近原则的情况"><code>3.</code><strong>主谓一致</strong><code>采用就近原则的情况：</code></h3>
<p><code>1). 当连词或连词短语连接两个成分，而重点强调的是“其中之一”的时候，要采用就近原则。</code></p>
<p><code>如，or, either...or..., neither...nor..., not only...but also...等连接主语时，谓语动词以后面的那个主语为主。 如，</code></p>
<p><code>You or he is right.</code></p>
<p><code>Either your teacher or your classmates were there.</code></p>
<p><code>Neither you nor he is able to finish the work in an hour.</code></p>
<p><code>Not only your parents but also I am proud of you . Not only he but also all his family are keen on concerts</code></p>
<p><strong>2). and 连接两个成分作主语时，谓语要用复数形式。</strong>如，You and she are on duty today.</p>
<p><strong>3). there be A and B 句型，there be的be动词由靠的近的A决定！</strong></p>
<p>There <strong>is</strong> <strong><em>a book</em></strong> and <em>two pens</em> in my bag.</p>
<p>4). as well as 连接主语时，谓语动词的单复数必须与前面那个保持一致。</p>
<p><strong>这是因为as well as 不等同于and! A as well as B 其实侧重在A</strong>，相当于not only B, but alse A. 这样理解的话谓语单复数就和重要的A保持一致了。</p>
<p>比如：</p>
<p>John, as well as Mary, come to the party. <strong>错</strong></p>
<p><strong>John</strong>,as well as Mary, <strong>comes</strong> to the party. 对</p>
<ol start="5" type="1">
<li><strong>rather than 连接两个名词或代词作主语时</strong>，<strong>谓语动词应与rather than 前面的名词或代词在</strong>人称和数上保持一致。</li>
</ol>
<p><strong>You rather than</strong> Smith <strong>are</strong> going to go camping. 是你而不是我要去要野营。</p>
<h3 id="有同位语时谓语动词与主语保持一致.">4. 有同位语时,谓语动词与主语保持一致.</h3>
<p><strong>We</strong> <em>each</em> <strong>have</strong> a cellphone： <em>each是we的同位语,</em>我们每个人的意思,谓语动词的数取决于主语,不取决于同位语each.</p>
<p>但若是Each of us……each 就是主语 后面谓语动词就是单数同位语：一个名词(或其它形式)对另一个名词或代词进行...</p>
<h3 id="more-than-one"><strong>5. more than one</strong></h3>
<p>分析如下：</p>
<p><strong>1). more than one +单数名词 作主语时, 尽管从意义上看是复数, 谓语动词用单数 (要从形式一致来考虑).</strong></p>
<p>例如: More than one boy【has read】the story. 注意 more than one boy中的boy用的是单数。</p>
<p>不止一个男孩已经读了这个故事。</p>
<p>2). more than one +数词＋复数名词 作主语时,谓语动词用复数.</p>
<p>例如: More than one thousand men and women【are working】in this factory.</p>
<p>有1000多男女工人在这家工厂做工。</p>
<p>3). more + 复数名词 + than one 作主语时,谓语动词用复数.</p>
<p>例如：More boys than one【have read】the story.</p>
<p>6. a set of + 复数名词 + 谓语动词用单复数都有可能</p>
<p>当“a set of + 复数名词”作主语时，谓语动词用单数或复数都有可能</p>
<p>A set of guidance notes <strong>is</strong> provided to assist applicants in completing the form. 有一系列注意事项，指导申请人填写表格</p>
<p>An important set of ideas <strong>have</strong> been advanced by the biologist Rupert Sheldrake. 生物学家鲁珀特•谢尔德雷克已提出了一整套重要的观点。</p>
<h2 id="否定句和疑问句中any接不可数名词或者可数名词的复数">11. 否定句和疑问句中any接不可数名词或者可数名词的复数</h2>
<p>如， You can't go out <strong>without any shoes</strong></p>
<p>any 也可以用在if或者whether之后，或者某些动词之后，表示 任何的，任一的，此时与可数名词单数使用。</p>
<p>如， take any book you like. Any color is OK. Any teacher will tell you that.</p>
<h2 id="of-的前后名词的单复数">12. of 的前后名词的单复数</h2>
<p>The teller mechine can accepts any dinomination of coins and notes 为什么这里的coin,note用复数，而dinomination是单数形式？</p>
<p>domination前面有any修饰 英语里面的可数名词一般很少单独以单数出现，要么有冠词，要么用复数 这里的coins和notes都是泛指，所以用复数</p>
<h2 id="并列结构前后一致的问题">13. 并列结构，前后一致的问题</h2>
<h3 id="rather-than">rather (than)</h3>
<p>1．rather than后面一般是 rather than do sth. <strong>跟情态动词would,should,will等连用构成固定搭配</strong>,有时rather than可以分开,意为“宁可”、“与其……倒不如”.此时,"<strong>rather than+ do sth</strong>". 例如： I'd rather than go there by air．我宁愿乘飞机去那里.</p>
<p>I'd rather have a quiet night, reading my favorite book. 这里would rather意思是“宁愿、宁可、更、最好、还是为好”，后接动词原形,常省略为’d rather.</p>
<p>This instrict should <strong>be</strong> encouraged rather than <strong>(be)</strong> laughed at. be动词</p>
<p>I'd rather walk than ride a bike.</p>
<p>2．作准<strong>并列连词</strong>,相等于and not,意义为：是...,而不是...:可以+doing 或 to do.例如： 1）He was engaged in writing "rather than" reading the newspaper．他正忙着写东西,而不是 在读报纸."rather than+ doing" 2）He is to be pitied rather than to be disliked．他应该得到怜悯而不是厌恶."rather than+ to do" 所以意义表“宁愿”+do sth; 表“是...,而不是...:”++doing 或 to do.</p>
<p><strong>注意：rather than 后接不定式时，不定式可以带to，也可以不带to</strong>, 如：</p>
<p>I decided to write rather than (to) telephone. 我决定写信而不打电话。</p>
<p><strong>但rather than位于句首时，则只能接不带to 的不定式</strong>。</p>
<p>如：Rather than allow the vegetables to go bad, he sold them at half price. 他唯恐蔬菜烂掉，把它们以半价卖掉了。</p>
<p>3）no... but rather... 不是......而是...... We don't discuss such problems, but rather deal with them.</p>
<p>4）You rather than I are going to go camping. 是你而不是我要去要野营。 注意：rather than 连接两个名词或代词作主语时，谓语动词应与rather than 前面的名词或代词在人称和数上保持一致。</p>
<p>5）连接两个分句 We should help him rather than he should help us. 是我们应该帮助他而不是他应该帮助我们。</p>
<p>6）连接两个动词，表示可观事实，而不是主观愿望宁愿怎么样。 He ran rather than walked. 他是跑来的，而不是走来的。 注意：这里rather than 后用了walked，而没有用walk，表示客观事实，而不是主观愿望。如果换成walk，则作“宁愿……而不愿 ……”解。</p>
<p>3. would rather (sooner) 后跟表示虚拟语气的宾语从句，谓语动词为过去式，表示对过去或者将来的虚拟。</p>
<p>I <strong>would rather</strong> （that) you <strong>told</strong> me the truth. 我宁愿你告诉我真相。</p>
<h3 id="不定式to在什么情况下可以省略">不定式to在什么情况下可以省略</h3>
<p><strong>1. 当and或or连接两个并列不定式时，第二个to常省。 </strong>摘录自：<a href="https://www.hjenglish.com/new/p1042599/" target="_blank" rel="noopener" class="uri">https://www.hjenglish.com/new/p1042599/</a></p>
<p>I plan <strong>to</strong> call him and discuss this question.</p>
<p>我计划给他打电话，讨论一下这个问题。</p>
<p>My friend in China asked me** to <strong>telephone or write</strong> **to her in my free time.</p>
<p>我中国的朋友让我有空给她打电话或写信。</p>
<p><strong>2. 当两个并列to有对比意义，第二个to不能省。</strong></p>
<p>I haven’t decided <strong>to</strong> stay at home or <strong>to</strong> travel to Beijing this holiday.</p>
<p>我还没决定假期是待在家里还是去北京旅行。</p>
<p><strong>To</strong> be, or not <strong>to</strong> be, that is the question.</p>
<p>生存还是毁灭，这是一个值得思考的问题。（《哈姆雷特》）</p>
<p><strong>3. 当两个to之间无并列连词，to不可省。</strong></p>
<p>I came here not <strong>to</strong> help you, but** to **fright you.</p>
<p>我来这不是为了帮你，而是为了吓唬你。</p>
<p><strong>4. 当三个或以上带to不定式构成排比，所有to不可省。</strong></p>
<p>Read not** to <strong>contradict or confute; nor </strong>to** believe and take for granted; not** to <strong>find talk and discourse; but </strong>to** weigh and consider.</p>
<p>读书时不可存心诘难作者，不可尽信书上所言，亦不可只为寻章摘句，而应推敲细思。（《论读书》）</p>
<p>因此，关于不定式，我们除了要分清带to不定式和不带to不定式，还要掌握带to不定式在哪些情况下要省略to这一符号，做到具体情况具体分析。</p>
<p><strong>5. 介词短语to作为并列的宾语，因为比较长，保留to可能会使得句子结构更加清晰？</strong></p>
<p>It <strong>applies</strong> equally <strong>to </strong>traditional historians who view history as only the external and internal criticism of sources, <strong>and to</strong> social science historians who .....</p>
<h3 id="but后接的动词不定式to-do的三种形式何时能够省略to">but后接的动词不定式to do的三种形式，何时能够省略to</h3>
<p>关于这个问题，大致可分三种情况。</p>
<p>第一种情况是but之后的动词不定式一般不带to，如：</p>
<p>(1)He <strong>did </strong>nothing but complain. （连词but后省去了that he did）</p>
<p>(2)Under such circumstances he could not but fail. （but之前省去了<strong>do</strong> anything）</p>
<p>(3)I cannot help but be sorry. （but省去to是受了例(1)结构的影响所致）</p>
<p>(4)He could not choose but love her. （也是受了例(1)的结构的影响，这种说法现在已不多见）</p>
<p>第二种情况是but之后的动词不定式一般须带to，如：</p>
<p>I have no choice but to accept the fact. （but后省去了the choice）</p>
<p>There was no choice but to bear it and grin. （理由同上）</p>
<p>第三种情况是but后的动词不定式可带to亦可不带to，如：</p>
<p>There is nothing to do but (to) fight it out. （部分地受到了例(1)结构的影响）</p>
<p>There was clearly nothing left to do but (to) flop down on the shabby little couch and weep. （理由同上）</p>
<p>There remained nothing but (to) get into the water…（理由同上）</p>
<p>except与but后的不定式何时可省to？</p>
<p>有一读者问：有这样四个句子：</p>
<p><strong>(1)He seldom goes back home except to ask for money from his parents.</strong></p>
<p><strong>(2)He did nothing there except watch TV for the whole night.</strong></p>
<p><strong>(3)I had no choice but to stay in bed.</strong></p>
<p><strong>(4)Last night I did nothing but prepare my lessons.</strong></p>
<p>我搞不懂except和but之后何时接to何时不接to。</p>
<p><strong>第一种情况：其前的谓语动词为do时(必须是实义动词do以及它的各种时态变形，而不是其他动词），可不用to，如例句(2)和(4)（用to也不为错）</strong></p>
<p><strong>第二种情况：其前的谓语动词不是do，则一般须用to，如例句(1)和(3)</strong></p>
<p><strong>第三种情况：其前为there is nothing to do，则可用可不用to，如例句(5)：</strong></p>
<p><strong>(5)There is nothing to do but (to) fight it out.</strong></p>
<h3 id="as引导比较状语从句">as引导比较状语从句</h3>
<p><strong>as引导比较状语从句，其基本结构是as…as。前一个as是副词，后一个as是比较状语从句的连词</strong>。否定结构为not so much … as …。例1：Small as it is, the ant is as much a creature as are all other animals on the earth. 尽管蚂蚁很小，但是它同地球上的任何其他动物一样，也是一种动物。 例2：It was not so much the many blows he received as (连词) the lack of fighting spirit that led to his losing the game. 与其说是他受到了许多打击，还不如说是缺乏斗志使他输掉了比赛。</p>
<p>例3. You had <strong>as good/well </strong>go <strong>(do)</strong> there on foot <strong>as</strong> wait <strong>(do) </strong>for the bus, since the company is not far away.</p>
<h3 id="并列原因状语">并列原因状语</h3>
<p>The behavioral sciences have been slow to change <strong>partly because</strong> the explanatory items often seem to be directly observed** and partly because** other kinds of explainations have been hard to find.</p>
<h3 id="并列表语从句">并列表语从句</h3>
<p>What is essential <strong>is not that</strong> his policy works, <strong>but that </strong>the publick beliebe that it dose.</p>
<h3 id="并列定语">并列定语</h3>
<p>This seems mostly effetively done by supporting a certain amount of** research ** not related to immediate goals but of possible consequence in the future.</p>
<p>等等很多复合结构....</p>
<h2 id="meet-the-need-of也有meet-the-needs-of应该是看后面指代的是一件事还是多件事">14. meet the need of，也有meet the needs of，应该是看后面指代的是一件事还是多件事</h2>
<h2 id="a-equals-b-equal是及物动词后面不需加to-等价于-a-is-equal-to-b-用作形容词">15. A equals B (equal是及物动词，后面不需加to） 等价于 A is equal to B (用作形容词）</h2>
<h2 id="in-detail-是对的但是可以说-the-details-of-...">16. In detail 是对的，但是可以说 the details of ...</h2>
<blockquote>
<p>In detail</p>
</blockquote>
<p>Is correct. Assuming that the context looks like this.</p>
<blockquote>
<p>[After general description]</p>
<p>In detail, the algorithm will....</p>
</blockquote>
<p>But you could always say..</p>
<blockquote>
<p>Here, I describe the details of the algorithm.</p>
</blockquote>
<h2 id="becasue-和-because-of">17. becasue 和 because of</h2>
<p>具体可参考：<a href="https://wenku.baidu.com/view/38c06463f011f18583d049649b6648d7c1c7089c.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/38c06463f011f18583d049649b6648d7c1c7089c.html</a></p>
<h3 id="because可以用来引导原因状语从句-或者-表语从句">because可以用来引导原因状语从句 或者 表语从句</h3>
<p>It is because you’re eating too much. 那是因为你吃得太多了.</p>
<p>汉语说“之所以……是因为……”，英语可以类似以下这样的句型（用that比用 because普通）。如：</p>
<p>The reason (why) I’m late is that [because] I missed the bus. <strong><em>(引导表语从句</em></strong>） 我迟到的原因是因为我没有赶上公共汽车。</p>
<p><strong>传统语法认为这类句型不能用 because, 但在现代英语中用because 的情形已很普遍。</strong></p>
<h3 id="because有时可引导一个句子作主语此时通常采用just-because这样的形式并且主句谓语动词通常当然不是一定为mean">because有时可引导一个句子作主语，此时通常采用just because这样的形式，并且主句谓语动词通常（当然不是一定）为mean。</h3>
<p>如： Just because you speak English doesn’t mean you can teach it. 你会说英语并不意味着你能教英语。</p>
<h3 id="because-of-用于构成复合介词because-of其后可接名词代词-动名词what-从句但不能是that从句或没有引导词的从句等">because of 用于构成复合介词because of，其后可接名词、代词、 动名词、what 从句（<strong>但不能是that从句</strong>或没有引导词的从句）等。</h3>
<p>如：</p>
<p>He is here because of you (that). 他为你（那事）而来这里。 可以接代词that，但是不能接that从句。</p>
<p>We said nothing about it, because of his wife’s being there. 因为他妻子在那儿，我们对此只字未提</p>
<p>He left the company because of what the boss said at the meeting. 他离开了这家公司，是因为老板在会上讲的话</p>
<h3 id="because-of通常用来引导状语而不能用于引导表语引导表语时可用-due-to但是若主语是代词不是名词则它引出的短语也可用作表语"><strong>because of通常用来引导状语，而不能用于引导表语（引导表语时可用 due to）</strong>。<strong>但是，若主语是代词（不是名词），则它引出的短语也可用作表语。</strong></h3>
<p>误：His absence is because of the rain. 正：His absence is due to the rain. 他因雨未来。</p>
<p>主语是代词，可以引导表语，如：</p>
<p>It is because of hard work. 那是因为辛苦工作的原因。 It will be because of money. 那将都是因为钱的原因。</p>
<h2 id="容易混淆的可数和不可数名词以及既可以可数又可以不可数">18. 容易混淆的可数和不可数名词，以及既可以可数又可以不可数</h2>
<p><strong>不可数的抽象名词用来表示可数的人或物----抽象名词具体化（名词的数和词义发生变化！！！）</strong></p>
<p>例如：详情见： <a href="http://blog.sina.com.cn/s/blog_53ca7b1b0102e0ac.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_53ca7b1b0102e0ac.html</a></p>
<p>1.<strong>Thought </strong> n. 1）[U] 思维;思考,考虑，深思; 没有复数形式。 • After serious thought, he decided to accept their terms.经认真考虑,他决定接受他们的条件。 • After much thought he decided not to buy the car.经过仔细考虑后他决定不买汽车了。</p>
<ol start="2" type="1">
<li><p>(常用在疑问、否定句中)意图，打算，倾向意图；目的：[U][(+of)] • He had no thought of hurting her.他没想要伤害她。 • There was no thought of coming home early.没想到提早回了家</p></li>
<li><p>[C] 想法，见解，观念；念头 (+of/about/on)]为可数名词有复数形式 • Please write and let me have your thoughts on the matter.请写信让我知道你对此事的看法。 • She's a quiet girl and doesn't share her thoughts.她是个内向的女孩，不表露她的想法。</p></li>
<li><p>关心,注意，留意；注意；关怀；悬念（常与of, to连用）[C] [U][(+for)] • The nurse was full of thought for the sick men. 那护士非常关怀病人。 • Her husband didn't give much thought to what she said.她的丈夫不在乎她说什么 With no thought for his own safety, the old man went off at a run to save the drowning boy.老汉毫不顾虑自己的安全,奔去救那溺水的男孩。 拓展辨析</p></li>
</ol>
<p>Thinking n. [U] 没有复数形式。 1） thinking思想,思考，考虑 • I have to do some thinking before making a decision.我得先思考一下,然后才好作决定。 2）thinking [U] 意见，想法，观点；意见；见解；看法 • To my thinking, this is not a good idea.我认为这不是个好主意 • independent thinking 独立思考 • wishful thinking一厢情愿的想法 • in modern thinking按照现代的想法</p>
<p>2. <strong>attraction </strong>（[U]）吸引，吸引力；</p>
<p>（[C]）有吸引力的人或事物。 1）The idea of traveling to the moon has little attraction for me. 到月球上旅行的想法对我没有什么吸引力。</p>
<p>2）The city's bright lights, theatres, and movies are great attractions.城里明亮的灯、戏院、电影等有巨大的吸引力。 3) One of the main attractions of the job is the high salary. 这份工作最吸引人的是薪水高。 3. <strong>comfort </strong> [U] 安慰，慰藉，宽恕；</p>
<p>[C]令人感到安慰的人或事物</p>
<p><strong>performance</strong></p>
<p>做表现，表演来说是可数的，但是作为性能来说是不可数的</p>
<p><strong>struture是否可数</strong></p>
<p>1.结构;构造;组织[U][C] We know a lot about the structure of genes now. 如今我们对基因的结构有了较多的了解。 2.构造体;建筑物[C] We visited the museum, a steel and glass structure. 我们参观了博物馆,它是一座钢和玻璃的建筑物。</p>
<h3 id="improvement看指的是整体的进步不可数还是因为某一个变动带来的具体的某个进步可数">improvement：看指的是整体的进步（不可数），还是因为某一个变动带来的具体的某个进步（可数）。</h3>
<p>1 [ uncountable ] the act of making sth better; the process of sth becoming better 2 [ countable ] a change in sth that makes it better; sth that is better than it was before Oxford Advanced Learner’s Dictionary, 8th edition</p>
<h3 id="technology-和-improvement-类似">technology 和 improvement 类似</h3>
<h3 id="data-这个单词是-datum-的复数形式">Data 这个单词是 datum 的复数形式，</h3>
<p>在学术写作中通常视其为一个可数名词的复数，因为这种观点承认了 datum 这个单词作为可数名词单数的存在，更加严谨，虽然在学术论文中我们并不用 datum 这个单词。所以 data 后面的 be 动词为 are 或者 were。</p>
<p>当然我们也可以在 data 前面加上量词，如 "a set of"，让读者更加明确。</p>
<p>例如：</p>
<p>While <strong>data</strong> for the annually averaged solar share are not available, it is reasonable to anticipate that this could approach ～17% with sufficient storage, which is a worthwhile target (Nathan et al., 2018).</p>
<h3 id="学术写作中-work-一般为不可数名词使用">学术写作中 work 一般为不可数名词使用，</h3>
<p>但如果 work 表示作品或者具体成果可以在后面加 "s"，例如 published works，这种表达方式也是少有遇见的。</p>
<p>例如：</p>
<p>Much <strong>work</strong> has been conducted over the past decade on applying compressive sensing methods to medical X-ray CT as a way of reducing patient radiation exposure (Jones &amp; Huthwaite, 2015).</p>
<h3 id="result"><strong>Result</strong></h3>
<p>学术写作中 result 一般为可数名词。</p>
<p>例如：</p>
<p>Despite the promising <strong>results</strong> reported from these and similar studies, the mass production of graphene by CVD has predominantly focused on the electronic device industry (Papageorgiou et al., 2017).</p>
<h3 id="information"><strong>Information</strong></h3>
<p>学术写作中 information 一般视为不可数名词。</p>
<p>例如：</p>
<p>If all <strong>information</strong> is known, then any price changes should be a "random walk," preventing any prediction on future values (Cooper et al., 2017).</p>
<h3 id="evidence"><strong>Evidence</strong></h3>
<p>学术写作中 evidence 一般视为不可数名词。</p>
<p>例如：</p>
<p>By these means, <strong>evidence</strong> is a weaker definition of truth than proof（Fossum et al., 2019）</p>
<p>Recent <strong>evidence</strong> indicates that NOXA can also act as an activator BH3 protein (Bhola &amp; Letai, 2015)</p>
<h3 id="research"><strong>Research</strong></h3>
<p>Research 这个单词既可以做可数名词，也可以做不可数名词，但是做不可数名词的情形更多。</p>
<p>例如：</p>
<p>Much <strong>research</strong> has examined the construction of nanoelectrodes and nanoelectrode assemblies, and although a number of ingenious strategies have been devised (Bhon, 2009).</p>
<p>当我们使用 "researches"，通常表示 "different or separate groups of research"，并且大家需要注意，从语言学的角度来说，"a research" 这种表述方式在语法上是错误的。</p>
<h3 id="development"><strong>Development</strong></h3>
<p>Development 做可数名词和不可数名词的情况都有。</p>
<p>当表示某个领域或者事物的“缓慢发展与进步”，使得在原本基础上更加先进、有竞争力，这种情况下视为不可数名词。</p>
<p>例如：</p>
<p>Ag-based nanowires were particularly effective sensor elements owing to the <strong>development</strong> of chemically responsive interfacial boundaries (Bhon, 2009).</p>
<p>如果表示产生了新的发现、成果，制造了新的产品，这种情形视为可数名词。</p>
<p>例如：</p>
<p>Recent <strong>developments</strong> in medical imaging driven by both increasing computational power and the desire to reduce patient X-ray exposure have led to the development of a number of limited view CT methodologies (Jones &amp; Huthwaite, 2015).</p>
<h3 id="environment"><strong>Environment</strong></h3>
<p>Enviroment 在学术论文中一般为可数名词，可以指自然界存在的环境，也可以指影响人或者事物发生发展结果的条件。</p>
<p>例如：</p>
<p>This is the major transport process relevant to many exposure <strong>environments</strong> and degradation mechanisms and so its reduction is the key to enhancing durability (Wong et al., 2015).</p>
<h3 id="meet-the-need-of也有meet-the-needs-of应该是看后面指代的是一件事还是多件事-1">meet the need of，也有meet the needs of，应该是看后面指代的是一件事还是多件事</h3>
<h3 id="principle-作原则原理行为规范准则来说是可数的作为操守道义为人之道来讲不可数">Principle 作原则、原理，（行为）规范/准则来说是可数的，作为操守，道义，为人之道来讲不可数</h3>
<p>We must come back to first principle<strong>s</strong>. (基本原则）</p>
<p>I live according to my** principless**. (行为规范）</p>
<p>It is a matter of <strong>principle</strong> with him to tell the truth. (操守）</p>
<h2 id="定语从句中先行词的位置">19. 定语从句中先行词的位置</h2>
<h3 id="定语从句谓语动词要与先行词保存一致">定语从句谓语动词要与先行词保存一致!</h3>
<p>He likes movies that __are__(be) about scary monster</p>
<h3 id="定语从句并不一定总是紧接着先行词的可能被分隔">定语从句并不一定总是紧接着先行词的，可能被分隔</h3>
<p>先行词通常与定语从句是“手拉手”在一起的，但也可能被分隔两处。被分隔的情况有三种，即按照英语的“尾重原则（principle of end weight），被定语、状语或谓语分隔。</p>
<p>常见的三种不是紧接着的情况见：<a href="http://www.01ue.com/--tutorial-show-g-1-i-3-po-32.html#" target="_blank" rel="noopener">http://www.01ue.com/--tutorial-show-g-1-i-3-po-32.html#</a></p>
<p>The <strong>mineral elements</strong> from the soil <strong>that are usable by the plant must be dissolved in the soil solution before they can be taken into the root. </strong>（被定语分隔）</p>
<p>Never leave <strong>that</strong> until tomorrow <strong>which you can do today</strong>. （被状语分隔）</p>
<p><strong>Social science</strong> is that branch of intellectual enquiry <strong>which </strong>seeks to study humans and their endeavors in the same reasoned, orderly, systematic, and dispassioned manner that natural scientists used for the study of natural phenomena. （被谓语分隔）</p>
<h2 id="consit-of表示由...组成是动词短语不是介词短语">20. consit of，表示由...组成，是动词短语，不是介词短语！</h2>
<p>It can not handle person poses which consist of only one keypoint.</p>
<h2 id="形容词变副词规则尤其是什么时候需要去e加ly">21. 形容词变副词规则，尤其是什么时候需要去e加ly</h2>
<p>1. 在形容词词尾直接加-ly.如:real-really; helpful-helpfully; careful-carefully; hopeful-hopefully; slow-slowly; quick-quickly; quiet-quietly 2. 以辅音字母加y结尾的形容词要变y为i,然后再加-ly.如:busy-busily; angry-angrily; easy-easily 3. 某些以辅音字母加不发音的字母e结尾和以-ue结尾的形容词要先去掉e,然后再加-y或-ly.如:terrible-terribly; true-truly; gentle-gently. 这里说是某些情况则肯定有特例，如 recursive - recursively 就么有去e</p>
<h2 id="辨析容易混淆的单词和短语">22. 辨析容易混淆的单词和短语</h2>
<p>warp:弯曲，使变形； wrap:包装</p>
<p>extent : 大小，程度，范围； extend: 延长，扩大，延期</p>
<h3 id="result-in-和-lead-to-的区别">result in 和 lead to 的区别：</h3>
<p>lead to 表示造成某种情形或结果的逐步变化过程。如： The storm led to serious floods. 暴雨造成了严重的洪水。 result in 表示意想不到的结果。如： Smoking too much will result in sickness. 吸咽过量会导致疾病。</p>
<h3 id="in-three-steps用介词in而不是at">in three steps，用介词in而不是at</h3>
<p>We perform the task in three steps.</p>
<h3 id="need-for而不是need-of">need for，而不是need of</h3>
<p>need作名词表示需要常于介词for 搭配, the need for money.</p>
<h3 id="suffer与suffer-from的区别">suffer与suffer from的区别</h3>
<p>suffer 经受，使遭受（坏事，不愉快的事），其宾语一般是loss(损失), pain（疼痛）, punishment（惩罚）, defeat（失败）, wrong, hardship, torture, grief, injustice, disappointment等。</p>
<p>I will not suffer such conduct. 我不能容忍这种行为。</p>
<p>They suffered huge losses in the financial crisis. 他们在经济危机时遭受了巨大损失。</p>
<p>suffer from 因……而痛苦 1. suffer from+疾病名词（或者他人闲言碎语、劳累、记忆力减退等），表示患病、为……受苦。</p>
<p>She suffers from headache. 她患头痛病。</p>
<p>I'm suffering from a lack of time this week.我为这周时间不够用而苦。</p>
<p>2. suffer from+自然灾害 suffer from drought 遭受旱灾；suffer from floods 遭受水灾</p>
<h3 id="vitrual-和-virtually">vitrual 和 virtually</h3>
<p><strong>虚拟的，实质上的，事实上地，几乎</strong>，<strong>而不是虚假的意思</strong>！</p>
<p>Virtual means so nearly that any difference is unimportant 。</p>
<p>virtual "虚拟的" 的解释是特指电脑方面的. 比如说 virtual imagery (虚拟图像), virtual computing (模拟计算), virtual tour (虚拟游览), 而virtual 的另一个意思是 "基本上", 类似于 almost.</p>
<p><strong>Virtually</strong> all cooking was done over coal-fired ranges. ** 事实上**所有的烹饪都是在烧煤的炉灶上完成的。</p>
<h3 id="most-和-mostly">most 和 mostly</h3>
<p>A. <strong>most作副词时，是much的最高级形式，常和多音节的形容词或副词连用，构成最高级，表示“最”</strong>。most还可以修饰动词表示“最”，其位置较为灵活，放动词前后均可。此外，若most修饰可数名词单数且带形容词的短语时，可用“a most...”结构，表示“非常”。</p>
<p>B. <strong>mostly 主要用来修饰be动词或介词短语等</strong>，<strong>表示“大部分地”、“大多数地”、“主要地”</strong>。</p>
<p>例句：</p>
<p>（1） It's the most important question. 这是最重要的问题。</p>
<p>（2） I like winter most. 我最喜欢冬天。</p>
<p>（3） Mary is a <strong>most </strong>beautiful girl. 玛丽是个<strong>非常</strong>漂亮的女孩。</p>
<p>（4） The guests are mostly her friends. 客人大多数是她的朋友。</p>
<p>（5） I am mostly out on Sunday. 星期日我多半不在家。</p>
<p>（6） He uses his car mostly for going to work. 他的汽车主要是上班用。</p>
<ol start="7" type="1">
<li>I am <strong>most grateful</strong> for your slefless donation. <strong>十分感谢</strong>.....</li>
</ol>
<h3 id="wishhopemay的用法区别">wish，hope，may的用法区别</h3>
<p><a href="https://wenku.baidu.com/view/e45f79f3b0717fd5360cdcf1.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/e45f79f3b0717fd5360cdcf1.html</a></p>
<p>may you + do原型</p>
<p>wish sb + 名词 或者 wish sb to do</p>
<p>hope that</p>
<p>但是 hope sb to do</p>
<p>例：Wish you success (名次). May you succeed (动词原型). Hope you will/can succeed.</p>
<h3 id="the-state-of-the-art-in-和-state-of-the-art">the state of the art in 和 state-of-the-art</h3>
<p>前者表示“...艺术/技艺的现状", 后者意为“顶级的，尖端的”</p>
<h3 id="fisrt-...-second-...third-....-和-fisrtly-...-secondly-...thirdly-....-and-finally....">Fisrt, ... Second, ...Third, .... 和 Fisrtly, ... Secondly, ...Thirdly, .... and finally,....</h3>
<p>前者用于列举说明各种并列的事实或者原因；后者表达的是“首先，····；其次，····；再次····；最后···"，<strong>有次序上的区别时应该用后者！</strong></p>
<p>当然，其中secondly可以替换成besides, in addition, what's more等，finally也可以替换成 at last, last but not least.</p>
<h3 id="demand-of-和-demand-for-前者是动词性的后者是名词性的">demand of 和 demand for, 前者是动词性的，后者是名词性的</h3>
<p><strong>(1)demand of 要求/期待...</strong></p>
<p>The court will be adjourned for an hour according to the <strong>demand of</strong> the defense. 根据被告及其辩护律师的要求，法庭将休庭一小时。</p>
<p>The factory is in great <strong>demand o</strong>f steel to keep up production. 那个工厂需要大量钢铁以维持生产。</p>
<p>The main demand of the Indians is for the return of one-and-a-half-million acres of forest to their communities.</p>
<p>印第安人的主要要求是将150万英亩的森林归还给他们族群。</p>
<p>(2)<strong>demadn for对 ... 的要求； 对 ... 的需求</strong></p>
<p>A fall in <strong>demand for</strong> oil tankers has put jobs in jeopardy.</p>
<p>油轮需求量的下降使很多工作职位受到威胁。</p>
<p>These developments have created a great demand for home computers. 这些发展促使家用电脑的需求量增大。</p>
<p>Demand for oil has nosedived. 对汽油的需求已骤减。</p>
<h3 id="relieve和alleviate-有什么区别">relieve和alleviate 有什么区别</h3>
<p><strong>1. Alleviate 指短暂的减轻压抑而没有解决其根源</strong>：“No arguments shall be wanting on my part that can alleviate so severe a misfortune” “对我来说要减轻如此严重的不幸,不需要任何争论”. <strong>2. Relieve 指缓解或使造成不快或压抑的某事变得可以忍受</strong>：“that misery which he strives in vain to relieve” “他徒劳地奋力去减轻苦难” “The counselor relieved her fears” “顾问消除了她的恐惧” 3.Allay意味着至少是暂时从造成负担或痛苦的事物中解脱出来：“This music crept by me upon the waters,/Allaying both their fury and my passion/With its sweet air”.“在水面音乐爬上我的心头/平息了他们的愤怒和我的热情/用它甜美的声音”.</p>
<h3 id="abilitycapability-和-capacity-的区别">ability、capability 和 capacity 的区别</h3>
<p>Ability、capability 和 capacity 就许多用法而言是近义词。这三个词常用于描述一个人完成某个动作的能力。例如，一个人可能拥有一周读两本的 ability、capability 或 capacity。但是 capacity——其与其它动词共有的含义通过比喻得到了进一步扩展——还有其它动词没有的特殊用法。Capacity 往往与体积、容量和数量相关。例如，“The vehicle’s fuel capacity is 120 gallons,”，在这句话中，capacity 指对车辆机油装载能力的一种度量，不能与剩下的两个词互换使用。“The vehicle’s fuel ability”听起来不正确。</p>
<p>ability 和 capacity 之间的另一个常见不同点在于，对于人类和动物而言，capacities 是天生的，而 abilities 是学到的。例如，一个孩子可能天生就具有成为厨师的 capacity，但是做菜的 ability 必须通过学习获得。</p>
<p>同时，Capability 通常表示 ability 的极限。例如，你说你有写好文章的 ability，我可能会问你有没有在明天之前写出10页文章的 capability。此外，capabilities 往往是一种非此即彼的命题，而ability 往往是程度上的问题。例如，我可能会说，我虽然有写作的 ability，但是我没有写小说的 capability。不过，跟这些词的其它区别一样，这些不同比较模糊，除了一般固定的用法之外，这些词都是可以互换使用的。</p>
<h3 id="ignoreneglect和overlook的区别">ignore,neglect和overlook的区别</h3>
<p><strong>ignore v.忽视，不理睬，指有意识地拒绝。</strong></p>
<p>She saw him coming but she ignored him.她看见他走过来，但是装作没看见。</p>
<p><strong>neglect v.忽视，忽略，疏忽，指无意识地忽视或忘记。</strong></p>
<p>He neglected to make repairs in his house.他忘记了修理房子。</p>
<p><strong>overlook v.忽略，疏漏，指有意识地遗漏，也指无意识地忽略。</strong></p>
<p>The mother overlooked her little boy's bad behavior.那位母亲忽视了她的小儿子的不良行为</p>
<h3 id="inter和intra的区别">inter和intra的区别</h3>
<h4 id="intra-表示在内内部">intra-表示“在内，内部”</h4>
<p>intraparty党内的抄(intra+party党) intracollegiate大学内的(intra+collegiate大学的，学院的) intrapersonal个人内心的(intra+personal个人的) intranational国内的(intra+national国家的)</p>
<h4 id="inter-表示在之间相互">inter-表示“在…之间，相互”</h4>
<p>international国际的(inter+national国家的) interpose置于，介入(inter+pose放→放在二者之间) intersect横断(inter+sect切割 →在中间切→zhidao横断) intervene干涉(inter+vene走→走在二者之间→干涉) interaction相互影响(inter+action行动→相互行动→影响) interchangeable可互换的(inter+changeable可改变的) interlude(活动间的)休息(inter+lude玩→在中间玩→活动间休息) interrelate相互关连(inter+relate关连)</p>
<h3 id="axiomprincipletheoremlawrule-在数学物理中的含义如何区分">axiom，principle、theorem、law、rule 在数学、物理中的含义如何区分？</h3>
<p><a href="https://www.zhihu.com/question/389899464/answer/1176208152" target="_blank" rel="noopener">参考1</a></p>
<p><a href="https://www.zhihu.com/question/20222198/answer/14386797" target="_blank" rel="noopener">参考2</a></p>
<p><a href="https://www.zhihu.com/question/389899464/answer/1174115855" target="_blank" rel="noopener">参考3</a></p>
<p>数学中的<strong>公理axiom</strong>，与物理中的<strong>定律 principle</strong>同属客观规律，<strong>但无法用实验证明只能被假定成立</strong>，<strong>只能在一定范围内归纳</strong>。<strong>原理和公理一样，无法被证明，</strong>也因此科学的发展出现大的突破总是以公理/原理被证伪的形式。但注意，<strong>数学上没有Principle，只有Axiom。</strong>数学和物理在最开始的起点处，一个称之为"原理"一个称之为"公理"的东西，本质上似乎是相同的，只不过"原理"往往要比公理更复杂，原理(Principle)更注重推导的结果是否丰富，而公理(Axiom)仅仅为逻辑演绎提供起点。</p>
<p><strong>定理(Theorem)</strong>则是通过逻辑演绎，由原理或者公理推导出来的用途比较广的，重要性很强的推论，一般的推论称为Corollary，其实和定理本身属性相同，只是重要性(应用范围)差很多，一般只用于作为重要定理的证明中间过程存在着。物理里凡是叫做 theorem 的东西，总是跟数学拖不了干系。不来点数学推导，似乎就很难被称作 theorem。</p>
<p><strong>注意：</strong>命名这事儿，还是要结合当时的时代背景，在当时看来无解的principle或者实验规律law，可能经过物理学和数学地进一步的发展，找到更加底层的principle或者law，进而被推演出啦，升级为theorem，<strong>但是很多时候名称依然沿用而被保留了下来。</strong></p>
<p><strong>定律(Law)主要是一开始一些直接从经验中总结出来的东西，一开始的角色其实和原理差不多，只不过形式更为具象，原理更为抽象。同时作为原理一般能够推导出多条定律，比如最小作用量原理，既可以给出牛顿定律，也可以给出相对论下的运动方程，还可以给出麦克斯韦方程组</strong>。相对来说，<strong>如果完全是靠经验总结而非逻辑归纳得到的命题，应该就只能叫做定律了</strong>。</p>
<p><strong>规则(Rule)则是一些用于计算或者简化计算的常规套路</strong>，它们本身并没有多少意义，只是在漫长的理论推导过程中由于经常被用到，<strong>人们更愿意记住它们而不是每次重复之前做过的同样的步骤</strong>，相当于围棋中的"定式"。</p>
<h2 id="英文标题大小写">23. 英文标题大小写</h2>
<p><strong>不定冠词a 在标题中是小写</strong></p>
<p>更加具体的标题大小写规则如下： <a href="https://www.zhihu.com/question/50427700/answer/121428231" target="_blank" rel="noopener">知乎回答</a></p>
<p><strong>Capitalize the first and the last words of the title and all other words (including words following hyphens in compound words) except articles, coordinating conjunctions (and, or, but, nor, for), short prepositions, and the <em>to</em>infinitives:</strong> e.g. My First Visit to the Palace Museum</p>
<p>The People Without a Country</p>
<p>Rules to Abide By (<strong>By是最后一个单词，所以大写</strong>）</p>
<p>Dickens and <em>David Copperfield</em></p>
<p>What Can the Artist Do in the World of Today?</p>
<p>What Reform Means to China</p>
<p>The Myth of <strong>a </strong>"Negro Literature"</p>
<p>The <strong>English-Speaking </strong>People in Quebec <strong>（符合词连字符之后的单词开头也要大写，除了介词等那些例外）</strong></p>
<p>The Top-down and Bottom-up Approaches (这里的复合连词后是介词吗？） Excerpt From: <em>A Handbook of Writing </em></p>
<p><strong>"Begin the first word and all other important words in the title with a capital letter. The articles, prepositions, and conjunctions are usually unimportant." </strong> Excerpt From: <em>The Writing of English</em></p>
<h2 id="on-the-contrary-by-contrast-或-in-contrast-towith-on-the-other-hand的区别">24. on the contrary, by contrast 或 in contrast (to/with), on the other hand的区别</h2>
<p>在写文章的时候，如果你已经预设了立场，要表达与之前的说法相反或强调之前的说法是错误的，可以用 on the contrary。 若要做客观的观点对比，不加入个人对错想法，只是单纯比较时就用 by contrast 或 in contrast (to/with)。 而 on the other hand 则是说明一件事情的两面，或是不同的看法，虽有部分人士认为此片语不够正式，但在学术语料库出现的频率可是这三个里面最高的。</p>
<h2 id="because-for-as-since表示因为时的区别">25. because ,for ,as ,since表示“因为”时的区别</h2>
<p><a href="http://www2.chinaedu.com/101resource004/wenjianku/200542/101ktb/ynjd/ynaewe0d/ynaewe0d.htm" target="_blank" rel="noopener">摘录自</a></p>
<p>because, as, for, since这几个词都是表示“原因”的连词，语气由强至弱依次为：because→since→as→for;其中because, since, as均为从属连词，引导原因状语从句；而for 是并列连词，引导并列句。</p>
<p><strong>1. because表示直接原因，它所指的原因通常是听话人所不知道的，其语气最强。常用来回答why的提问，一般放于主句之后，也可以单独存在。</strong>例如：</p>
<p>(1)I stayed at home because it rained. 因为下雨我呆在家里。</p>
<p>(2)Because Lingling was ill, she didn”t come to school. 玲玲因病，没有上学。</p>
<p>(3)�Why is she absent? 她为什么缺席？</p>
<p>�Because she is sick. 因为她病了。</p>
<p><strong>此外，在强调句型中，只能用because</strong>。例如：</p>
<p>(4)It was because I missed the early bus that I was late for school. 我上学迟到是因为我没有赶上早班汽车。</p>
<p><strong>2. since侧重主句，从句表示显然的或已为人所知的理由，常译为“因为”、“既然”，语气比because稍弱，通常置于句首，表示一种含有勉强语气的原因</strong>。例如：</p>
<p>(1)Since he asks you, you”ll tell him why. 他既然问你，那就告诉他为什么吧。</p>
<p>(2)Since everyone is here, let”s start. 既然大家都到齐了，我们就出发吧！</p>
<p>(3)Since I understood very little Japanese, I couldn”t follow the conversation. 我日语懂得不多，因而听不懂对话。</p>
<p><strong>3. as是常用词，它表示的“原因”是双方已知的事实或显而易见的原因</strong>，或者理由不是很重要，含义与since相同，但语气更弱，没有since正式，<strong>常译为“由于，鉴于”。从句说明原因，主句说明结果，主从并重</strong>。例如：</p>
<p>(1)We all like her as she is kind. 我们都喜欢她，因为她善良。</p>
<p>(2)As I had a cold, I was absent from school. 因为我感冒了，所以没去上课。</p>
<p>(3)As Xiaowang was not ready, we went without him. 由于小王没有准备好，我们只好不带他去了。</p>
<p>4. for用作连词时，与because相似，但它所表示的原因往往提供上文未交待过的情况。<strong>for不表示直接原因</strong>，表明附加或推断的理由，因此for被看作等立连词，它所引导的分句只能放在句子后部(或单独成为一个句子)，并且前后两个分句间的逻辑关系不一定是因果关系，其间用逗号隔开，且for不可置于句首，for的这一用法常用在书面语中，较正式。例如：</p>
<p>(1)The days are short, for it is now December. 白天短了，因为现在已是十二月份。</p>
<p>(2)It must have rained, for the ground is wet. (从“地面潮湿”作出“下过雨”的推测，但地湿并不一定是下雨所致, for不可以换为because。)</p>
<p>(3)The ground is wet because it has rained. (“下雨”是“地上潮湿”的直接原因。)</p>
<p>前后两个分句间有一定的因果关系时(有时很难区分是直接原因，还是推测性原因)，for与because可以互换使用。例如：</p>
<p>(4)I could not go, for / because I was ill. 我没能去，是因为我病了。</p>
<p>(5)He felt no fear, for / because he was a brave boy. 他没有害怕，因为他是个勇敢的男孩</p>
<h2 id="定语从句中的关系代词">26. 定语从句中的关系代词</h2>
<h3 id="that-的用法以及什么情况下只能用that不能用which">that 的用法，以及什么情况下只能用that不能用which</h3>
<p>完整的定语从句语法说明：<a href="https://blog.csdn.net/iteye_9508/article/details/81796219" target="_blank" rel="noopener">摘录自</a></p>
<p><strong>1）不用that的情况</strong> a) 在引导非限定性定语从句时。 (错) The tree, that is four hundred years old, is very famous here. b) 介词后不能用。 We depend on the land from which we get our food. We depend on the land that/which we get our food from.</p>
<p><strong>2) 只能用that作为定语从句的关系代词的情况</strong> a) 在there be 句型中，只用that，不用which。 b) 在不定代词，如：anything, nothing, the one, all, much, few, any, little等这些不定代词作先行词时，只用that，不用which。 c) 先行词有the only, the very修饰时，只用that。 d) 先行词为序数词、数词、形容词最高级时，只用that。. e) 先行词既有人，又有物时。 举例： All that is needed is a supply of oil. 所需的只是供油问题。 Finally, the thief handed everything that he had stolen to the police. 那贼最终把偷的全部东西交给了警察。</p>
<h3 id="as引导定语从句">as引导定语从句</h3>
<p>as引导定语从句时，既可以引导限定性定语从句，又可以引导非限定性定语从句。区分as引导定语从句和其它从句的关键特征是：as引导定语从句时在从句中做成分，通常做主语或宾语。 ** 1、as引导限定性定语从句。**</p>
<p>** 如从句所修饰的名词前有such、the same、as出现，后面的定语从句将由as引导**，形成such...as，the same...as，as…as这样的固定结构，译为“和……一样”。 例1：I never heard such stories as he told. 我从未听过他所讲的那样的故事。</p>
<p>例2：He’ll repeat such questions as are discussed in the book. 他将重复书中讨论过的问题。</p>
<p>例3：They made the same mistake as others would have made on such an occasion. 他们犯了和其他人在这种场合下会犯的同样错误。</p>
<p>** 2、as引导非限定性定语从句**</p>
<p><strong>as引导非限定性定语从句，往往指代一整句话</strong>，通常表示“正如”的意思。as引导的非限制性定语从句位置相对比较灵活，可以位于先行词之前、之后或中间。 例：As is known to everybody, the moon travels round the earth. 众所周知，月亮绕着地球转。</p>
<h3 id="butasthan作关系代词引导定语从句">but，as，than作关系代词引导定语从句！</h3>
<p>摘录自：<a href="https://www.hjenglish.com/new/p981200/" target="_blank" rel="noopener" class="uri">https://www.hjenglish.com/new/p981200/</a> ； <a href="https://wenku.baidu.com/view/14850f6727d3240c8447eff6.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/14850f6727d3240c8447eff6.html</a></p>
<p><strong>As、but和than通常都是以介词、连词等身份被大家所熟知，而它们居然可以摇身一变成为关系代词，在定语从句中做主语和宾语。</strong></p>
<p><strong>but可被看作关系代词，引导定语从句，在句中作主语，在意义上相当于 who not或that not，即用在否定词或具有否定意义的词后，构成双重否定。</strong></p>
<p>There is <strong>no </strong>mother <strong>but</strong> loves her own children．（=There is <strong>no</strong> mother** that/who does not** love her own children．）没有不爱自己孩子的母亲。</p>
<p>He is as brave a man as ever lived. 他是世界上最勇敢的人。(as作主语）</p>
<p>Don’t read such books as are not worth reading．不要读那些不值得读的书。（as作主语）</p>
<p>Don't read such poems as you can't understnad. 不要看你（们）看不懂的诗词。 （as作宾语）</p>
<p>He is not such a man as he used to be. 他已经不是过去的那个样子了。 （as作表语）</p>
<p><strong>than既可指人，也可指物，可作关系代词来引导定语从句。than前通常有表比较的词。</strong></p>
<p>例如：Fewer friends than we had expected came to our evening party．来参加晚会的朋友比我们预料的还要少。</p>
<p>He got more money than was wanted．他得到了更多的钱。</p>
<p>This enable the delivery of more information with greater speed to** more** locations <strong>than</strong> has ever been possible.</p>
<h2 id="同位语和定语从句的区别">27. 同位语和定语从句的区别</h2>
<p>摘录自：<a href="https://wenku.baidu.com/view/71317919227916888486d7ed.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/71317919227916888486d7ed.html</a></p>
<p>同位语从句和定语从句从表面来看十分相似，但实质上是截然不同的两种从句：</p>
<h3 id="同位语从句所修饰的词是有限的一些抽象名词而定语从句就没有这种限制">（1）同位语从句所修饰的词是有限的一些抽象名词，而定语从句就没有这种限制。</h3>
<p>同位语从句经常用于下列有限的几个词后： hope（希望），idea（想法）， news（消息）， order（命令）， fact（事实）， question（问题），reason（理由），belief（相信），doubt（怀疑），evidence（根据），conclusion（结论），truth（真理），result（结果）等。</p>
<h3 id="句法功能不同">（2）句法功能不同。</h3>
<p>同位语从句同它所修饰的名词在内容上是等同关系，在句中的语法作用处于同等地位。而定语从句说明前一名词的性质、特征，对先行词进行修饰、限制，是先行词不可缺少的定语。</p>
<h3 id="引导词that在同位语从句中不作句子成分而在定语从句中必作句子成分是从句所修饰的词的替代词"><strong>（3）引导词that在同位语从句中不作句子成分；而在定语从句中必作句子成分，是从句所修饰的词的替代词。</strong></h3>
<p>试比较： The fact that the Chinese people invented the compass is known to all．中国人发明指南针这个事实是众所周知的。（同位语从句） The fact that we talked about is very important．我们谈论的这个事实很重要。（定语从句）</p>
<h3 id="如何区别同位语从句和定语从句">如何区别同位语从句和定语从句</h3>
<p>同位语从句和定语从句在形式上基本相同，都是跟在名词或代词之后，且又常由that引导。但它们的句法功能却是不同的，我们可以从三个方面来加以区别，具体说明见上述链接文章 <a href="https://wenku.baidu.com/view/71317919227916888486d7ed.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/71317919227916888486d7ed.html</a></p>
<h2 id="一些长句中短语或者词组的语序问题">28. 一些长句中短语或者词组的语序问题</h2>
<h3 id="whether-or-not">（1）whether or not</h3>
<p>例句：we just wish to determine <strong>whether or not</strong> the words you hear are understandable.** **</p>
<p><strong>whether后所接的从句末尾可以加or not, 也可以不加</strong>。</p>
<p>We use the world, church to refer to all religious insititutions, whetehr they are Christrian, Islanminc, Buddhis, and so on.</p>
<p>whether</p>
<p>conj (连词）. 是否；不论</p>
<p>pron (代词）. 两个中的哪一个</p>
<h3 id="按照英语的尾重原则principle-of-end-weight结构复杂的从句或成分置于其他成分后面">（2） 按照英语的“尾重原则（principle of end weight）”，结构复杂的从句或成分置于其他成分后面</h3>
<p>This is an abstract concept whcih** makes** (谓语) ** possible (<strong>宾补</strong>)** <strong>immense amounts of concrete and research and understanding</strong> (宾语太长了放后面）<strong>.</strong></p>
<p><strong>Thos</strong> force to exercise their smiling muscles reacted more enthusiastically to funny cartoons <strong>than did those</strong> whose mouth are contracted in a frown, suggesting that .... （than从句中的主语太长，做了倒装，did代替主句中的整个谓语，并且对比的主语没有歧义，所以此处也可以省略did)</p>
<h2 id="名词修饰名词名词作定语若后面被修饰的名词使用了复数则注意前面的修饰名词一般用单数特殊情况用复数"><strong>29. 名词修饰名词（名词作定语），若后面被修饰的名词使用了复数，则注意前面的修饰名词一般用单数，特殊情况用复数！！</strong></h2>
<p>如：This kindergarten only employs <strong>women (woman的复数) teachers</strong>.</p>
<p>转载自：<a href="http://ask.yygrammar.com/app_q-25901.html" target="_blank" rel="noopener">http://ask.yygrammar.com/app_q-25901.html</a></p>
<h3 id="名词作定语时一般用单数形式但在个别情况下也有用复数的"><strong>名词作定语时，一般用单数形式，但在个别情况下也有用复数的。</strong></h3>
<p>goods train货车，sports meeting运动会，machines hall展览机器, games console游戏机, doctors certificate of advice 医生证明， operations manager 运营经理</p>
<p>不过，有少数的复合名词，前一个词单、复数的形式都有，而各有不同的指涉，比方说：number game 是给小朋友玩的"数字游戏"，而numbers game 指的是"结果全靠机率的一件事"、time table 是"时间表"，而times table 是"九九乘法表"（这裡“times”其实是介词：“four times three is twelve”）、art education 是"美术教育"，arts education 是"（各种）艺术教育"。不确定时就用单数，通常不会错。</p>
<p><strong>注意：被修饰的名词变复数时，一般情况下，作定语用的名词不需要变为复数形式，</strong></p>
<h3 id="但由man或woman作定语修饰的名词变成复数时两部分皆要变为复数形式"><strong>但由man或woman作定语修饰的名词变成复数时，两部分皆要变为复数形式。</strong></h3>
<p>man doctor—men doctors 男医生 woman singer—women singers 女歌手</p>
<p>有的作定语用的名词有与之相应的同根形容词。</p>
<p>一般情况下，名词作定语侧重说明被修饰的名词的内容或性质（内在）；</p>
<h3 id="同根形容词作定语则常常描写被修饰的名词的特征"><strong>同根形容词作定语则常常描写被修饰的名词的特征。</strong></h3>
<p>"gold watch"指手表含有金的性质；</p>
<p>而"golden watch"则表示手表是金色的特征，不一定含有金。</p>
<p>stone house 石头造的房子 stony heart 铁石般的心肠</p>
<p>peace conference 和平会议 peaceful construction 和平建设</p>
<h3 id="名词作定语与名词所有格作定语有时是有区别的"><strong>名词作定语与名词所有格作定语有时是有区别的</strong></h3>
<p><strong>一般来说，名词作定语通常说明被修饰的词的性质，而名词所有格作定语则强调对被修饰的词的所有（权）关系或表示逻辑上的谓语关系。</strong></p>
<p>在“the Party members（党员）”中，名词定语表示members的性质；</p>
<p>在“the Party's calls（党的号召）”中，Party具有动作发出者的作用，calls虽然是 名词，却具有动作的含义 .</p>
<p>a student teacher 实习教师</p>
<p>a student's teacher 一位学生的老师</p>
<h2 id="倒装">30. 倒装</h2>
<p>摘录自：<a href="https://wenku.baidu.com/view/fd9ac363a88271fe910ef12d2af90242a895abcd.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/fd9ac363a88271fe910ef12d2af90242a895abcd.html</a></p>
<p><strong>特别注意部分倒装，是前一个从句主谓倒装还是后一个从句主谓倒装。</strong></p>
<p>部分倒装：仅仅把助动词或情态动词提前；如 Hardly did I know that. 助动词didi提前，而谓语动词know不变位置。</p>
<p>完全倒装：把谓语动词全部提前。如 Out rushed his mother. 直接谓语动词 rushed 提前</p>
<p><img src="https://img-blog.csdnimg.cn/20190925111121944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" /></p>
<p><a href="https://raw.githubusercontent.com/hellojialee/PictureBed/master/img2bolg/20200527171303.png" target="_blank" rel="noopener">备份图片地址</a></p>
<p>在这里补充知识点</p>
<p><strong>介词短语是不能做主语的。有时介词短语在前看似做主语，其实可能是完全倒装。</strong></p>
<p>例如：</p>
<p>In the next photo are my grandparents.</p>
<p>这个句子是完全倒装。完全倒装就是句子的主语和谓语的位置互换。当表示地点的副词或介词短语置于句首，而且句子的主语为名词时，句子用完全倒装。 你的这句话原本是My grandparents are in the next photo.而in the next photo置前，且主语my grandparents 是名词，用了完全倒装。 如the boy stood under the tree变成 under the tree stood the boy. 再如The man went out.相当于Out went the man. out 是表地点的副词。 但是如果是 He went out 只能变成 out he went. <strong>因为句子主语是代词，就没有倒装</strong>。</p>
<p><strong>虚拟条件句的省略倒装 </strong> 虚拟条件句的从句部分含有were， should， 或had时， <strong>可省略if，再把were， should或had 移到从句的句首，实行倒装</strong>。</p>
<p>例如：</p>
<p>Were they here now， they could help us. =If they were here now， they could help us. 他们现在在的话，就会帮助我们了。</p>
<p>Had you come earlier， you would have met him. =If you had come earlier， you would have met him. 你来得早一点，就碰到他了。</p>
<p>Should it rain， the crops would be saved. = Were it to rain，the crops would be saved.假如下雨，庄稼就有救了。 注意：在虚拟语气的从句中，动词'be'的过去时态一律用"were"，不用was，</p>
<h2 id="分词作状语">31. 分词作状语</h2>
<p>摘录自：<a href="https://wenku.baidu.com/view/1e88a91052d380eb62946de0.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/1e88a91052d380eb62946de0.html</a></p>
<p>现在分词与过去分词作状语的区别。 现在分词做状语与过去分词做状语的最主要区别在于两者与所修饰的主语的主动与被动关系的区别</p>
<p>分词作状语（关键找逻辑主语）</p>
<h3 id="a放在句首的分词往往看作-时间状语-以及-原因状语"><strong>a)放在句首的分词往往看作 时间状语 以及 原因状语</strong></h3>
<p>1. Looking (when I looked) at the picture, I couldn't help missing my middle school days. （时间状语）</p>
<p>2.Seriously injured, Allen was rushed to the hospital. （原因状语） =As he was seriously injured, Allen was rushed to the hospital.</p>
<h3 id="b放在句中或句末常常看作为伴随状态并列句"><strong>b)放在句中或句末常常看作为伴随状态(并列句) </strong></h3>
<p>The girl was left alone in the room，weeping（crying ）bitterly.</p>
<p>she watched all the gifts , greatly amazed.(=she watched all the gifts, and was greatly amazed.)</p>
<p><strong>伴随状语出现的条件是由一个主语发出两个动作或同一个生语处于两种状态，或同一个主语发出一个动作时又伴随有某一种状态。伴随状语的逻辑主语一般情况下必须是全句的生语，伴随状语与谓语动词所表示的动作或状态是同时发生的。一般而已，伴随状语中的动词具有“延续性”。而结果状语中的动词一般是非延续性的。</strong> The dog entered the room, following his master(这条狗跟着主人进了屋)。 The master entered the room,followed by his dog(主人进了屋，后面跟着他的狗)。视为“伴随状语”或“方式状语”都可以。</p>
<h3 id="c-但注意特殊generallyfrankly-speaking...-taken-as-a-whole总的来讲不考虑逻辑主语看作为独立成分"><strong>c) 但注意特殊：Generally/frankly speaking... / taken as a whole(总的来讲）不考虑逻辑主语，看作为独立成分。</strong></h3>
<h3 id="d-结果状语">d) 结果状语</h3>
<p>以下三句中的分词短语应看作“结果状语”，而不是“伴随状语”： The fire lasted nearly two days, leaving nothing valuable. 大火持续了将近两天，几乎没剩下什么值钱的东西。 My grandpa fell off the bike, breaking his right arm and leg. 我爷爷从自行车上掉下来，摔断了右胳膊和右腿。 It has rained for over ten days, causing the river to rise. 下了十多天雨，致使河水上涨。</p>
<h2 id="情态动词">31. 情态动词</h2>
<h3 id="shall-和-will-的区别-考研英语必背500句p170第150句">shall 和 will 的区别 《考研英语必背500句》P170，第150句</h3>
<p><a href="https://wenku.baidu.com/view/558d13b7551810a6f52486e5.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/558d13b7551810a6f52486e5.html</a></p>
<h3 id="will-和-would-的区别">will 和 would 的区别</h3>
<p><a href="https://zhidao.baidu.com/question/747096438925750852.html" target="_blank" rel="noopener" class="uri">https://zhidao.baidu.com/question/747096438925750852.html</a></p>
<h2 id="however-nevertheless-how-ever-的区别">32. however, nevertheless, how ever 的区别</h2>
<h3 id="however-和-nevertheless">however 和 nevertheless</h3>
<p><a href="http://m.gaosan.com/gaokao/253523.html" target="_blank" rel="noopener" class="uri">http://m.gaosan.com/gaokao/253523.html</a></p>
<p>however 作连接副词用时, 是 然而 的意思 承上启下 要用逗号隔开 nevertheless 作连接副词用时, 是“然而”的意思 与however 用法一样 如 there was no news, nevertheless, she went on hoping .消息杳然 尽管如此 她继续盼望着. 区别在于 however 所指关系比较 松弛 , nevertheless所指关系比较 紧凑, nevertheless指尽管作出完全让步，也不会发生任何影响。</p>
<p>She was very tired, nevertheless she kept on working. 她虽然很疲倦，可仍在继续工作。</p>
<p>He's stupid, but I like him nevertheless. 他是很笨，然而我喜欢他。</p>
<p><strong>更简单的处理方式是：You can always replace "nevertheless" with "however," but you cannot always replace "however" with "nevertheless." however的使用场景比nevertheless更多。</strong></p>
<p>详情见，很详细：<a href="https://wenku.baidu.com/view/155a13287375a417866f8f77.html" target="_blank" rel="noopener" class="uri">https://wenku.baidu.com/view/155a13287375a417866f8f77.html</a></p>
<h3 id="however-和-how-ever">however 和 how ever</h3>
<p>however是副词或者是连接副词，意为“不论如何，但是”，<strong>而how ever中的ever是起强调作用，意为“究竟怎样”</strong></p>
<p>How ever did you manage to get the car started? 你究竟如何把车发动起来的？</p>
<h2 id="there-be-n-doingto-dodone-句型的辨析">33. There be + n + doing/to do/done 句型的辨析</h2>
<p>there be +名词+ doing 或 there be + 名词+ done 及 there be +名词+ to do 这三个句型都是there be 句型，there be 当“有”讲，确是倒装句。句中的名词是主语，所以谓语动词be与其后的名词在人称和数上保持一致（即英语的主谓一致原则），<strong>而 doing，done 还有to do 在there be 句型中都是定语，修饰其前面的名词！</strong> 所以呀，there be 后不可能直接加doing , 只能是加了名词后，才可能根据语义需要加出doing,done或者to do 结构。</p>
<p>1、解释：There be + 主语 + to do …通常表示动作尚未发生…如：</p>
<p>There are a lot of flowers to be watered. 那里还有很多花没被浇水。</p>
<p>There was nobody to look after the child. 没有人照顾这孩子。</p>
<p>There was a large crowd to send him off. 有一大群人要来给他送行。</p>
<p>注：当其中的宾语与其后的不定式为被动关系时，可用主动表被动，也可用被动式：</p>
<p>如：There is much work to do. = There is much work to be done. 有许多工作要做。</p>
<p><strong>There is much homework to do</strong>.= There is much homework to be done.有许多作业要做。</p>
<p>2、There be + 主语 + doing …通常表示动作正在发生：</p>
<p>There are many boys playing basketball. 那里有许多男孩正在打篮球。</p>
<p><strong>There is a boy standing under the tree</strong>.大树下有个男孩。</p>
<h2 id="一些可以提升逼格的高级词汇">34. 一些可以提升逼格的高级词汇</h2>
<p>said: adj, 上述的,用来替换mentioned above</p>
<p>e.g. Clicking on the "LaTeXiT" button on the bottom right will produce <strong>said </strong>formula in the viewer</p>
<h2 id="based-on-与-on-the-basis-of用法不同">35. based on 与 on the basis of用法不同</h2>
<p><a href="https://wallaceediting.cn/blog/progress/grammar-usage/%E5%A6%82%E4%BD%95%E4%BE%9D%E7%85%A7%E6%9C%9F%E5%88%8A%E8%A7%84%E5%AE%9A%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8based-on%E4%B8%8Eon-the-basis-of.html" target="_blank" rel="noopener">原文</a></p>
<p>based on不能替换according to，而 on the basis of能替换according to。</p>
<p><strong>词组 based on 和 on the basis of 经常被替换使用，然而，这是不恰当的</strong>。因为based on 是分词，可被用来修饰名词、代名词和名词词组；而需修饰动词时，只能用介词词组on the basis of 。Based on 具有动词或形容词的功用，做形容词时，可修饰前置的名词或代名词，例如： ˙ This conclusion is based on four years of experience. (base on这里当动词用) ˙ Conclusions based on experience may still require testing. (based on这里修饰最邻近的名词）</p>
<p>若要修饰动词，请使用词组on the basis of。 例句1：病句：Based on the first four years of results, we discarded the original hypothesis. （这里逻辑错误，based on修饰主句中的主语we） 修正句：On the basis of our results, we discarded the original hypothesis.</p>
<p>例句2：病句：The administration sent a document on the basis of your suggestion. 修正句：The administration sent a document based on your suggestion.</p>
<p>在例句2的修正句中，based on 被用来修饰前置名词 document，显示文件内容是基于your suggestion。而在原句中，on the basis of修饰动词send，表示根据your suggestion，文件已寄出。简单来说，当要表达according to（根据）的意思时，可以使用on the basis of，而当表达has a foundation in/has resulted from（基于／由于）的意思时，则应使用 based on。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>易错点</tag>
      </tags>
  </entry>
  <entry>
    <title>Apex相关（同时涉及egg-info的说明）</title>
    <url>/2020/02/12/Apex%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<h2 id="what-is-the-difference-between-fusedadam-optimizer-in-nvidia-amp-package-with-the-adam-optimizer-in-pytorch">What is the difference between FusedAdam optimizer in Nvidia AMP package with the Adam optimizer in Pytorch?</h2>
<p><a href="https://discuss.pytorch.org/t/fusedadam-optimizer-in-nvidia-amp-package/47544" target="_blank" rel="noopener">摘录自</a></p>
<p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries out optimizer.step() by looping over parameters, and launching a series of kernels for each parameter. This can require hundreds of small launches that are mostly bound by CPU-side Python looping and kernel launch overhead, resulting in poor device utilization. Currently, the FusedAdam implementation in Apex flattens the parameters for the optimization step, then carries out the optimization step itself via a fused kernel that combines all the Adam operations. In this way, the loop over parameters as well as the internal series of Adam operations for each parameter are fused such that optimizer.step() requires only a few kernel launches.</p>
<p>The current implementation (in Apex master) is brittle and only works with Amp opt_level O2. I’ve got a WIP branch to make it work for any opt_level (<a href="https://github.com/NVIDIA/apex/pull/351" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/pull/351</a>). I recommend waiting until this is merged then trying it.</p>
<h2 id="how-to-use-tensor-cores">How to use Tensor Cores</h2>
<p><a href="https://github.com/NVIDIA/apex/issues/221" target="_blank" rel="noopener">摘录自</a></p>
<p><strong>Convolutions:</strong> For cudnn versions 7.2 and ealier, <span class="citation" data-cites="vaibhav0195">@vaibhav0195</span> is correct: input channels, output channels, and batch size should be multiples of 8 to use tensor cores. However, this requirement is lifted for cudnn versions 7.3 and later. For cudnn 7.3 and later, you don't need to worry about making your channels/batch size multiples of 8 to enable Tensor Core use.</p>
<p><strong>GEMMs (fully connected layers):</strong> For matrix A x matrix B, where A has size [I, J] and B has size [J, K], I, J, and K must be multiples of 8 to use Tensor Cores. This requirement exists for all cublas and cudnn versions. This means that for bare fully connected layers, the batch size, input features, and output features must be multiples of 8, and for RNNs, you usually (but not always, it can be architecture-dependent depending on what you use for encoder/decoder) need to have batch size, hidden size, embedding size, and dictionary size as multiples of 8.</p>
<p><strong>It may also help to set torch.backends.cudnn.benchmark=True</strong> at the top of your script, which enables pytorch‘s autotuner. Each time pytorch encounters a new set of convolution parameters, it will test all available cudnn algorithms to find the fastest one, then cache that choice to reuse whenever it encounters the same set of convolution parameters again. The first iteration of your network will be slower as pytorch tests all the cudnn algorithms for each convolution, but the second iteration and later iterations will likely be faster.</p>
<h2 id="fp16半精度带来的精度误差">FP16半精度带来的精度误差</h2>
<figure>
<img src="https://img-blog.csdnimg.cn/20190911164622328.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<h2 id="install-nvidia-apex">Install Nvidia Apex</h2>
<p>若第一次安装需要把项目从github克隆到本地</p>
<h3 id="clean-the-old-install-before-rebuilding">Clean the old install before rebuilding:</h3>
<blockquote>
<p>pip uninstall apex cd apex_repo_dir rm -rf build (if present) rm -rf apex.egg-info (if present)</p>
</blockquote>
<h3 id="install-package">Install package：</h3>
<p>注：<strong>--no-cache-dir功能: 不使用缓存在pip目录下的cache中的文件</strong></p>
<blockquote>
<p>pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./ 或者 python setup.py install --cuda_ext --cpp_ext</p>
</blockquote>
<h3 id="扩展1.-pip-install---editable-.-vs-python-setup.py-develop">扩展1. <code>pip install --editable .</code> vs <code>python setup.py develop</code></h3>
<p><a href="https://stackoverflow.com/questions/30306099/pip-install-editable-vs-python-setup-py-develop" target="_blank" rel="noopener">转载自</a> Try to avoid calling setup.py directly, it will not properly tell pip that you've installed your package.</p>
<p><strong>With pip install -e</strong>:</p>
<p>其中 -e 选项全称是--editable. For local projects, the “SomeProject.egg-info” directory is created relative to the project path (<strong>相对于此项目目录的路径</strong>). This is one advantage over just using setup.py develop, which creates the “egg-info” directly relative the current working directory (<strong>相对于当前工作环境目录的路径</strong>).</p>
<h3 id="扩展2.-弄懂一个命令-pip3-install---editable-.traintest">扩展2. 弄懂一个命令 <code>pip3 install --editable '.[train,test]'</code></h3>
<p><a href="https://github.com/vita-epfl/openpifpaf/blob/21baabf9c6bbd0bea3e8e465a726abfa8dbeeccf/setup.py#L76" target="_blank" rel="noopener">例子在这里</a> When you then did <code>pip install --editable .</code>, the command installs the Python package in the current directory (signified by the dot .) with the optional dependencies needed for training and testing ('[train,test]'). 上面的安装命令中，-e选项全称是--editable，也就是可编辑的意思，以可继续开发的模式进行安装，<font color="#dd00dd"> '.' 表示当前目录，也就是setup.py存在的 那个目录，此时pip install将会把包安装在当前文件目录下，而不是安装到所使用的python环境中的-site-packages。</font> [train,test] 只是我们举的一个例字，是可选参数，在setup.py中可以找到这两个选项（也可能叫其他名字或者根本就没有）之下包含了哪些第三方包。</p>
<h3 id="扩展3.-关于egg-info">扩展3. 关于egg-info</h3>
<p>注意⚠️：选则本地安装<code>pip install .</code>成功安装完成后，apex.egg-info文件夹可以只处于当前项目文件夹下而不是安装在系统环境中，只需要在当前使用的python虚拟环境-site-packages中一个指向该egg-info文件的超链接即可(这个是在本地安装自动的行为，不需要我们关心操作)，这样就能找到使用Apex包时所需的apex.egg-info文件夹里的信息。</p>
<h3 id="ps-如果遇到cuda版本不兼容的问题解决办法见升级pytoch-1.3后-cuda10.1不匹配版本的警告已经消失">PS: 如果遇到Cuda版本不兼容的问题，解决办法见（升级pytoch 1.3后 cuda10.1不匹配版本的警告已经消失）：</h3>
<p><a href="https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952" target="_blank" rel="noopener">https://github.com/NVIDIA/apex/issues/350#issuecomment-500390952</a></p>
<p>如果没有出现其他error，可以直接使用上面链接的建议，删除版本检查抛出的报错。见以下讨论：</p>
<p><a href="https://github.com/NVIDIA/apex/issues/350" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/issues/350</a></p>
<p><a href="https://github.com/NVIDIA/apex/pull/323" target="_blank" rel="noopener" class="uri">https://github.com/NVIDIA/apex/pull/323</a></p>
<h3 id="apex的使用">Apex的使用</h3>
<h4 id="命令行启动训练">命令行启动训练</h4>
<p>---也是如何Pycharm运行时添加命令行参数的例子</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=4 train_distributed.py</span><br></pre></td></tr></table></figure>
<h4 id="不使用命令行运行而是使用pycharm启动同步夸卡训练的配置">不使用命令行运行，而是使用Pycharm启动同步夸卡训练的配置</h4>
<figure>
<img src="https://img-blog.csdnimg.cn/20200212185619928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><figcaption>在这里插入图片描述</figcaption>
</figure>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>python</tag>
        <tag>环境配置</tag>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch中的Batch Normalization layer踩坑</title>
    <url>/2020/02/03/Pytorch%E4%B8%AD%E7%9A%84Batch-Normalization-layer%E8%B8%A9%E5%9D%91/</url>
    <content><![CDATA[<h2 id="注意momentum的定义">1. 注意momentum的定义</h2>
<p>Pytorch中的BN层的动量平滑和常见的动量法计算方式是相反的，默认的momentum=0.1 <span class="math display">\[
\hat{x}_{\text { new }}=(1-\text { momentum }) \times \hat{x}+\text { momemtum } \times x_{t}
\]</span> BN层里的表达式为： <span class="math display">\[
y=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}} * \gamma+\beta
\]</span> 其中<em>γ</em>和<em>β</em>是可以学习的参数。在Pytorch中，BN层的类的参数有：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASS torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>每个参数具体含义参见文档，需要注意的是，affine定义了BN层的参数<em>γ</em>和<em>β</em>是否是可学习的(不可学习默认是常数1和0).</p>
<h2 id="注意bn层中含有统计数据数值即均值和方差">2. 注意BN层中含有统计数据数值，即均值和方差</h2>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
<p>在训练过程中model.train()，train过程的BN的统计数值—均值和方差是<strong>通过当前batch数据估计的</strong>。</p>
<p>并且测试时，model.eval()后，若track_running_stats=True，模型此刻所使用的统计数据是Running status 中的，即通过指数衰减规则，积累到当前的数值。否则依然使用基于当前batch数据的估计值。</p>
<h2 id="bn层的统计数据更新是在每一次训练阶段model.train后的forward方法中自动实现的而不是在梯度计算与反向传播中更新optim.step中完成">3. BN层的统计数据更新是在每一次训练阶段model.train()后的forward()方法中自动实现的，<strong>而不是</strong>在梯度计算与反向传播中更新optim.step()中完成</h2>
<h2 id="冻结bn及其统计数据">4. 冻结BN及其统计数据</h2>
<p>从上面的分析可以看出来，正确的冻结BN的方式是在模型训练时，把BN单独挑出来，重新设置其状态为eval (在model.train()之后覆盖training状态）.</p>
<p>解决方案：<a href="https://discuss.pytorch.org/t/freeze-batchnorm-layer-lead-to-nan/8385" target="_blank" rel="noopener">转载自</a></p>
<blockquote>
<p>You should use apply instead of searching its children, while named_children() doesn’t iteratively search submodules.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_bn_eval</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">      m.eval()</span><br><span class="line"></span><br><span class="line">model.apply(set_bn_eval)</span><br></pre></td></tr></table></figure>
<p>或者，重写module中的train()方法：<a href="https://discuss.pytorch.org/t/how-to-train-with-frozen-batchnorm/12106/8" target="_blank" rel="noopener">转载自</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, mode=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Override the default train() to freeze the BN parameters</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(MyNet, self).train(mode)</span><br><span class="line">        <span class="keyword">if</span> self.freeze_bn:</span><br><span class="line">            print(<span class="string">"Freezing Mean/Var of BatchNorm2D."</span>)</span><br><span class="line">            <span class="keyword">if</span> self.freeze_bn_affine:</span><br><span class="line">                print(<span class="string">"Freezing Weight/Bias of BatchNorm2D."</span>)</span><br><span class="line">        <span class="keyword">if</span> self.freeze_bn:</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> self.backbone.modules():</span><br><span class="line">                <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                    m.eval()</span><br><span class="line">                    <span class="keyword">if</span> self.freeze_bn_affine:</span><br><span class="line">                        m.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">                        m.bias.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="fixfrozen-batch-norm-when-training-may-lead-to-runtimeerror-expected-scalar-type-half-but-found-float">5. Fix/frozen Batch Norm when training may lead to RuntimeError: expected scalar type Half but found Float</h2>
<p>解决办法：<a href="https://github.com/NVIDIA/apex/issues/122" target="_blank" rel="noopener">转载自</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> apex.fp16_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fix_bn</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.eval()</span><br><span class="line"></span><br><span class="line">model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.cuda()</span><br><span class="line">model = network_to_half(model)</span><br><span class="line">model.train()</span><br><span class="line">model.apply(fix_bn) <span class="comment"># fix batchnorm</span></span><br><span class="line">input = Variable(torch.FloatTensor(<span class="number">8</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>).cuda().half())</span><br><span class="line">output = model(input)</span><br><span class="line">output_mean = torch.mean(output)</span><br><span class="line">output_mean.backward()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Please do</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def fix_bn(m):</span><br><span class="line"> classname &#x3D; m.__class__.__name__</span><br><span class="line"> if classname.find(&#39;BatchNorm&#39;) !&#x3D; -1:</span><br><span class="line">     m.eval().half()</span><br></pre></td></tr></table></figure>
<p>Reason for this is, for regular training it is better (performance-wise) <font color="#dd0000">to <strong>use cudnn batch norm, which requires its weights to be in fp32</strong>, thus batch norm modules are not converted to half in <code>network_to_half</code>. However, cudnn does not support batchnorm backward in the eval mode</font> , which is what you are doing, and to use pytorch implementation for this, weights have to be of the same type as inputs.</p>
</blockquote>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>L1，L2正则化的理解</title>
    <url>/2020/01/28/L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>摘录自：</p>
<p>https://zhuanlan.zhihu.com/p/35356992</p>
<p>https://zhuanlan.zhihu.com/p/29360425</p>
<a id="more"></a>
<h2 id="正则化理解之结构最小化">正则化理解之结构最小化</h2>
<h3 id="首先给出一个例子解释l1的作用可以使得模型获得稀疏解">首先给出一个例子解释<strong>L1的作用可以使得模型获得稀疏解</strong></h3>
<p><a href="https://blog.csdn.net/xiaojiajia007/article/details/90611838" target="_blank" rel="noopener">点击查看：L1正则能够使得模型的解稀疏</a> <img src="https://img-blog.csdnimg.cn/20190529105352198.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<p><strong>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。</strong></p>
<p>给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。</p>
<p><strong>L1正则化和L2正则化：</strong></p>
<p>L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。</p>
<h3 id="作图说明">作图说明</h3>
<p><a href="https://blog.csdn.net/liangdong2014/article/details/79517638" target="_blank" rel="noopener">摘录自</a></p>
<p><img src="https://img-blog.csdnimg.cn/20190529163213574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> <strong>从等高线和取值空间的交点可以看到L1更容易倾向一个权重偏大一个权重为0。L2更容易倾向权重都较小。</strong></p>
<p>而通过求导数可以看出，对于两种正则带来的梯度更新：</p>
<ul>
<li>L1减少的是一个常量，L2减少的是权重的固定比例</li>
<li>孰快孰慢取决于权重本身的大小，权重刚大时可能L2快，较小时L1快</li>
<li>L1使权重稀疏，L2使权重平滑，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0</li>
</ul>
<hr />
<h2 id="正则化理解之最大后验概率估计map">正则化理解之最大后验概率估计（MAP）</h2>
<h3 id="在最大似然估计中1">在最大似然估计中<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h3>
<p>设<span class="math inline">\(X\)</span>、<span class="math inline">\(y\)</span>为训练样本和相应的标签，<span class="math inline">\(X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)\)</span> 是一组抽样数据，满足独立同分布假设（i.i.d），假设权重<span class="math inline">\(w\)</span>是未知的参数，从而求得对数<strong>似然函数</strong>: <span class="math display">\[
\operatorname{MLP}=l(w)=\log [P(y | X ; w)]=\log \left[\prod_{i} P\left(y^{i} | x^{i} ; w\right)\right]
\]</span> 通过假设<span class="math inline">\(y^{i}\)</span>的不同概率分布，即可得到不同的模型（即变成已知模型形式，拟合模型参数的问题，<font color="#000066">w的写法是前面加分号，表示它是某一固定参数值，而不是概率条件！</font>）。例如若假设 <span class="math inline">\(y^{i} \sim N\left(w^{T} x^{i}, \sigma^{2}\right)\)</span> 的高斯分布（<span class="math inline">\(x^{i}\)</span>也是一系列随机向量，随机向量的每个分量都对<span class="math inline">\(y^{i}\)</span>有影响，若随机向量的维度很大，可以认为<span class="math inline">\(y^{i}\)</span>服从正态分布，而一般正态分布可以转化成标准正态分布求解），则有： <span class="math display">\[
l(w)=\log \left[\prod_{i} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y^{i}-w^{T} x^{i}\right)^{2}}{2 \sigma^{2}}}\right]=-\frac{1}{2 \sigma^{2}} \sum_{i}\left(y^{i}-w^{T} x^{i}\right)^{2}+C
\]</span> 式子中<span class="math inline">\(C\)</span>是常数项，常数项和系数项不影响求最大值，因而可令<span class="math inline">\(J(w ; X, y)=-l(w)\)</span>即可得到线性回归的代价函数。这里我们可以看到，假设<span class="math inline">\(y^{i}\)</span> <font color=purple face=bold>服从正态分布的极大似然估计方法和均方误差最小化求解线性回归的结果是一样的！</font></p>
<h3 id="在最大后验概率估计中">在最大后验概率估计中</h3>
<p>将权重<span class="math inline">\(w\)</span>看作随机变量，也具有某种分布，从而有： <span class="math display">\[
P(w|X,y)=\frac { P(w,X,y) }{ P(X,y) } =\frac { P(X,y|w)P(w) }{ P(X,y) } =\frac { P(X)P(y|w,X)P(w) }{ P(X,y) } \propto P(y|X,w)P(w)
\]</span> 上面式子中<span class="math inline">\(P(X,y)\)</span>等对于特定问题已经是固定值了，与<span class="math inline">\(w\)</span>无关，所以求的<span class="math inline">\(P(w|X,y)\)</span>正比于<span class="math inline">\(P(y|X,w)P(w)\)</span>。 那我们利用最大后验概率估计求参数 <span class="math inline">\(w\)</span> 的时候，同样取对数有<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>： <span class="math display">\[
\operatorname{MAP}=\log P(y | X, w) P(w)=\log P(y | X, w)+\log P(w)
\]</span> 可以看出后验概率函数为在似然函数的基础上增加了一项 <span class="math inline">\(\log P(w)\)</span>。<span class="math inline">\(P(w)\)</span>的意义是对权重系数<span class="math inline">\(w\)</span>的概率分布的先验假设,在收集到训练样本{<span class="math inline">\(X,y\)</span>}后，则可根据w在{<span class="math inline">\(X,y\)</span>}下的后验概率对<span class="math inline">\(w\)</span>进行修正，从而可以对<span class="math inline">\(w\)</span>更好地估计。</p>
<blockquote>
<p><strong>这里补充一下周志华老师的西瓜书149页的知识：</strong></p>
<p>概率学派认为参数虽然未知，但确实是客观存在的固定值，而贝叶斯学派则认为参数是未观察到的随机变量，其本身也有分布。因此可以先假定参数服从某个先验分布（没有观测到任何当前的数据前的先验知识），然后基于当前的观测值来计算参数的后验分布。</p>
</blockquote>
<h3 id="若假设w_j的先验分布为0均值的高斯分布即w_j-sim-nleft0-sigma2right">若假设<span class="math inline">\(w_{j}\)</span>的先验分布为0均值的高斯分布，即<span class="math inline">\(w_{j} \sim N\left(0, \sigma^{2}\right)\)</span>，</h3>
<p>则有： <span class="math display">\[
\log P(w)=\log \prod_{j} P\left(w_{j}\right)=\log \prod_{j}\left[\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(w_{j}\right)^{2}}{2 \sigma^{2}}}\right]=-\frac{1}{2 \sigma^{2}} \sum_{j} w_{j}^{2}+C^{\prime}
\]</span> <img src="https://img-blog.csdnimg.cn/20190529105226461.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="高斯分布" /> 可以看到，在高斯分布下<span class="math inline">\(\log{P(w)}\)</span>的效果等价于在代价函数中增加<span class="math inline">\(L_{2}\)</span>正则项，也就是说<strong>在MAP中使用一个高斯分布的先验等价于在MLE中采用L2的正则</strong>。从上图可以看出<span class="math inline">\(w\)</span>值取到0附近的概率特别大。也就是说我们提前先假设了<span class="math inline">\(w\)</span>的解更容易取到0的附近。</p>
<h3 id="若假设w_j服从均值为0参数为a的拉普拉斯分布即pleftw_jrightfrac12-a-efrac-leftw_jrighta">若假设<span class="math inline">\(w_{j}\)</span>服从均值为0、参数为a的拉普拉斯分布，即：<span class="math inline">\(P\left(w_{j}\right)=\frac{1}{2 a} e^{\frac{-\left|w_{j}\right|}{a}}\)</span></h3>
<p>则有： <span class="math display">\[
\log P(w)=\log \prod_{j} \frac{1}{2 a} e^{\frac{-\left|w_{j}\right|}{a}}=-\frac{1}{a} \sum_{j}\left|w_{j}\right|+C^{\prime}
\]</span> <img src="https://img-blog.csdnimg.cn/2019052910525070.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDc=,size_16,color_FFFFFF,t_70" alt="拉普拉斯分布" /> 可以看到，在拉普拉斯分布下<span class="math inline">\(\log{P(w)}\)</span>的效果等价于在代价函数中增加<span class="math inline">\(L_{1}\)</span>正则项。从下图可以看出<span class="math inline">\(w\)</span>值取到0的概率特别大。也就是说我们提前先假设了<span class="math inline">\(w\)</span>的解更容易取到0。</p>
<h3 id="我们得到对于l_1l_2正则化的一种最大后验角度理解">我们得到对于<span class="math inline">\(L_{1}\)</span>、<span class="math inline">\(L_{2}\)</span>正则化的一种最大后验角度理解</h3>
<ul>
<li><span class="math inline">\(L_{1}\)</span>正则化可通过假设权重<span class="math inline">\(w\)</span> 的先验分布为拉普拉斯分布，由最大后验概率估计导出</li>
<li><span class="math inline">\(L_{2}\)</span>正则化可通过假设权重<span class="math inline">\(w\)</span> 的先验分布为高斯分布，由最大后验概率估计导出</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="math inline">\(\arg \max \sum_{i=1}^{n} \log P\left(x_{i} ; \theta\right)\)</span>。对于离散的用分布律P，对于连续型的变量在数学中已知模型表达式则用概率密度函数f<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p>辨析：MLP（Maximum A Posteriori）是最大后验概率估计（贝叶斯学派）； MAP（Maximum Likelihood Estimation）是最大似然估计（概率学派）。<a href="https://zhuanlan.zhihu.com/p/32480810" target="_blank" rel="noopener">请参考</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn3" role="doc-endnote"><p>这里应该是省略的写法，比如<span class="math inline">\(P(w|X,y)\)</span>，里面三个都是随机变量，实际上对于一个具体的训练样本完整的写法应该是<span class="math inline">\(P(w=a|X=x1,Y=y1)\)</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>一个 Pytorch 训练实践 (分布式训练 + 半精度_混合精度训练)</title>
    <url>/2020/01/28/%E4%B8%80%E4%B8%AA%20Pytorch%20%E8%AE%AD%E7%BB%83%E5%AE%9E%E8%B7%B5%20%EF%BC%88%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%20+%20%E5%8D%8A%E7%B2%BE%E5%BA%A6_%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%89/</url>
    <content><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/96408719" target="_blank" rel="noopener" title="Permalink to 一个 Pytorch 训练实践 （分布式训练 + 半精度/混合精度训练）">Source</a></p>
<h3 id="内容速览">内容速览</h3>
<ol type="1">
<li>'train.py': single training process on one GPU only.</li>
<li>'train_parallel.py': signle training process on multiple GPUs using <strong>Dataparallel</strong> (包括不同GPU之间的负载均衡).</li>
<li>'train_distributed.py' (<strong>recommended</strong>): multiple training processes on multiple GPUs using <strong>Nvidia Apex</strong> &amp; <strong>Distributed Training:</strong></li>
</ol>
<p><code>python -m torch.distributed.launch --nproc_per_node=4 train_distributed.py</code></p>
<a id="more"></a>
<h3 id="项目完整代码地址">项目（完整代码）地址</h3>
<p>网页地址：</p>
<p><a href="https://link.zhihu.com/?target=https%253A//github.com/jialee93/Improved-Body-Parts">https://github.com/jialee93/Improved-Body-Parts​github.com</a></p>
<p><strong>Please cite <a href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">this paper</a> kindly in your publications if the corresponding projects helps your research.</strong></p>
<p><a href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">https://arxiv.org/abs/1911.10529​arxiv.org</a></p>
<pre><code>inproceedings{li2019simple,
    title={Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation},
    author={Jia Li and Wen Su and Zengfu Wang},
    booktitle = {arXiv preprint arXiv:1911.10529},
    year={2019}
}</code></pre>
<h3 id="项目声明">项目声明</h3>
<p>最近开源了一个项目的代码，因为平时有很多其他事情需要处理，业余时间自己又喜欢享受生活，科研对我来说简直彻底成为了副业，羞愧。</p>
<p>之前一直使用TensorFlow以及Keras，但是一直觉得它们没有继承Python丝滑的特性，所以最近果断转移到Pytorch。因此在做这份工作时，我以新入门的用户身份开始，把整个训练和测试流程给走一遍，踩到并且努力解决全过程的坑。这里也是给自己实验过程的踩坑做个总结，并且希望通过这个分享能够提高自己工作的关注度，以及希望自己的经验能够为他人所用。当然了，如果我的分享能够帮助到大家（<strong>主要针对Pytorch新手或者普通玩家，高级玩家请忽略本菜鸟，轻拍，谢谢</strong>），也欢迎引用我的对应的项目论文。</p>
<h3 id="多gpu分布式训练混合精度加速训练">多GPU分布式训练+混合精度加速训练</h3>
<p>对应上面开源链接的<em>train_distributed.py</em>脚本。相信最直接的代码以及注释就是最好的说明。该脚本同时包含了最常用的代码模版，包括例如 多进程的训练数据准备，模型权重的保存与载入，冻结部分网络层的权重，变相增加batch size，使用Nvidia官方的Apex包通过半精度或混合精度进行模型压缩和加速等等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> apex.optimizers <span class="keyword">as</span> apex_optim</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> config.config <span class="keyword">import</span> GetConfig, COCOSourceConfig, TrainingOpt</span><br><span class="line"><span class="keyword">from</span> data.mydataset <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> models.posenet <span class="keyword">import</span> Network</span><br><span class="line"><span class="keyword">from</span> models.loss_model <span class="keyword">import</span> MultiTaskLoss</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> apex.optimizers <span class="keyword">as</span> apex_optim</span><br><span class="line">    <span class="keyword">from</span> apex.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line">    <span class="keyword">from</span> apex.fp16_utils <span class="keyword">import</span> *</span><br><span class="line">    <span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br><span class="line">    <span class="keyword">from</span> apex.multi_tensor_apply <span class="keyword">import</span> multi_tensor_applier</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">raise</span> ImportError(<span class="string">"Please install apex from https://www.github.com/nvidia/apex to run this example."</span>)</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'PoseNet Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--resume'</span>, <span class="string">'-r'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>, help=<span class="string">'resume from checkpoint'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--freeze'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    help=<span class="string">'freeze the pre-trained layers before output layers'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--warmup'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>, help=<span class="string">'using warm-up learning rate'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--checkpoint_path'</span>, <span class="string">'-p'</span>, default=<span class="string">'link2checkpoints_distributed'</span>, help=<span class="string">'save path'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--max_grad_norm'</span>, default=<span class="number">10</span>, type=float,</span><br><span class="line">                    help=(<span class="string">"If the norm of the gradient vector exceeds this, "</span></span><br><span class="line">                          <span class="string">"re-normalize it to have the norm equal to max_grad_norm"</span>))</span><br><span class="line"><span class="comment"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied automatically by torch.distributed.launch.</span></span><br><span class="line">parser.add_argument(<span class="string">"--local_rank"</span>, default=<span class="number">0</span>, type=int)</span><br><span class="line">parser.add_argument(<span class="string">'--opt-level'</span>, type=str, default=<span class="string">'O1'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--sync_bn'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">True</span>,</span><br><span class="line">                    help=<span class="string">'enabling apex sync BN.'</span>)  <span class="comment"># 无触发为false， -s 触发为true</span></span><br><span class="line">parser.add_argument(<span class="string">'--keep-batchnorm-fp32'</span>, type=str, default=<span class="literal">None</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--loss-scale'</span>, type=str, default=<span class="literal">None</span>)  <span class="comment"># '1.0'</span></span><br><span class="line">parser.add_argument(<span class="string">'--print-freq'</span>, <span class="string">'-f'</span>, default=<span class="number">10</span>, type=int, metavar=<span class="string">'N'</span>, help=<span class="string">'print frequency (default: 10)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="comment"># ###################################  Setup for some configurations ###########################################</span></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span>  <span class="comment"># 如果我们每次训练的输入数据的size不变，那么开启这个就会加快我们的训练速度</span></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">checkpoint_path = args.checkpoint_path</span><br><span class="line">opt = TrainingOpt()</span><br><span class="line">config = GetConfig(opt.config_name)</span><br><span class="line">soureconfig = COCOSourceConfig(opt.hdf5_train_data)  <span class="comment"># # 对于分布式训练，total_batch size = batch_size*world_size</span></span><br><span class="line">train_data = MyDataset(config, soureconfig, shuffle=<span class="literal">False</span>, augment=<span class="literal">True</span>)  <span class="comment"># shuffle in data loader</span></span><br><span class="line"></span><br><span class="line">soureconfig_val = COCOSourceConfig(opt.hdf5_val_data)</span><br><span class="line">val_data = MyDataset(config, soureconfig_val, shuffle=<span class="literal">False</span>, augment=<span class="literal">False</span>)  <span class="comment"># shuffle in data loader</span></span><br><span class="line"></span><br><span class="line">best_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">start_epoch = <span class="number">0</span>  <span class="comment"># 从0开始或者从上一个epoch开始</span></span><br><span class="line"></span><br><span class="line">args.distributed = <span class="literal">False</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'WORLD_SIZE'</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">    args.distributed = int(os.environ[<span class="string">'WORLD_SIZE'</span>]) &gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">args.gpu = <span class="number">0</span></span><br><span class="line">args.world_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,</span></span><br><span class="line"><span class="comment"># the 'WORLD_SIZE' environment variable will also be set automatically.</span></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    args.gpu = args.local_rank</span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    <span class="comment"># Initializes the distributed backend which will take care of synchronizing nodes/GPUs</span></span><br><span class="line">    torch.distributed.init_process_group(backend=<span class="string">'nccl'</span>, init_method=<span class="string">'env://'</span>)</span><br><span class="line">    args.world_size = torch.distributed.get_world_size()  <span class="comment"># 获取分布式训练的进程数</span></span><br><span class="line">    print(<span class="string">"World Size is :"</span>, args.world_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.backends.cudnn.enabled, <span class="string">"Amp requires cudnn backend to be enabled."</span></span><br><span class="line"></span><br><span class="line">model = Network(opt, config, dist=<span class="literal">True</span>, bn=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.sync_bn:  <span class="comment"># 用累计loss来达到sync bn 是不是更好，更改bn的momentum大小</span></span><br><span class="line">    <span class="comment">#  This should be done before model = DDP(model, delay_allreduce=True),</span></span><br><span class="line">    <span class="comment">#  because DDP needs to see the finalized model parameters</span></span><br><span class="line">    <span class="comment"># We rely on torch distributed for synchronization between processes. Only DDP support the apex sync_bn now.</span></span><br><span class="line">    <span class="keyword">import</span> apex</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Using apex synced BN."</span>)</span><br><span class="line">    model = apex.parallel.convert_syncbn_model(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># It should be called before constructing optimizer if the module will live on GPU while being optimized.</span></span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">        print(<span class="string">'Parameters of network: Autograd'</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="comment"># ######################################## Froze some layers to fine-turn the model  ########################</span></span><br><span class="line"><span class="comment"># ##############################################################################################################</span></span><br><span class="line"><span class="keyword">if</span> args.freeze:</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():  <span class="comment"># 带有参数名的模型的各个层包含的参数遍历</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'out'</span> <span class="keyword">or</span> <span class="string">'merge'</span> <span class="keyword">or</span> <span class="string">'before_regress'</span> <span class="keyword">in</span> name:  <span class="comment"># 判断参数名字符串中是否包含某些关键字</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># #############################################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Actual working batch size on multi-GPUs is 4 times bigger than that on one GPU</span></span><br><span class="line"><span class="comment"># fixme: add up momentum if the batch grows?</span></span><br><span class="line"><span class="comment"># fixme: change weight_decay?</span></span><br><span class="line"><span class="comment">#    nesterov = True</span></span><br><span class="line"><span class="comment"># optimizer = apex_optim.FusedSGD(filter(lambda p: p.requires_grad, model.parameters()),</span></span><br><span class="line"><span class="comment">#                                 lr=opt.learning_rate * args.world_size, momentum=0.9, weight_decay=5e-4)</span></span><br><span class="line">optimizer = optim.SGD(filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()),</span><br><span class="line">                      lr=opt.learning_rate * args.world_size, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"><span class="comment"># optimizer = apex_optim.FusedAdam(model.parameters(), lr=opt.learning_rate * args.world_size, weight_decay=1e-4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置学习率下降策略, extract the "bare"  Pytorch optimizer before Apex wrapping.</span></span><br><span class="line"><span class="comment"># scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.4, last_epoch=-1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Amp.  Amp accepts either values or strings for the optional override arguments,</span></span><br><span class="line"><span class="comment"># for convenient interoperation with argparse.</span></span><br><span class="line"><span class="comment"># For distributed training, wrap the model with apex.parallel.DistributedDataParallel.</span></span><br><span class="line"><span class="comment"># This must be done AFTER the call to amp.initialize.</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer,</span><br><span class="line">                                  opt_level=args.opt_level,</span><br><span class="line">                                  keep_batchnorm_fp32=args.keep_batchnorm_fp32,</span><br><span class="line">                                  loss_scale=args.loss_scale)  <span class="comment"># Dynamic loss scaling is used by default.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    <span class="comment"># By default, apex.parallel.DistributedDataParallel overlaps communication with computation in the backward pass.</span></span><br><span class="line">    <span class="comment"># model = DDP(model)</span></span><br><span class="line">    <span class="comment"># delay_allreduce delays all communication to the end of the backward pass.</span></span><br><span class="line">    <span class="comment"># DDP模块同时也计算整体的平均梯度, 这样我们就不需要在训练步骤计算平均梯度。</span></span><br><span class="line">    model = DDP(model, delay_allreduce=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ###################################  Resume from checkpoint ###########################################</span></span><br><span class="line"><span class="keyword">if</span> args.resume:</span><br><span class="line">    <span class="comment"># Use a local scope to avoid dangling references</span></span><br><span class="line">    <span class="comment"># dangling references: a variable that refers to an object that was deleted prematurely</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">resume</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(opt.ckpt_path):</span><br><span class="line">            print(<span class="string">'Resuming from checkpoint ...... '</span>)</span><br><span class="line">            checkpoint = torch.load(opt.ckpt_path,</span><br><span class="line">                                    map_location=torch.device(<span class="string">'cpu'</span>))  <span class="comment"># map to cpu to save the gpu memory</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># #################################################</span></span><br><span class="line">            <span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">            new_state_dict = OrderedDict()</span><br><span class="line">            <span class="comment"># # #################################################</span></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> checkpoint[<span class="string">'weights'</span>].items():</span><br><span class="line">                <span class="comment"># Exclude the regression layer by commenting the following code when we change the output dims!</span></span><br><span class="line">                <span class="comment"># if 'out' or 'merge' or 'before_regress'in k:</span></span><br><span class="line">                <span class="comment">#     continue</span></span><br><span class="line">                name = <span class="string">'module.'</span> + k  <span class="comment"># add prefix 'module.'</span></span><br><span class="line">                new_state_dict[name] = v</span><br><span class="line">            model.load_state_dict(new_state_dict, strict=<span class="literal">False</span>)  <span class="comment"># , strict=False</span></span><br><span class="line">            <span class="comment"># # #################################################</span></span><br><span class="line">            <span class="comment"># model.load_state_dict(checkpoint['weights'])  # 加入他人训练的模型，可能需要忽略部分层，则strict=False</span></span><br><span class="line">            print(<span class="string">'Network weights have been resumed from checkpoint...'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># amp.load_state_dict(checkpoint['amp'])</span></span><br><span class="line">            <span class="comment"># print('AMP loss_scalers and unskipped steps have been resumed from checkpoint...')</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ############## We must convert the resumed state data of optimizer to gpu  ##############</span></span><br><span class="line">            <span class="comment"># """It is because the previous training was done on gpu, so when saving the optimizer.state_dict, the stored</span></span><br><span class="line">            <span class="comment">#  states(tensors) are of cuda version. During resuming, when we load the saved optimizer, load_state_dict()</span></span><br><span class="line">            <span class="comment">#  loads this cuda version to cpu. But in this project, we use map_location to map the state tensors to cpu.</span></span><br><span class="line">            <span class="comment">#  In the training process, we need cuda version of state tensors, so we have to convert them to gpu."""</span></span><br><span class="line">            optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_weight'</span>])</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> optimizer.state.values():</span><br><span class="line">                <span class="keyword">for</span> k, v <span class="keyword">in</span> state.items():</span><br><span class="line">                    <span class="keyword">if</span> torch.is_tensor(v):</span><br><span class="line">                        state[k] = v.cuda()</span><br><span class="line">            print(<span class="string">'Optimizer has been resumed from checkpoint...'</span>)</span><br><span class="line">            <span class="keyword">global</span> best_loss, start_epoch  <span class="comment"># global declaration. otherwise best_loss and start_epoch can not be changed</span></span><br><span class="line">            best_loss = checkpoint[<span class="string">'train_loss'</span>]</span><br><span class="line">            print(<span class="string">'******************** Best loss resumed is :'</span>, best_loss, <span class="string">'  ************************'</span>)</span><br><span class="line">            start_epoch = checkpoint[<span class="string">'epoch'</span>] + <span class="number">1</span></span><br><span class="line">            print(<span class="string">"========&gt; Resume and start training from Epoch &#123;&#125; "</span>.format(start_epoch))</span><br><span class="line">            <span class="keyword">del</span> checkpoint</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"========&gt; No checkpoint found at '&#123;&#125;'"</span>.format(opt.ckpt_path))</span><br><span class="line"></span><br><span class="line">    resume()</span><br><span class="line"></span><br><span class="line">train_sampler = <span class="literal">None</span></span><br><span class="line">val_sampler = <span class="literal">None</span></span><br><span class="line"><span class="comment"># Restricts data loading to a subset of the dataset exclusive to the current process</span></span><br><span class="line"><span class="comment"># Create DistributedSampler to handle distributing the dataset across nodes when training 创建分布式采样器来控制训练中节点间的数据分发</span></span><br><span class="line"><span class="comment"># This can only be called after distributed.init_process_group is called 这个只能在 distributed.init_process_group 被调用后调用</span></span><br><span class="line"><span class="comment"># 这个对象控制进入分布式环境的数据集以确保模型不是对同一个子数据集训练，以达到训练目标。</span></span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)</span><br><span class="line">    val_sampler = torch.utils.data.distributed.DistributedSampler(val_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器，在训练和验证步骤中喂数据</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">                                           num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>, sampler=train_sampler, drop_last=<span class="literal">True</span>)</span><br><span class="line">val_loader = torch.utils.data.DataLoader(val_data, batch_size=opt.batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                         num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>, sampler=val_sampler, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">        print(<span class="string">'Parameters of network: Autograd'</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #  Update the learning rate for start_epoch times</span></span><br><span class="line"><span class="comment"># for i in range(start_epoch):</span></span><br><span class="line"><span class="comment">#     scheduler.step()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    print(<span class="string">'\n ############################# Train phase, Epoch: &#123;&#125; #############################'</span>.format(epoch))</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># DistributedSampler 中记录目前的 epoch 数， 因为采样器是根据 epoch 来决定如何打乱分配数据进各个进程</span></span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        train_sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># scheduler.step()  use 'adjust learning rate' instead</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># adjust_learning_rate_cyclic(optimizer, epoch, start_epoch)  # start_epoch</span></span><br><span class="line">    print(<span class="string">'\nLearning rate at this epoch is: %0.9f\n'</span> % optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>])  <span class="comment"># scheduler.get_lr()[0]</span></span><br><span class="line"></span><br><span class="line">    batch_time = AverageMeter()</span><br><span class="line">    losses = AverageMeter()</span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, target_tuple <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment"># # ##############  Use schedule step or fun of 'adjust learning rate' #####################</span></span><br><span class="line">        adjust_learning_rate(optimizer, epoch, batch_idx, len(train_loader), use_warmup=args.warmup)</span><br><span class="line">        <span class="comment"># print('\nLearning rate at this epoch is: %0.9f\n' % optimizer.param_groups[0]['lr'])  # scheduler.get_lr()[0]</span></span><br><span class="line">        <span class="comment"># # ##########################################################</span></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            <span class="comment">#  这允许异步 GPU 复制数据也就是说计算和数据传输可以同时进.</span></span><br><span class="line">            target_tuple = [target_tensor.cuda(non_blocking=<span class="literal">True</span>) <span class="keyword">for</span> target_tensor <span class="keyword">in</span> target_tuple]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]</span></span><br><span class="line">        images, mask_misses, heatmaps = target_tuple  <span class="comment"># , offsets, mask_offsets</span></span><br><span class="line">        <span class="comment"># images = Variable(images)</span></span><br><span class="line">        <span class="comment"># loc_targets = Variable(loc_targets)</span></span><br><span class="line">        <span class="comment"># conf_targets = Variable(conf_targets)</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># zero the gradient buff</span></span><br><span class="line">        loss = model(target_tuple)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> loss.item() &gt; <span class="number">2e5</span>:  <span class="comment"># try to rescue the gradient explosion</span></span><br><span class="line">            print(<span class="string">"\nOh My God ! \nLoss is abnormal, drop this batch !"</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">            scaled_loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)  # fixme: 可能是这个的问题吗？</span></span><br><span class="line">        optimizer.step()  <span class="comment"># TODO：可以使用累加的loss变相增大batch size，但对于bn层需要减少默认的momentum</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train_loss += loss.item()  # 累加的loss !</span></span><br><span class="line">        <span class="comment"># 使用loss += loss.detach()来获取不需要梯度回传的部分。</span></span><br><span class="line">        <span class="comment"># 或者使用loss.item()直接获得所对应的python数据类型，但是仅仅限于only one element tensors can be converted to Python scalars</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % args.print_freq == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Every print_freq iterations, check the loss, accuracy, and speed.</span></span><br><span class="line">            <span class="comment"># For best performance, it doesn't make sense to print these metrics every</span></span><br><span class="line">            <span class="comment"># iteration, since they incur an allreduce and some host&lt;-&gt;device syncs.</span></span><br><span class="line">            <span class="comment"># print 会触发allreduce，而这个操作比较费时</span></span><br><span class="line">            <span class="keyword">if</span> args.distributed:</span><br><span class="line">                <span class="comment"># We manually reduce and average the metrics across processes. In-place reduce tensor.</span></span><br><span class="line">                reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                reduced_loss = loss.data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># to_python_float incurs a host&lt;-&gt;device sync</span></span><br><span class="line">            losses.update(to_python_float(reduced_loss), images.size(<span class="number">0</span>))  <span class="comment"># update needs average and number</span></span><br><span class="line">            torch.cuda.synchronize()  <span class="comment"># 因为所有GPU操作是异步的，应等待当前设备上所有流中的所有核心完成，测试的时间才正确</span></span><br><span class="line">            batch_time.update((time.time() - end) / args.print_freq)</span><br><span class="line">            end = time.time()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">                print(<span class="string">'==================&gt; Epoch: [&#123;0&#125;][&#123;1&#125;/&#123;2&#125;]\t'</span></span><br><span class="line">                      <span class="string">'Time &#123;batch_time.val:.3f&#125; (&#123;batch_time.avg:.3f&#125;)\t'</span></span><br><span class="line">                      <span class="string">'Speed &#123;3:.3f&#125; (&#123;4:.3f&#125;)\t'</span></span><br><span class="line">                      <span class="string">'Loss &#123;loss.val:.10f&#125; (&#123;loss.avg:.4f&#125;) &lt;================ \t'</span>.format(</span><br><span class="line">                    epoch, batch_idx, len(train_loader),</span><br><span class="line">                    args.world_size * opt.batch_size / batch_time.val,</span><br><span class="line">                    args.world_size * opt.batch_size / batch_time.avg,</span><br><span class="line">                    batch_time=batch_time,</span><br><span class="line">                    loss=losses))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">global</span> best_loss</span><br><span class="line">    <span class="comment"># DistributedSampler控制进入分布式环境的数据集以确保模型不是对同一个子数据集训练，以达到训练目标。</span></span><br><span class="line">    <span class="comment"># train_loss /= (len(train_loader))  # Each GPU process can only see 1/(world_size) training samples per epoch</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Write the log file each epoch.</span></span><br><span class="line">        os.makedirs(checkpoint_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        logger = open(os.path.join(<span class="string">'./'</span> + checkpoint_path, <span class="string">'log'</span>), <span class="string">'a+'</span>)</span><br><span class="line">        logger.write(<span class="string">'\nEpoch &#123;&#125;\ttrain_loss: &#123;&#125;'</span>.format(epoch, losses.avg))  <span class="comment"># validation时不要\n换行</span></span><br><span class="line">        logger.flush()</span><br><span class="line">        logger.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> losses.avg &lt; float(<span class="string">'inf'</span>):  <span class="comment"># &lt; best_loss</span></span><br><span class="line">            <span class="comment"># Update the best_loss if the average loss drops</span></span><br><span class="line">            best_loss = losses.avg</span><br><span class="line">            print(<span class="string">'\nSaving model checkpoint...\n'</span>)</span><br><span class="line">            state = &#123;</span><br><span class="line">                <span class="comment"># not posenet.state_dict(). then, we don't ge the "module" string to begin with</span></span><br><span class="line">                <span class="string">'weights'</span>: model.module.state_dict(),</span><br><span class="line">                <span class="string">'optimizer_weight'</span>: optimizer.state_dict(),</span><br><span class="line">                <span class="comment"># 'amp': amp.state_dict(),</span></span><br><span class="line">                <span class="string">'train_loss'</span>: losses.avg,</span><br><span class="line">                <span class="string">'epoch'</span>: epoch</span><br><span class="line">            &#125;</span><br><span class="line">            torch.save(state, <span class="string">'./'</span> + checkpoint_path + <span class="string">'/PoseNet_'</span> + str(epoch) + <span class="string">'_epoch.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    print(<span class="string">'\n ############################# Test phase, Epoch: &#123;&#125; #############################'</span>.format(epoch))</span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment"># DistributedSampler 中记录目前的 epoch 数， 因为采样器是根据 epoch 来决定如何打乱分配数据进各个进程</span></span><br><span class="line">    <span class="comment"># if args.distributed:</span></span><br><span class="line">    <span class="comment">#     val_sampler.set_epoch(epoch)  # 验证集太小，不够4个划分</span></span><br><span class="line">    batch_time = AverageMeter()</span><br><span class="line">    losses = AverageMeter()</span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, target_tuple <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">        <span class="comment"># images.requires_grad_()</span></span><br><span class="line">        <span class="comment"># loc_targets.requires_grad_()</span></span><br><span class="line">        <span class="comment"># conf_targets.requires_grad_()</span></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            <span class="comment">#  这允许异步 GPU 复制数据也就是说计算和数据传输可以同时进.</span></span><br><span class="line">            target_tuple = [target_tensor.cuda(non_blocking=<span class="literal">True</span>) <span class="keyword">for</span> target_tensor <span class="keyword">in</span> target_tuple]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]</span></span><br><span class="line">        images, mask_misses, heatmaps = target_tuple  <span class="comment"># , offsets, mask_offsets</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            _, loss = model(target_tuple)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.distributed:</span><br><span class="line">            <span class="comment"># We manually reduce and average the metrics across processes. In-place reduce tensor.</span></span><br><span class="line">            reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reduced_loss = loss.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_python_float incurs a host&lt;-&gt;device sync</span></span><br><span class="line">        losses.update(to_python_float(reduced_loss), images.size(<span class="number">0</span>))  <span class="comment"># update needs average and number</span></span><br><span class="line">        torch.cuda.synchronize()  <span class="comment"># 因为所有GPU操作是异步的，应等待当前设备上所有流中的所有核心完成，测试的时间才正确</span></span><br><span class="line">        batch_time.update((time.time() - end))</span><br><span class="line">        end = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">            print(<span class="string">'==================&gt;Test: [&#123;0&#125;/&#123;1&#125;]\t'</span></span><br><span class="line">                  <span class="string">'Time &#123;batch_time.val:.3f&#125; (&#123;batch_time.avg:.3f&#125;)\t'</span></span><br><span class="line">                  <span class="string">'Speed &#123;2:.3f&#125; (&#123;3:.3f&#125;)\t'</span></span><br><span class="line">                  <span class="string">'Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;)\t'</span>.format(</span><br><span class="line">                batch_idx, len(val_loader),</span><br><span class="line">                args.world_size * opt.batch_size / batch_time.val,</span><br><span class="line">                args.world_size * opt.batch_size / batch_time.avg,</span><br><span class="line">                batch_time=batch_time, loss=losses))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># Print them in the Process 0</span></span><br><span class="line">        <span class="comment"># Write the log file each epoch.</span></span><br><span class="line">        os.makedirs(checkpoint_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        logger = open(os.path.join(<span class="string">'./'</span> + checkpoint_path, <span class="string">'log'</span>), <span class="string">'a+'</span>)</span><br><span class="line">        logger.write(<span class="string">'\tval_loss: &#123;&#125;'</span>.format(losses.avg))  <span class="comment"># validation时不要\n换行</span></span><br><span class="line">        logger.flush()</span><br><span class="line">        logger.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span><span class="params">(optimizer, epoch, step, len_epoch, use_warmup=False)</span>:</span></span><br><span class="line">    factor = epoch // <span class="number">15</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch &gt;= <span class="number">78</span>:</span><br><span class="line">        factor = (epoch - <span class="number">78</span>) // <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    lr = opt.learning_rate * args.world_size * (<span class="number">0.2</span> ** factor)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""Warmup"""</span></span><br><span class="line">    <span class="keyword">if</span> use_warmup:</span><br><span class="line">        <span class="keyword">if</span> epoch &lt; <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># print('=============&gt;  Using warm-up learning rate....')</span></span><br><span class="line">            lr = lr * float(<span class="number">1</span> + step + epoch * len_epoch) / (<span class="number">3.</span> * len_epoch)  <span class="comment"># len_epoch=len(train_loader)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># if(args.local_rank == 0):</span></span><br><span class="line">    <span class="comment">#     print("epoch = &#123;&#125;, step = &#123;&#125;, lr = &#123;&#125;".format(epoch, step, lr))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate_cyclic</span><span class="params">(optimizer, current_epoch, start_epoch, swa_freqent=<span class="number">5</span>, lr_max=<span class="number">4e-5</span>, lr_min=<span class="number">2e-5</span>)</span>:</span></span><br><span class="line">    epoch = current_epoch - start_epoch</span><br><span class="line"></span><br><span class="line">    lr = lr_max - (lr_max - lr_min) / (swa_freqent - <span class="number">1</span>) * (epoch - epoch // swa_freqent * swa_freqent)</span><br><span class="line">    lr = round(lr, <span class="number">8</span>)</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageMeter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Computes and stores the average and current value"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.val = <span class="number">0</span></span><br><span class="line">        self.avg = <span class="number">0</span></span><br><span class="line">        self.sum = <span class="number">0</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, val, n=<span class="number">1</span>)</span>:</span></span><br><span class="line">        self.val = val</span><br><span class="line">        self.sum += val * n</span><br><span class="line">        self.count += n</span><br><span class="line">        self.avg = self.sum / self.count</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="comment"># Reduces the tensor data across all machines</span></span><br><span class="line">    <span class="comment"># If we print the tensor, we can get:</span></span><br><span class="line">    <span class="comment"># tensor(334.4330, device='cuda:1') *********************, here is cuda:  cuda:1</span></span><br><span class="line">    <span class="comment"># tensor(359.1895, device='cuda:3') *********************, here is cuda:  cuda:3</span></span><br><span class="line">    <span class="comment"># tensor(263.3543, device='cuda:2') *********************, here is cuda:  cuda:2</span></span><br><span class="line">    <span class="comment"># tensor(340.1970, device='cuda:0') *********************, here is cuda:  cuda:0</span></span><br><span class="line">    rt = tensor.clone()  <span class="comment"># The function operates in-place.</span></span><br><span class="line">    dist.all_reduce(rt, op=dist.reduce_op.SUM)</span><br><span class="line">    rt /= args.world_size</span><br><span class="line">    <span class="keyword">return</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, start_epoch + <span class="number">100</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test(epoch)</span><br></pre></td></tr></table></figure>
<p>后面有时间的话再继续编辑，希望自己的总结可以帮助Pytorch新手们。如果我的具体代码或者具体的工作任务能够为大家提供一些帮助，那当然更加欢迎引用我的工作啦：</p>
<p>“Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation”</p>
<p><a href="https://link.zhihu.com/?target=https%253A//arxiv.org/abs/1911.10529">https://arxiv.org/abs/1911.10529​arxiv.org</a></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo实用技巧随记</title>
    <url>/2020/01/27/hello-world/</url>
    <content><![CDATA[<h1 id="welcome-to-hexo">Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>!</h1>
<p>This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line"><span class="comment"># or hexo s</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line"><span class="comment"># or hexo g</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line"><span class="comment"># or hexo d</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<h3 id="generate-and-deploy-in-one-command">Generate and deploy in one command</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure>
<hr />
<p>以下搬运自我的博客 <a href="https://blog.csdn.net/xiaojiajia007/article/details/104105250" target="_blank" rel="noopener">hexo实用技巧随记</a></p>
<h1 id="关于hexo">关于hexo</h1>
<h2 id="安装hexo及主题">安装hexo及主题</h2>
<p>安装hexo，可以参考：<a href="https://ryanluoxu.github.io/2017/11/24/%E7%94%A8-Hexo-%E5%92%8C-GitHub-Pages-%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">用 Hexo 和 GitHub Pages 搭建博客</a></p>
<p>解决latex公式渲染的问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
<p>然后到配置的主题next下的配置文件 /Users/lijia/科研&amp;学习/github博客/themes/next/_config.yml，更改配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  # Default (true) will load mathjax &#x2F; katex script on demand.</span><br><span class="line">  # That is it only render those page which has &#96;mathjax: true&#96; in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax &#x2F; katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: false</span><br><span class="line"></span><br><span class="line">  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br><span class="line">    # See: https:&#x2F;&#x2F;mhchem.github.io&#x2F;MathJax-mhchem&#x2F;</span><br><span class="line">    mhchem: false</span><br></pre></td></tr></table></figure>
<h2 id="行内数学公式显示异常">行内数学公式显示异常</h2>
<p>未知问题：有时在typora中显示正常的行内数学公式，在hexo中就显示异常了，比如下面一个情况 <img src="https://img-blog.csdnimg.cn/20200528154750912.png" alt="在这里插入图片描述" /> <strong>暂时的接近办法</strong>：通过尝试发现，对于有些显示不正常的行内公式<span class="math inline">\(X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)\)</span>我们只需要在前面或者后面一个$符号和正文之间插入一个空格，公式渲染显示就正常了。</p>
<h2 id="更改markdown信息头模板">更改markdown信息头模板</h2>
<p>默认的信息头不全，我们把markdown模板 /Users/lijia/科研&amp;学习/github博客/scaffolds/post.md 里面最开始的信息更改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">description: 点击阅读前文前, 首页能看到的文章的简短描述 </span><br><span class="line">categories:</span><br></pre></td></tr></table></figure>
<h2 id="添加搜索功能">添加搜索功能</h2>
<p>首先在blog文件夹目录下安装依赖 <code>npm install hexo-generator-searchdb --save</code></p>
<p>然后在站点配置文件中_config.yml中增加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>最后在主题配置文件（_config.yml），修改local_search的值如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;flashlab&#x2F;hexo-generator-search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="关于next主题">关于next主题</h1>
<p>首先进入本地blog目录，使用git clone，把主题项目克隆下来, <code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code> 然后在站点配置文件中更改theme选项为 next.</p>
<h2 id="更改主题细节设置">更改主题细节设置</h2>
<h3 id="页面配置">页面配置</h3>
<p>设置开始页面显示，设置头像，设置社交媒体链接等等， 可以参考这个博客：<a href="http://www.zhangblog.com/2019/06/16/hexo04/" target="_blank" rel="noopener">Linux下使用 github+hexo 搭建个人博客04-next主题优化</a> 或者参考这个文章：<a href="https://zhuanlan.zhihu.com/p/25959864" target="_blank" rel="noopener">Hexo+Next配置Blog</a></p>
<h3 id="更改默认字体大小">更改默认字体大小</h3>
<p>主题默认的字体太大了，一个页面放不下很多内容，根据我们创建的路径，在/Users/lijia/科研&amp;学习/github博客/themes/next/source/css/_variables/base.styl 中找到font size，把font-size-base默认值更改即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Font size</span><br><span class="line">$font-size-base           &#x3D; (hexo-config(&#39;font.enable&#39;) and hexo-config(&#39;font.global.size&#39;) is a &#39;unit&#39;) ? unit(hexo-config(&#39;font.global.size&#39;), em) : 14px;</span><br></pre></td></tr></table></figure>
<h3 id="网址缩略图">网址缩略图</h3>
<p><img src="https://img-blog.csdnimg.cn/20200203182511137.png" alt="在这里插入图片描述" /> 如果我们想要更改网址栏左侧的缩略图标，我们先制作好图标，放入对应的 <code>/Users/lijia/科研&amp;学习/github博客/themes/next/source/images</code> 文件夹下，然后更改主题（我们用的是next）下的配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: &#x2F;images&#x2F;flower-16x16.png</span><br><span class="line">  medium: &#x2F;images&#x2F;flower.ico # 建议使用ico图标而不是png图片，以防兼容性问题</span><br><span class="line">  apple_touch_icon: &#x2F;images&#x2F;flower.png # apple-touch-icon-next.png</span><br><span class="line">  #safari_pinned_tab: &#x2F;images&#x2F;flower.svg #logo.svg</span><br><span class="line">  #android_manifest: &#x2F;images&#x2F;manifest.json</span><br><span class="line">  #ms_browserconfig: &#x2F;images&#x2F;browserconfig.xml</span><br></pre></td></tr></table></figure>
<h3 id="接入评论系统以及文章阅读次数显示">接入评论系统以及文章阅读次数显示</h3>
<p>参考： http://www.zhangblog.com/2019/06/16/hexo05/ http://www.zhangblog.com/2019/06/16/hexo06/ 使用 <a href="https://leancloud.cn/dashboard/applist.html#/apps" target="_blank" rel="noopener">LeanCloud</a>，可以同时实现评论留言系统以及文章阅读次数统计。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Valine</span><br><span class="line"># For more information: https:&#x2F;&#x2F;valine.js.org, https:&#x2F;&#x2F;github.com&#x2F;xCss&#x2F;Valine</span><br><span class="line">valine:</span><br><span class="line">  enable: true</span><br><span class="line">  appid: hcHrBNtFBdqhBMIhjL67LT0t-gzGzoHsz # Your leancloud application appid</span><br><span class="line">  appkey: Pe1HEgFp3AUcNCQ7dpQ3HRE4 # Your leancloud application appkey</span><br><span class="line">  notify: false # Mail notifier</span><br><span class="line">  verify: false # Verification code</span><br><span class="line">  placeholder: 欢迎讨论留言！ # Comment box placeholder</span><br><span class="line">  avatar: mm # Gravatar style</span><br><span class="line">  guest_info: nick,mail,link # Custom comment header</span><br><span class="line">  pageSize: 10 # Pagination size</span><br><span class="line">  language: # Language, available values: en, zh-cn</span><br><span class="line">  visitor: true # Article reading statistic # 这里控制显示文章阅读次数</span><br></pre></td></tr></table></figure> 设置完成后文章的开头就会显示如下： <img src="https://img-blog.csdnimg.cn/20200203203817826.png" alt="在这里插入图片描述" /> 增加了 Views 和 Valine，而博客文章下面就有了留言板。</p>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>测试专用</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
